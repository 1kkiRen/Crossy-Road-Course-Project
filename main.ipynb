{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = torch.device(\"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple\n",
    "\n",
    "\n",
    "class RecurrentIQN(nn.Module):\n",
    "    def __init__(self, input_size: int, output_size: int, hidden_size: int, n_quantiles=32) -> None:\n",
    "        \"\"\" Initialize the Recurrent IQN model\n",
    "\n",
    "        Args:\n",
    "            input_size (int): The size of the input matrix\n",
    "            output_size (int): The size of the actions\n",
    "            hidden_size (int): The size of the hidden layer\n",
    "            n_quantiles (int, optional): The number of quantiles. Defaults to 32.\n",
    "        \"\"\"\n",
    "        \n",
    "        super(RecurrentIQN, self).__init__()\n",
    "        self.n_quantiles = n_quantiles\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
    "        self.quantile_embed = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x: torch.FloatTensor, quantiles: torch.Tensor, hidden: Tuple[torch.Tensor, torch.Tensor]\n",
    ") -> Tuple[torch.FloatTensor | Tuple[torch.Tensor]]:\n",
    "        \"\"\" Forward pass of the Recurrent IQN model\n",
    "\n",
    "        Args:\n",
    "            x (torch.FloatTensor): The input tensor\n",
    "            quantiles (torch.Tensor): The quantiles\n",
    "            hidden (Tuple[torch.Tensor, torch.Tensor]): The hidden state\n",
    "\n",
    "        Returns:\n",
    "            torch.FloatTensor: The output tensor\n",
    "            Tuple: The hidden state of the model\n",
    "        \"\"\"\n",
    "        \n",
    "        lstm_out, hidden = self.lstm(x, hidden)\n",
    "        \n",
    "        quantiles = quantiles.unsqueeze(-1)  \n",
    "        pi = torch.acos(torch.zeros(1)).item() * 2\n",
    "        quantile_feats = torch.cos(pi * quantiles * torch.arange(1, self.hidden_size + 1).to(x.device))\n",
    "        quantile_feats = F.relu(self.quantile_embed(quantile_feats)) \n",
    "\n",
    "        lstm_out = lstm_out[:, -1, :].unsqueeze(1) \n",
    "        x = lstm_out * quantile_feats \n",
    "\n",
    "        x = self.fc(x) \n",
    "        return x, hidden\n",
    "\n",
    "    def act(self, state: List, hidden: Tuple[torch.Tensor, torch.Tensor], epsilon: float) -> Tuple[int, Tuple[torch.Tensor]]:\n",
    "        \"\"\" Acting function of the Recurrent IQN model\n",
    "\n",
    "        Args:\n",
    "            state (List): The state of the environment\n",
    "            hidden (Tuple[torch.Tensor, torch.Tensor]): The hidden state of the model\n",
    "            epsilon (float): The epsilon value\n",
    "\n",
    "        Returns:\n",
    "            int: The action to take\n",
    "            Tuple: The hidden state of the model\n",
    "        \"\"\"\n",
    "        \n",
    "        if random.random() > epsilon:\n",
    "            print(\"Model acting\")\n",
    "            with torch.no_grad():\n",
    "                state = torch.FloatTensor(state).unsqueeze(0).unsqueeze(0).to(next(self.parameters()).device)\n",
    "                quantiles = torch.rand(1, self.n_quantiles).to(state.device)\n",
    "                q_values, hidden = self.forward(state, quantiles, hidden)\n",
    "                q_values = q_values.mean(dim=1)\n",
    "                action = q_values.argmax(dim=1).item()\n",
    "        else:\n",
    "            action = random.randrange(self.fc.out_features)\n",
    "        return action, hidden\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "from numpy import dtype, ndarray\n",
    "\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity: int) -> None:\n",
    "        \"\"\" Initialize the ReplayBuffer\n",
    "\n",
    "        Args:\n",
    "            capacity (int): The capacity of the buffer\n",
    "        \"\"\"\n",
    "        \n",
    "        self.capacity = capacity\n",
    "        self.buffer = []\n",
    "        self.position = 0\n",
    "        \n",
    "    def push(self, state: list, action: int, reward: float, next_state: list, done: int) -> None:\n",
    "        \"\"\" Push a new experience to the buffer\n",
    "\n",
    "        Args:\n",
    "            state (list): State of the environment\n",
    "            action (int): The action taken\n",
    "            reward (float): The reward received\n",
    "            next_state (list): The next state of the environment\n",
    "            done (int): The done flag\n",
    "        \"\"\"\n",
    "        \n",
    "        if len(self.buffer) < self.capacity:\n",
    "            self.buffer.append(None)\n",
    "        self.buffer[self.position] = (state, action, reward, next_state, done)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "        \n",
    "    def sample(self, batch_size: int) -> Tuple[ndarray[Any, dtype[Any]], ndarray[Any, dtype[Any]], ndarray[Any, dtype[Any]], ndarray[Any, dtype[Any]], ndarray[Any, dtype[Any]]]:\n",
    "        \"\"\" Sample a batch from the buffer\n",
    "\n",
    "        Args:\n",
    "            batch_size (int): The size of the batch\n",
    "\n",
    "        Returns:\n",
    "            Tuple[ndarray[Any, dtype[Any]], ndarray[Any, dtype[Any]], ndarray[Any, dtype[Any]], ndarray[Any, dtype[Any]], ndarray[Any, dtype[Any]]: The batch of experiences\n",
    "        \"\"\"\n",
    "        \n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        state, action, reward, next_state, done = map(np.stack, zip(*batch))\n",
    "        return state, action, reward, next_state, done\n",
    "    \n",
    "    def pop(self, count=1) -> None:\n",
    "        \"\"\" Pop the first element from the buffer\n",
    "        \n",
    "        Args:\n",
    "            count (int, optional): The number of elements to pop. Defaults to 1.\n",
    "        \"\"\"\n",
    "        \n",
    "        for _ in range(count):\n",
    "            self.buffer.pop(0)\n",
    "            self.position -= 1\n",
    "            if self.position < 0:\n",
    "                self.position = 0\n",
    "     \n",
    "    def __len__(self):\n",
    "        \"\"\" Get the length of the buffer\n",
    "        \n",
    "        Returns:\n",
    "            int: The length of the buffer\n",
    "        \"\"\"\n",
    "        \n",
    "        return len(self.buffer)\n",
    "    \n",
    "\n",
    "class IQNAgent:\n",
    "    def __init__(self, input_size, output_size, hidden_size, replay_buffer_capacity, batch_size, gamma, epsilon_start, epsilon_end, epsilon_decay, n_quantiles=32) -> None:\n",
    "        \"\"\" Initialize the IQN Agent. The IQN agent is a recurrent IQN model with a replay buffer.\n",
    "\n",
    "        Args:\n",
    "            input_size (int): The size of the input matrix\n",
    "            output_size (int): The size of the actions\n",
    "            hidden_size (int): The size of the hidden layer\n",
    "            replay_buffer_capacity (int): The capacity of the replay buffer\n",
    "            batch_size (int): The size of the batch for training\n",
    "            gamma (float): The discount factor\n",
    "            epsilon_start (float): The starting epsilon value\n",
    "            epsilon_end (float): The ending epsilon value\n",
    "            epsilon_decay (float): The decay rate of epsilon\n",
    "            n_quantiles (int, optional): The number of quantiles. Defaults to 32.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.replay_buffer_capacity = replay_buffer_capacity\n",
    "        self.batch_size = batch_size\n",
    "        self.gamma = gamma\n",
    "        self.epsilon_start = epsilon_start\n",
    "        self.epsilon_end = epsilon_end\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.n_quantiles = n_quantiles\n",
    "\n",
    "        self.model = RecurrentIQN(input_size, output_size, hidden_size, n_quantiles).to(device)\n",
    "        self.model_target = RecurrentIQN(input_size, output_size, hidden_size, n_quantiles).to(device)\n",
    "        self.model_target.load_state_dict(self.model.state_dict())\n",
    "\n",
    "        self.hidden = (torch.zeros(1, 1, hidden_size).to(device),\n",
    "                       torch.zeros(1, 1, hidden_size).to(device))\n",
    "\n",
    "        self.replay_buffer = ReplayBuffer(replay_buffer_capacity)\n",
    "        self.optimizer = optim.Adam(self.model.parameters())\n",
    "\n",
    "        self.steps_done = 0\n",
    "\n",
    "        self.update_counter = 0\n",
    "        self.target_update_freq = 1000\n",
    "\n",
    "    def select_action(self, state: list) -> int:\n",
    "        \"\"\" Select an action based on the state\n",
    "\n",
    "        Args:\n",
    "            state (list): The state of the environment\n",
    "\n",
    "        Returns:\n",
    "            int: The action to take\n",
    "        \"\"\"\n",
    "        \n",
    "        epsilon = self.epsilon_end + (self.epsilon_start - self.epsilon_end) * np.exp(-1. * self.steps_done / self.epsilon_decay)\n",
    "        self.steps_done += 1\n",
    "        action, self.hidden = self.model.act(state, self.hidden, epsilon)\n",
    "        return action\n",
    "\n",
    "    def optimize_model(self) -> None | float:\n",
    "        \"\"\" Optimize the model\n",
    "\n",
    "        Returns:\n",
    "            float: The loss of the model\n",
    "        \"\"\"\n",
    "        \n",
    "        if len(self.replay_buffer) < self.batch_size:\n",
    "            return\n",
    "        \n",
    "        self.model.train()\n",
    "        print(\"Optimizing model...\")\n",
    "        states, actions, rewards, next_states, dones = self.replay_buffer.sample(self.batch_size)\n",
    "\n",
    "        states = torch.FloatTensor(states).unsqueeze(1).to(device)\n",
    "        actions = torch.LongTensor(actions).to(device)\n",
    "        rewards = torch.FloatTensor(rewards).to(device)\n",
    "        next_states = torch.FloatTensor(next_states).unsqueeze(1).to(device)\n",
    "        dones = torch.FloatTensor(dones).to(device)\n",
    "        \n",
    "        quantiles = torch.rand(self.batch_size, self.n_quantiles).to(device)\n",
    "\n",
    "        hidden = (torch.zeros(1, self.batch_size, self.hidden_size).to(device),\n",
    "                  torch.zeros(1, self.batch_size, self.hidden_size).to(device))\n",
    "\n",
    "        current_q, _ = self.model(states, quantiles, hidden)\n",
    "        current_q = current_q.gather(2, actions.unsqueeze(-1).unsqueeze(-1).expand(-1, self.n_quantiles, -1)).squeeze(-1)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            next_hidden = (torch.zeros(1, self.batch_size, self.hidden_size).to(device),\n",
    "                           torch.zeros(1, self.batch_size, self.hidden_size).to(device))\n",
    "            next_quantiles = torch.rand(self.batch_size, self.n_quantiles).to(device)\n",
    "            next_q, _ = self.model_target(next_states, next_quantiles, next_hidden)\n",
    "            next_q = next_q.max(2)[0]\n",
    "            target_q = rewards.unsqueeze(1) + self.gamma * next_q * (1 - dones.unsqueeze(1))\n",
    "\n",
    "        td_errors = target_q.unsqueeze(1) - current_q\n",
    "        huber_loss = F.smooth_l1_loss(current_q, target_q.unsqueeze(1), reduction='none')\n",
    "        quantile_loss = (torch.abs(quantiles.unsqueeze(-1) - (td_errors.detach() < 0).float()) * huber_loss).mean()\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        quantile_loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        self.update_counter += 1\n",
    "        if self.update_counter % self.target_update_freq == 0:\n",
    "            self.update_target_network()\n",
    "\n",
    "        return quantile_loss.item()\n",
    "\n",
    "    def update_target_network(self) -> None:\n",
    "        \"\"\" Update the target network \"\"\"\n",
    "        \n",
    "        self.model_target.load_state_dict(self.model.state_dict())\n",
    "\n",
    "    def push(self, state: list, action: int, reward: float, next_state: list, done: int) -> None:\n",
    "        \"\"\" Push a new experience to the replay buffer\n",
    "\n",
    "        Args:\n",
    "            state (list): The state of the environment\n",
    "            action (int): The action taken\n",
    "            reward (float): The reward received\n",
    "            next_state (list): The next state of the environment\n",
    "            done (int): The done flag\n",
    "        \"\"\"\n",
    "        \n",
    "        self.replay_buffer.push(state, action, reward, next_state, done)\n",
    "\n",
    "    def save(self, path: str) -> None:\n",
    "        \"\"\" Save the model to a file\n",
    "\n",
    "        Args:\n",
    "            path (str): The path to save the model\n",
    "        \"\"\"\n",
    "        torch.save(self.model.state_dict(), path)\n",
    "\n",
    "    def load(self, path: str) -> None:\n",
    "        \"\"\" Load the model from a file\n",
    "\n",
    "        Args:\n",
    "            path (str): The path to load the model from\n",
    "        \"\"\" \n",
    "        \n",
    "        self.model.load_state_dict(torch.load(path))\n",
    "        self.model.eval()\n",
    "\n",
    "    def reset(self) -> None:\n",
    "        \"\"\" Reset the agent \"\"\"\n",
    "        self.steps_done = 0\n",
    "    \n",
    "    def pop(self, count=1):\n",
    "        \"\"\" Pop the first element from the replay buffer\n",
    "        \n",
    "        Args:\n",
    "            count (int, optional): The number of elements to pop. Defaults to 1.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.replay_buffer.pop(count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simplify_state(state: np.ndarray) -> Tuple[Tuple[int, int], Tuple, Tuple, Tuple, Tuple]:\n",
    "    \"\"\" Simplify the state of the environment\n",
    "\n",
    "    Args:\n",
    "        state (np.ndarray): The state of the environment\n",
    "\n",
    "    Returns:\n",
    "        Tuple[Tuple[int, int], Tuple[Tuple[int, int]], Tuple[Tuple[int, int]]]: The simplified state\n",
    "    \"\"\"\n",
    "    \n",
    "    agent_x, agent_y = get_agent_position(state)\n",
    "    obstacles = get_obstacles(state)\n",
    "    timbers = get_timbers(state)\n",
    "    cars = get_cars(state)\n",
    "    water = get_water(state)\n",
    "    \n",
    "    simplified_state = (\n",
    "        (agent_x, agent_y),\n",
    "        tuple(set(obstacles)),\n",
    "        tuple(set(timbers)),\n",
    "        tuple(set(cars)),\n",
    "        tuple(set(water))\n",
    "        )\n",
    "    \n",
    "    return simplified_state\n",
    "\n",
    "def get_agent_position(state: np.ndarray) -> Tuple[int, int]:\n",
    "    \"\"\" Get the position of the agent in the state\n",
    "\n",
    "    Args:\n",
    "        state (np.ndarray): The state of the environment\n",
    "\n",
    "    Returns:\n",
    "        tuple[int, int]: The position of the agent\n",
    "    \"\"\"\n",
    "    \n",
    "    for i, row in enumerate(state):\n",
    "        for j, cell in enumerate(row):\n",
    "            if cell == 2:\n",
    "                return j, i\n",
    "            \n",
    "    return state.shape[1] // 2, state.shape[0] - 1\n",
    "    \n",
    "def get_cars(state: np.ndarray) -> list:\n",
    "    \"\"\" Get the positions of the cars in the state\n",
    "\n",
    "    Args:\n",
    "        state (np.ndarray): The state of the environment\n",
    "\n",
    "    Returns:\n",
    "        list: The positions of the cars\n",
    "    \"\"\"\n",
    "    \n",
    "    cars = []\n",
    "    for i, row in enumerate(state):\n",
    "        for j, cell in enumerate(row):\n",
    "            if cell == 1:\n",
    "                cars.append((j, i))\n",
    "    return cars\n",
    "            \n",
    "def get_obstacles(state: np.ndarray) -> list:\n",
    "    \"\"\" Get the positions of the obstacles in the state\n",
    "\n",
    "    Args:\n",
    "        state (np.ndarray): The state of the environment\n",
    "\n",
    "    Returns:\n",
    "        list: The positions of the obstacles\n",
    "    \"\"\"\n",
    "    \n",
    "    obstacles = []\n",
    "    for i, row in enumerate(state):\n",
    "        for j, cell in enumerate(row):\n",
    "            if cell == 2:\n",
    "                obstacles.append((j, i))\n",
    "    return obstacles\n",
    "\n",
    "def get_timbers(state: np.ndarray) -> list:\n",
    "    \"\"\" Get the positions of the timbers in the state\n",
    "\n",
    "    Args:\n",
    "        state (np.ndarray): The state of the environment\n",
    "\n",
    "    Returns:\n",
    "        list: The positions of the timbers\n",
    "    \"\"\"\n",
    "    \n",
    "    timbers = []\n",
    "    for i, row in enumerate(state):\n",
    "        for j, cell in enumerate(row):\n",
    "            if cell == 4:\n",
    "                timbers.append((j, i))\n",
    "    return timbers\n",
    "\n",
    "def get_water(state: np.ndarray) -> list:\n",
    "    \"\"\" Get the positions of the water in the state\n",
    "\n",
    "    Args:\n",
    "        state (np.ndarray): The state of the environment\n",
    "\n",
    "    Returns:\n",
    "        list: The positions of the water\n",
    "    \"\"\"\n",
    "    \n",
    "    water = []\n",
    "    for i, row in enumerate(state):\n",
    "        for j, cell in enumerate(row):\n",
    "            if cell == 5:\n",
    "                water.append((j, i))\n",
    "    return water\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "from numpy._typing._array_like import NDArray\n",
    "import pyautogui\n",
    "import cv2\n",
    "import time\n",
    "import keyboard\n",
    "import torchvision\n",
    "from ultralytics import YOLO\n",
    "\n",
    "\n",
    "RES_X = 1920\n",
    "RES_Y = 1080\n",
    "\n",
    "GAME_REGION = (405, 210, 850, 480)\n",
    "restart_button = cv2.imread('restart_button.png', cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "\n",
    "def get_screen(region: Tuple[int, int , int, int]) -> Tuple:\n",
    "    \"\"\" Get the screenshot of the game\n",
    "\n",
    "    Args:\n",
    "        region (Tuple): The region of the screen to capture\n",
    "\n",
    "    Returns:\n",
    "        Tuple: The screenshot of the game\n",
    "    \"\"\"\n",
    "    \n",
    "    screen = pyautogui.screenshot(region=(region[0], region[1], region[2], region[3]))\n",
    "    \n",
    "    non_crop = screen.copy()\n",
    "\n",
    "    transforms = torchvision.transforms.Compose([\n",
    "        torchvision.transforms.RandomRotation((14, 14)),\n",
    "        torchvision.transforms.CenterCrop((320, 566)),\n",
    "        torchvision.transforms.Resize((240, 425)),\n",
    "    ])\n",
    "    \n",
    "    screen = transforms(screen)   \n",
    "    \n",
    "    screen = cv2.cvtColor(np.array(screen), cv2.COLOR_RGB2BGR)\n",
    "    \n",
    "    non_crop = cv2.cvtColor(np.array(non_crop), cv2.COLOR_RGB2BGR)\n",
    "    non_crop = cv2.resize(non_crop, (425, 240))\n",
    "    \n",
    "    return screen, non_crop\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def map_to_grid(image_size, grid_size, boxes, class_labels) -> np.ndarray[Any]:\n",
    "    \"\"\"\n",
    "    Map detected bounding boxes to a grid representation.\n",
    "\n",
    "    Args:\n",
    "        image_size: Tuple (width, height) of the image.\n",
    "        grid_size: Tuple (N, M) of the grid dimensions.\n",
    "        boxes: List of bounding boxes [(x_min, y_min, x_max, y_max)].\n",
    "        class_labels: List of class labels corresponding to the boxes.\n",
    "\n",
    "    Returns:\n",
    "        grid: 2D numpy array of shape (N, M) with object class labels.\n",
    "    \"\"\"\n",
    "    \n",
    "    width, height = image_size\n",
    "    grid_width, grid_height = grid_size\n",
    "    grid = np.zeros((grid_height, grid_width), dtype=int)\n",
    "\n",
    "    cell_width = width / grid_width\n",
    "    cell_height = height / grid_height\n",
    "\n",
    "    for (x_min, y_min, x_max, y_max), label in zip(boxes, class_labels):\n",
    "        x_start = int(x_min // cell_width)\n",
    "        y_start = int(y_min // cell_height)\n",
    "        x_end = int(np.ceil(x_max / cell_width))\n",
    "        y_end = int(np.ceil(y_max / cell_height))\n",
    "\n",
    "        for y in range(y_start, y_end):\n",
    "            for x in range(x_start, x_end):\n",
    "                grid[y, x] = label + 1\n",
    "\n",
    "    return grid\n",
    "\n",
    "\n",
    "def get_state(screen) -> ndarray[Any, dtype[Any]]:\n",
    "    \"\"\" Get the state of the environment\n",
    "\n",
    "    Args:\n",
    "        screen (MatLike): The screenshot of the game\n",
    "\n",
    "    Returns:\n",
    "        ndarray[Any, dtype[Any]]: The grid state of the environment\n",
    "    \"\"\"\n",
    "    \n",
    "    results = cv_model(screen, verbose=False)\n",
    "\n",
    "    image_size = (425, 240)\n",
    "    grid_size = (36, 32)\n",
    "\n",
    "    boxes = []\n",
    "    labels = []\n",
    "    \n",
    "    boxes_ = results[0].boxes\n",
    "    for box in boxes_:\n",
    "        x_min, y_min, x_max, y_max = box.xyxy[0].tolist()\n",
    "        \n",
    "        class_id = int(box.cls[0].item())\n",
    "        \n",
    "        boxes.append((x_min, y_min, x_max, y_max))\n",
    "        labels.append(class_id)\n",
    "\n",
    "    try:\n",
    "        boxes, labels = zip(*sorted(zip(boxes, labels), key=lambda x: -x[1]))    \n",
    "    \n",
    "    except ValueError:\n",
    "        boxes = []\n",
    "        labels = []\n",
    "    \n",
    "    grid = map_to_grid(image_size, grid_size, boxes, labels)\n",
    "\n",
    "    return grid\n",
    "\n",
    "def is_game_over(image, score_threshold=0.5, scale=0.5):\n",
    "    \"\"\" Check if the game is over\n",
    "    \n",
    "    Args:\n",
    "        image (MatLike): The screenshot of the game\n",
    "        score_threshold (float, optional): The score threshold. Defaults to 0.5.\n",
    "        scale (float, optional): The scale of the image. Defaults to 0.5.\n",
    "        \n",
    "    Returns:\n",
    "        bool: True if the game is over, False otherwise\n",
    "    \"\"\"\n",
    "    \n",
    "    grey_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    resized_template = cv2.resize(restart_button, (0, 0), fx=scale, fy=scale)\n",
    "    h, w = grey_image.shape\n",
    "\n",
    "    cropped_search_box = grey_image[int(h * 0.87):, int(w * 0.43):int(w * 0.57)]\n",
    "    \n",
    "    result = cv2.matchTemplate(cropped_search_box, resized_template, cv2.TM_CCOEFF_NORMED)\n",
    "    result = np.sort(result.flatten())[::-1]\n",
    "    \n",
    "    return result.max() > score_threshold\n",
    "\n",
    "def process_state(state, max_obstacles=1152, max_timbers=1152, max_cars=1152, max_water=1152) -> list:\n",
    "    \"\"\" Process the state of the environment\n",
    "\n",
    "    Args:\n",
    "        state (Tuple): The state of the environment\n",
    "        max_obstacles (int, optional): The maximum number of obstacles. Defaults to 1152.\n",
    "        max_timbers (int, optional): The maximum number of timbers. Defaults to 1152.\n",
    "        max_cars (int, optional): The maximum number of cars. Defaults to 1152.\n",
    "        max_water (int, optional): The maximum number of water. Defaults to 1152.\n",
    "\n",
    "    Returns:\n",
    "        list: The processed state of the environment\n",
    "    \"\"\"\n",
    "    \n",
    "    agent_pos, obstacles, timbers, cars, water = state[0], state[1], state[2], state[3], state[4]\n",
    "    \n",
    "    state_vector = list(agent_pos)\n",
    "    \n",
    "    items = [(list(obstacles), max_obstacles), (list(timbers), max_timbers), (list(cars), max_cars), (list(water), max_water)]\n",
    "    \n",
    "    for item, max_count in items:\n",
    "        for i in range(max_count):\n",
    "            if i < len(item):\n",
    "                state_vector.extend(item[i])\n",
    "            else:\n",
    "                state_vector.extend([0, 0])\n",
    "    \n",
    "    return state_vector\n",
    "\n",
    "def compute_reward(reward_state) -> float | Literal[-100]:\n",
    "    \"\"\" Compute the reward of the environment\n",
    "\n",
    "    Returns:\n",
    "        float: The reward of the environment\n",
    "    \"\"\"\n",
    "    \n",
    "    time = reward_state['time']\n",
    "    state = reward_state['state']\n",
    "    action = reward_state['action']\n",
    "    prev_action = reward_state['prev_action']\n",
    "    next_state = reward_state['next_state']\n",
    "    total_reward = reward_state['total_reward']\n",
    "    non_crop_state = reward_state['non_crop_state']\n",
    "    reward = 0\n",
    "    \n",
    "    if is_game_over(non_crop_state):\n",
    "        reward = -100\n",
    "        return reward\n",
    "    \n",
    "    if action == 0:\n",
    "        reward += 2\n",
    "    elif action == 2:\n",
    "        reward += 0.25\n",
    "    elif action == 3:\n",
    "        reward += 0.25\n",
    "    elif action == 4:\n",
    "        reward += 0.2\n",
    "    \n",
    "    hen_count = 0\n",
    "    for row in next_state:\n",
    "        for cell in row:\n",
    "            if cell == 1:\n",
    "                hen_count += 1\n",
    "\n",
    "    if action == prev_action:\n",
    "        if state[0][0] // 5 == next_state[0][0] // 5 \\\n",
    "        and state[0][1] // 5 == next_state[0][1] // 5:\n",
    "            reward -= 10\n",
    "            \n",
    "    reward = round(reward, 2)\n",
    "    \n",
    "    return reward\n",
    "    \n",
    "    \n",
    "actions = [0, 1, 2, 3, 4]\n",
    "\n",
    "input_size = 9218\n",
    "agent = IQNAgent(input_size, 5, 128, 10000, 32, 0.99, 1.0, 0.1, 1000)\n",
    "cv_model = YOLO('best_cv.pt')\n",
    "episodes = 1000\n",
    "episode_length = 1000\n",
    "losses = []\n",
    "rewards = []\n",
    "\n",
    "print(\"Model is ready to train\")\n",
    "\n",
    "keyboard.wait('q')\n",
    "\n",
    "for episode in range(episodes):\n",
    "    screenshot, non_crop_state = get_screen(GAME_REGION)\n",
    "    state_raw = get_state(screenshot)\n",
    "    \n",
    "    total_reward = 0\n",
    "    total_loss = 0\n",
    "    action = 0\n",
    "\n",
    "    state = simplify_state(state_raw)\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "    \n",
    "    \n",
    "    start_time = time.time()\n",
    "    for step in range(episode_length):\n",
    "        prev_action = action\n",
    "        time.sleep(0.025)\n",
    "        \n",
    "        state_vector = process_state(state)\n",
    "        \n",
    "        action = agent.select_action(state_vector)\n",
    "        \n",
    "        if action < 4:\n",
    "            pyautogui.press(['up', 'down', 'left', 'right'][action])\n",
    "        \n",
    "        \n",
    "        next_screenshot, non_crop_state = get_screen(GAME_REGION)\n",
    "        next_state_raw = get_state(next_screenshot)\n",
    "        \n",
    "        next_state = simplify_state(next_state_raw)\n",
    "        next_state_vector = process_state(next_state)\n",
    "        \n",
    "        reward_state = {\n",
    "            'state': state,\n",
    "            'action': action,\n",
    "            'prev_action': prev_action,\n",
    "            'next_state': next_state,\n",
    "            'time': time.time() - start_time,\n",
    "            'total_reward': total_reward,\n",
    "            'non_crop_state': non_crop_state\n",
    "        }\n",
    "        \n",
    "        done = 0\n",
    "        reward = compute_reward(reward_state)\n",
    "        if is_game_over(non_crop_state):\n",
    "            done = 1\n",
    "            agent.push(state_vector, action, reward, next_state_vector, done)\n",
    "            break\n",
    "        \n",
    "\n",
    "        agent.push(state_vector, action, reward, next_state_vector, done)\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "        \n",
    "        loss = agent.optimize_model()\n",
    "\n",
    "        print(f\"Step: {step}, Action: {action}, Reward: {reward}, Total Reward: {total_reward}, Loss: {loss}\")\n",
    " \n",
    "    keyboard.press_and_release('space')\n",
    "    losses.append(total_loss)\n",
    "    rewards.append(total_reward)\n",
    "    print('\\nepisode: {}, reward: {}\\n'.format(episode, total_reward))\n",
    "    \n",
    "    agent.pop(4)    \n",
    "\n",
    "    if episode % 10 == 0:\n",
    "        agent.save('dqn.pth')\n",
    "        print(\"Model saved\")\n",
    "    \n",
    "    time.sleep(3.25)\n",
    "    keyboard.press_and_release('space')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
