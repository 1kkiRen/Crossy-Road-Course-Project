{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DQN model for Crossy Road game\n",
    "# input is image of the game screen\n",
    "# output is the action to take (up, down, left, right)\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_size):\n",
    "        super(DQN, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=5, stride=2)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=5, stride=2)\n",
    "        self.conv3 = nn.Conv2d(32, 32, kernel_size=5, stride=2)\n",
    "        \n",
    "        def conv2d_size_out(size, kernel_size=5, stride=2):\n",
    "            return (size - (kernel_size - 1) - 1) // stride + 1\n",
    "        \n",
    "        convw = conv2d_size_out(conv2d_size_out(conv2d_size_out(input_size[1])))\n",
    "        convh = conv2d_size_out(conv2d_size_out(conv2d_size_out(input_size[2])))\n",
    "        linear_input_size = convw * convh * 32\n",
    "        \n",
    "        self.fc1 = nn.Linear(linear_input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.relu(self.fc1(x.reshape(x.size(0), -1)))\n",
    "        return self.fc2(x)\n",
    "        \n",
    "    def act(self, state, epsilon):\n",
    "        if random.random() > epsilon:\n",
    "            state = torch.FloatTensor(state).unsqueeze(0)\n",
    "            q_value = self.forward(state)\n",
    "            action = q_value.max(1)[1].data[0]\n",
    "        else:\n",
    "            action = random.randrange(self.output_size)\n",
    "        return action\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test passed\n"
     ]
    }
   ],
   "source": [
    "# Replay buffer for DQN\n",
    "# stores the transitions (state, action, reward, next_state, done)\n",
    "# and samples a batch of transitions for training\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.buffer = []\n",
    "        self.position = 0\n",
    "        \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        if len(self.buffer) < self.capacity:\n",
    "            self.buffer.append(None)\n",
    "        self.buffer[self.position] = (state, action, reward, next_state, done)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "        \n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        state, action, reward, next_state, done = map(np.stack, zip(*batch))\n",
    "        return state, action, reward, next_state, done\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "    \n",
    "    \n",
    "# DQN agent for Crossy Road game\n",
    "# uses DQN model and replay buffer for training\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self, input_size, output_size, hidden_size, replay_buffer_capacity, batch_size, gamma, epsilon_start, epsilon_end, epsilon_decay):\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.replay_buffer_capacity = replay_buffer_capacity\n",
    "        self.batch_size = batch_size\n",
    "        self.gamma = gamma\n",
    "        self.epsilon_start = epsilon_start\n",
    "        self.epsilon_end = epsilon_end\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        \n",
    "        self.model = DQN(input_size, output_size, hidden_size)\n",
    "        self.replay_buffer = ReplayBuffer(replay_buffer_capacity)\n",
    "        self.optimizer = optim.Adam(self.model.parameters())\n",
    "        \n",
    "        self.steps_done = 0\n",
    "        \n",
    "    def select_action(self, state):\n",
    "        epsilon = self.epsilon_end + (self.epsilon_start - self.epsilon_end) * np.exp(-1. * self.steps_done / self.epsilon_decay)\n",
    "        self.steps_done += 1\n",
    "        return self.model.act(state, epsilon)\n",
    "    \n",
    "    def optimize_model(self):\n",
    "        if len(self.replay_buffer) < self.batch_size:\n",
    "            return\n",
    "        \n",
    "        state, action, reward, next_state, done = self.replay_buffer.sample(self.batch_size)\n",
    "        state = torch.FloatTensor(np.float32(state))\n",
    "        action = torch.LongTensor(action)\n",
    "        reward = torch.FloatTensor(reward)\n",
    "        next_state = torch.FloatTensor(np.float32(next_state))\n",
    "        done = torch.FloatTensor(done)\n",
    "        \n",
    "        # print(state.shape, action.shape, reward.shape, next_state.shape, done.shape)\n",
    "        \n",
    "        next_state = next_state.permute(0, 3, 1, 2)\n",
    "        state = state.permute(0, 3, 1, 2)\n",
    "        \n",
    "        q_values = self.model(state)\n",
    "        next_q_values = self.model(next_state)\n",
    "        q_value = q_values.gather(1, action.unsqueeze(1)).squeeze(1)\n",
    "        next_q_value = next_q_values.max(1)[0]\n",
    "        expected_q_value = reward + self.gamma * next_q_value * (1 - done)\n",
    "        \n",
    "        loss = F.smooth_l1_loss(q_value, expected_q_value)\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        return loss.item()\n",
    "    \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        self.replay_buffer.push(state, action, reward, next_state, done)\n",
    "        \n",
    "    def save(self, path):\n",
    "        torch.save(self.model.state_dict(), path)\n",
    "        \n",
    "    def load(self, path):\n",
    "        self.model.load_state_dict(torch.load(path))\n",
    "        self.model.eval()\n",
    "        \n",
    "    def reset(self):\n",
    "        self.steps_done = 0\n",
    "        \n",
    "\n",
    "# test DQN agent\n",
    "agent = DQNAgent((3, 84, 84), 4, 128, 10000, 32, 0.99, 1.0, 0.1, 10000)\n",
    "state = np.random.rand(3, 84, 84)\n",
    "action = agent.select_action(state)\n",
    "reward = 1.0\n",
    "next_state = np.random.rand(3, 84, 84)\n",
    "done = 0\n",
    "agent.push(state, action, reward, next_state, done)\n",
    "loss = agent.optimize_model()\n",
    "agent.save('dqn.pth')\n",
    "agent.load('dqn.pth')\n",
    "agent.reset()\n",
    "print('test passed')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model is ready to train\n",
      "step: 0, loss: None, reward: 0.03911564350128183\n",
      "step: 1, loss: None, reward: 2.0767778158187866\n",
      "step: 2, loss: None, reward: 2.1142251968383787\n",
      "step: 3, loss: None, reward: 0.15172569751739506\n",
      "step: 4, loss: None, reward: 0.25\n",
      "step: 5, loss: None, reward: 1.4774322032928466\n",
      "step: 6, loss: None, reward: 1.5173386573791503\n",
      "step: 7, loss: 3.1072330474853516, reward: 2.3049256086349486\n",
      "step: 8, loss: 57.31833267211914, reward: 1.6109858751296997\n",
      "step: 9, loss: 52.83697509765625, reward: 2.4069194078445433\n",
      "step: 10, loss: 1.9456959962844849, reward: 0.25\n",
      "step: 11, loss: 6.626850605010986, reward: 0.25\n",
      "step: 12, loss: 5.461476802825928, reward: 2.5464776039123533\n",
      "step: 13, loss: 2.4603328704833984, reward: 1.8416924238204957\n",
      "step: 14, loss: 0.5790379643440247, reward: 0.6388439416885376\n",
      "step: 15, loss: 0.8058202862739563, reward: 0.25\n",
      "step: 16, loss: 0.5580230951309204, reward: 0.25\n",
      "step: 17, loss: 0.6754741072654724, reward: 2.777116870880127\n",
      "step: 18, loss: 1.157921552658081, reward: 2.0721599578857424\n",
      "step: 19, loss: 1.3403921127319336, reward: 2.116766405105591\n",
      "step: 20, loss: 1.354246735572815, reward: 2.9124890089035036\n",
      "step: 21, loss: 1.0806851387023926, reward: 2.9583415031433105\n",
      "step: 22, loss: 1.0207502841949463, reward: 1.0037668704986573\n",
      "step: 23, loss: 1.0016281604766846, reward: 1.0507195472717283\n",
      "step: 24, loss: 1.3148915767669678, reward: 2.3472583532333378\n",
      "step: 25, loss: 1.7317240238189697, reward: 0.25\n",
      "step: 26, loss: 1.1718181371688843, reward: 2.438297533988953\n",
      "step: 27, loss: 1.105305790901184, reward: 3.2343159675598145\n",
      "step: 28, loss: 1.0046076774597168, reward: 0.25\n",
      "step: 29, loss: 1.2729370594024658, reward: 1.324951100349426\n",
      "step: 30, loss: 0.9149747490882874, reward: 1.3702955484390262\n",
      "step: 31, loss: 1.6531288623809814, reward: 3.4160185098648075\n",
      "step: 32, loss: 1.1403299570083618, reward: 3.4632778644561766\n",
      "step: 33, loss: 1.434851884841919, reward: 2.759186124801636\n",
      "step: 34, loss: 1.123503565788269, reward: 2.804312133789063\n",
      "step: 35, loss: 1.5919214487075806, reward: 2.8502177715301515\n",
      "step: 36, loss: 1.2804349660873413, reward: 0.25\n",
      "step: 37, loss: 1.3112233877182007, reward: 0.25\n",
      "step: 38, loss: 1.3949123620986938, reward: 0.25\n",
      "step: 39, loss: 0.674501359462738, reward: 1.783482623100281\n",
      "step: 40, loss: 0.9107563495635986, reward: 3.08014407157898\n",
      "step: 41, loss: 1.2000919580459595, reward: 0.25\n",
      "step: 42, loss: 0.9808679819107056, reward: 0.25\n",
      "step: 43, loss: 1.1209720373153687, reward: 3.2181061267852784\n",
      "step: 44, loss: 1.3419644832611084, reward: 4.01335129737854\n",
      "step: 45, loss: 1.2313975095748901, reward: 3.3077757358551025\n",
      "step: 46, loss: 1.5653660297393799, reward: 0.25\n",
      "step: 47, loss: 1.2219042778015137, reward: 3.398844313621521\n",
      "step: 48, loss: 1.5572311878204346, reward: 0.25\n",
      "step: 49, loss: 1.2158501148223877, reward: 3.4898700952529906\n",
      "step: 50, loss: 1.008518934249878, reward: 0.25\n",
      "step: 51, loss: 0.6728351712226868, reward: 3.581481671333313\n",
      "step: 52, loss: 1.4143331050872803, reward: 3.6273929595947267\n",
      "step: 53, loss: 0.9318287968635559, reward: 2.422446417808533\n",
      "step: 54, loss: 0.904017448425293, reward: 4.46824357509613\n",
      "step: 55, loss: 1.288473129272461, reward: 4.51418924331665\n",
      "step: 56, loss: 1.4023690223693848, reward: 0.25\n",
      "step: 57, loss: 1.6356028318405151, reward: 0.25\n",
      "step: 58, loss: 2.4651806354522705, reward: 3.9023464918136597\n",
      "step: 59, loss: 2.1269519329071045, reward: 3.947816252708435\n",
      "step: 60, loss: 1.1338765621185303, reward: 0.25\n",
      "step: 61, loss: 1.784681797027588, reward: 0.25\n",
      "step: 62, loss: 1.3370884656906128, reward: 2.832367491722107\n",
      "step: 63, loss: 1.1903350353240967, reward: 4.1280090570449826\n",
      "step: 64, loss: 2.1435978412628174, reward: 0.25\n",
      "step: 65, loss: 1.5844006538391113, reward: 0.25\n",
      "step: 66, loss: 1.9885731935501099, reward: 5.015680980682373\n",
      "step: 67, loss: 1.596038818359375, reward: 3.0614641189575202\n",
      "step: 68, loss: 1.1277977228164673, reward: 0.25\n",
      "step: 69, loss: 0.9373121857643127, reward: 3.1521897554397587\n",
      "step: 70, loss: 1.6660239696502686, reward: 0.25\n",
      "step: 71, loss: 1.2584171295166016, reward: 4.494482707977295\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 90\u001b[0m\n\u001b[0;32m     87\u001b[0m action \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mselect_action(state)\n\u001b[0;32m     88\u001b[0m pyautogui\u001b[38;5;241m.\u001b[39mpress([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mup\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdown\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mright\u001b[39m\u001b[38;5;124m'\u001b[39m][action])\n\u001b[1;32m---> 90\u001b[0m \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.25\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     92\u001b[0m next_state \u001b[38;5;241m=\u001b[39m get_screen(GAME_REGION)\n\u001b[0;32m     94\u001b[0m reward_state \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     95\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstate\u001b[39m\u001b[38;5;124m'\u001b[39m: state,\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124maction\u001b[39m\u001b[38;5;124m'\u001b[39m: action,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     99\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtotal_reward\u001b[39m\u001b[38;5;124m'\u001b[39m: total_reward,\n\u001b[0;32m    100\u001b[0m }\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# run DQN agent on Crossy Road game to train and play the game\n",
    "import pyautogui\n",
    "import cv2\n",
    "import time\n",
    "import keyboard\n",
    "\n",
    "RES_X = 1920\n",
    "RES_Y = 1080\n",
    "\n",
    "GAME_REGION = (405, 210, 850, 480)\n",
    "\n",
    "restart_button = cv2.imread('restart_button.png', cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "# Init restart button image\n",
    "restart_button = cv2.imread('restart_button.png', cv2.IMREAD_GRAYSCALE)\n",
    "restart_button = cv2.normalize(restart_button, None, alpha=0, beta=1, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_32F)\n",
    "\n",
    "def get_screen(region):\n",
    "    screen = pyautogui.screenshot(region=(region[0], region[1], region[2], region[3]))\n",
    "    screen = np.array(screen)\n",
    "    screen = cv2.resize(screen, (425, 240))\n",
    "    # screen = np.moveaxis(screen, 2, 0)\n",
    "    return screen\n",
    "\n",
    "def is_game_over(image, score_threshold=0.9, scale=0.5):\n",
    "    # Check if the game is over by checking if the restart button is visible\n",
    "    grey_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    resized_screenshot = cv2.resize(grey_image, (0, 0), fx=scale, fy=scale)\n",
    "    resized_template = cv2.resize(restart_button, (0, 0), fx=scale, fy=scale)\n",
    "    h, w = resized_screenshot.shape\n",
    "    \n",
    "    resized_screenshot = resized_screenshot.astype(np.float32)\n",
    "    \n",
    "    cropped_search_box = resized_screenshot[int(h * 0.7):, int(w * 0.3):int(w * 0.7)]\n",
    "    result = cv2.matchTemplate(cropped_search_box, resized_template, cv2.TM_CCOEFF_NORMED)\n",
    "    locations = np.where(result >= score_threshold)\n",
    "    return len(locations[0]) > 0\n",
    "\n",
    "def compute_reward(reward_state):\n",
    "    # Compute reward based on the change in the game screen   \n",
    "    time = reward_state['time']\n",
    "    state = reward_state['state']\n",
    "    action = reward_state['action']\n",
    "    next_state = reward_state['next_state']\n",
    "    total_reward = reward_state['total_reward']\n",
    "    reward = 0\n",
    "    \n",
    "    if is_game_over(next_state):\n",
    "        reward = -100\n",
    "    else:\n",
    "        reward = 1\n",
    "        \n",
    "    reward += time * 0.1\n",
    "    \n",
    "    if action == 0:\n",
    "        reward += 1\n",
    "    elif action == 1:\n",
    "        reward -= 1\n",
    "    elif action == 2:\n",
    "        reward = 0.25\n",
    "    elif action == 3:\n",
    "        reward += 0.25\n",
    "    \n",
    "    return reward\n",
    "    \n",
    "\n",
    "# train DQN agent\n",
    "agent = DQNAgent((3, 425, 240), 4, 128, 1000, 8, 0.99, 1.0, 0.1, 10000)\n",
    "episodes = 1000\n",
    "episode_length = 1000\n",
    "losses = []\n",
    "rewards = []\n",
    "\n",
    "print(\"Model is ready to train\")\n",
    "# start the train after pressing 's' key\n",
    "keyboard.wait('s')\n",
    "\n",
    "for episode in range(episodes):\n",
    "    state = get_screen(GAME_REGION)\n",
    "    total_reward = 0\n",
    "    total_loss = 0\n",
    "    \n",
    "    \n",
    "    start_time = time.time()\n",
    "    for step in range(episode_length):\n",
    "        action = agent.select_action(state)\n",
    "        pyautogui.press(['up', 'down', 'left', 'right'][action])\n",
    "        \n",
    "        time.sleep(0.25)\n",
    "        \n",
    "        next_state = get_screen(GAME_REGION)\n",
    "        \n",
    "        reward_state = {\n",
    "            'state': state,\n",
    "            'action': action,\n",
    "            'next_state': next_state,\n",
    "            'time': time.time() - start_time,\n",
    "            'total_reward': total_reward,\n",
    "        }\n",
    "        \n",
    "        reward = compute_reward(reward_state)\n",
    "        done = 0\n",
    "        \n",
    "        agent.push(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "        \n",
    "        if is_game_over(state):\n",
    "            done = 1\n",
    "            agent.push(state, action, reward, next_state, done)\n",
    "            break\n",
    "        \n",
    "        loss = agent.optimize_model()\n",
    "        if loss is not None:\n",
    "            total_loss += loss\n",
    "        \n",
    "        print('step: {}, loss: {}, reward: {}'.format(step, loss, reward))\n",
    "    \n",
    "    # tap space key to restart the game\n",
    "    keyboard.press_and_release('space')\n",
    "    time.sleep(4)\n",
    "    keyboard.press_and_release('space')\n",
    "    time.sleep(1)\n",
    "            \n",
    "    losses.append(total_loss)\n",
    "    rewards.append(total_reward)\n",
    "    print('episode: {}, loss: {}, reward: {}'.format(episode, total_loss, total_reward))\n",
    "    agent.save('dqn.pth')\n",
    "    agent.reset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
