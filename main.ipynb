{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = torch.device(\"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DQN model for Crossy Road game\n",
    "# input is the 2d matrix of the game state (90 x 90)\n",
    "# output is the action to take (0, 1, 2, 3) for (up, down, left, right)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class RecurrentIQN(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_size, n_quantiles=32):\n",
    "        super(RecurrentIQN, self).__init__()\n",
    "        self.n_quantiles = n_quantiles\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
    "        self.quantile_embed = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x, quantiles, hidden):\n",
    "        # x shape: (batch_size, seq_len, input_size)\n",
    "        lstm_out, hidden = self.lstm(x, hidden)  # lstm_out: (batch_size, seq_len, hidden_size)\n",
    "        \n",
    "        # Expand quantiles to match lstm_out\n",
    "        quantiles = quantiles.unsqueeze(-1)  # (batch_size, n_quantiles, 1)\n",
    "        pi = torch.acos(torch.zeros(1)).item() * 2  # pi value\n",
    "        quantile_feats = torch.cos(pi * quantiles * torch.arange(1, self.hidden_size + 1).to(x.device))\n",
    "        quantile_feats = F.relu(self.quantile_embed(quantile_feats))  # (batch_size, n_quantiles, hidden_size)\n",
    "\n",
    "        # Combine LSTM output with quantile features\n",
    "        lstm_out = lstm_out[:, -1, :].unsqueeze(1)  # (batch_size, 1, hidden_size)\n",
    "        x = lstm_out * quantile_feats  # (batch_size, n_quantiles, hidden_size)\n",
    "\n",
    "        x = self.fc(x)  # (batch_size, n_quantiles, output_size)\n",
    "        return x, hidden\n",
    "\n",
    "    def act(self, state, hidden, epsilon):\n",
    "        if random.random() > epsilon:\n",
    "            print(\"Model acting\")\n",
    "            with torch.no_grad():\n",
    "                # Prepare the state tensor\n",
    "                state = torch.FloatTensor(state).unsqueeze(0).unsqueeze(0).to(next(self.parameters()).device)  # (1, 1, input_size)\n",
    "                # Sample quantiles\n",
    "                quantiles = torch.rand(1, self.n_quantiles).to(state.device)  # (1, n_quantiles)\n",
    "                # Forward pass\n",
    "                q_values, hidden = self.forward(state, quantiles, hidden)  # q_values: (1, n_quantiles, output_size)\n",
    "                # Average over quantiles\n",
    "                q_values = q_values.mean(dim=1)  # (1, output_size)\n",
    "                # Select action with highest Q-value\n",
    "                action = q_values.argmax(dim=1).item()\n",
    "        else:\n",
    "            # Random action\n",
    "            action = random.randrange(self.fc.out_features)\n",
    "        return action, hidden\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.buffer = []\n",
    "        self.position = 0\n",
    "        \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        if len(self.buffer) < self.capacity:\n",
    "            self.buffer.append(None)\n",
    "        self.buffer[self.position] = (state, action, reward, next_state, done)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "        \n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        state, action, reward, next_state, done = map(np.stack, zip(*batch))\n",
    "        return state, action, reward, next_state, done\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "    \n",
    "    \n",
    "# DQN agent for Crossy Road game\n",
    "# uses DQN model and replay buffer for training\n",
    "\n",
    "\n",
    "class IQNAgent:\n",
    "    def __init__(self, input_size, output_size, hidden_size, replay_buffer_capacity, batch_size, gamma, epsilon_start, epsilon_end, epsilon_decay, n_quantiles=32):\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.replay_buffer_capacity = replay_buffer_capacity\n",
    "        self.batch_size = batch_size\n",
    "        self.gamma = gamma\n",
    "        self.epsilon_start = epsilon_start\n",
    "        self.epsilon_end = epsilon_end\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.n_quantiles = n_quantiles\n",
    "\n",
    "        self.model = RecurrentIQN(input_size, output_size, hidden_size, n_quantiles).to(device)\n",
    "        self.model_target = RecurrentIQN(input_size, output_size, hidden_size, n_quantiles).to(device)\n",
    "        self.model_target.load_state_dict(self.model.state_dict())\n",
    "\n",
    "        self.hidden = (torch.zeros(1, 1, hidden_size).to(device),\n",
    "                       torch.zeros(1, 1, hidden_size).to(device))\n",
    "\n",
    "        self.replay_buffer = ReplayBuffer(replay_buffer_capacity)\n",
    "        self.optimizer = optim.Adam(self.model.parameters())\n",
    "\n",
    "        self.steps_done = 0\n",
    "\n",
    "        # Add counters for target network updates\n",
    "        self.update_counter = 0\n",
    "        self.target_update_freq = 1000  # Update target network every 1000 steps\n",
    "\n",
    "    def select_action(self, state):\n",
    "        epsilon = self.epsilon_end + (self.epsilon_start - self.epsilon_end) * np.exp(-1. * self.steps_done / self.epsilon_decay)\n",
    "        self.steps_done += 1\n",
    "        action, self.hidden = self.model.act(state, self.hidden, epsilon)\n",
    "        return action\n",
    "\n",
    "    def optimize_model(self):\n",
    "        if len(self.replay_buffer) < self.batch_size:\n",
    "            return\n",
    "        \n",
    "        self.model.train()\n",
    "        print(\"Optimizing model...\")\n",
    "        states, actions, rewards, next_states, dones = self.replay_buffer.sample(self.batch_size)\n",
    "\n",
    "        # Convert to tensors\n",
    "        states = torch.FloatTensor(states).unsqueeze(1).to(device)\n",
    "        actions = torch.LongTensor(actions).to(device)\n",
    "        rewards = torch.FloatTensor(rewards).to(device)\n",
    "        next_states = torch.FloatTensor(next_states).unsqueeze(1).to(device)\n",
    "        dones = torch.FloatTensor(dones).to(device)\n",
    "\n",
    "        # Sample quantiles\n",
    "        quantiles = torch.rand(self.batch_size, self.n_quantiles).to(device)\n",
    "\n",
    "        # Initialize hidden states\n",
    "        hidden = (torch.zeros(1, self.batch_size, self.hidden_size).to(device),\n",
    "                  torch.zeros(1, self.batch_size, self.hidden_size).to(device))\n",
    "\n",
    "        # Compute current Q-values\n",
    "        current_q, _ = self.model(states, quantiles, hidden)\n",
    "        current_q = current_q.gather(2, actions.unsqueeze(-1).unsqueeze(-1).expand(-1, self.n_quantiles, -1)).squeeze(-1)\n",
    "\n",
    "        # Compute target Q-values\n",
    "        with torch.no_grad():\n",
    "            next_hidden = (torch.zeros(1, self.batch_size, self.hidden_size).to(device),\n",
    "                           torch.zeros(1, self.batch_size, self.hidden_size).to(device))\n",
    "            next_quantiles = torch.rand(self.batch_size, self.n_quantiles).to(device)\n",
    "            next_q, _ = self.model_target(next_states, next_quantiles, next_hidden)\n",
    "            next_q = next_q.max(2)[0]\n",
    "            target_q = rewards.unsqueeze(1) + self.gamma * next_q * (1 - dones.unsqueeze(1))\n",
    "\n",
    "        # Compute quantile Huber loss\n",
    "        td_errors = target_q.unsqueeze(1) - current_q\n",
    "        huber_loss = F.smooth_l1_loss(current_q, target_q.unsqueeze(1), reduction='none')\n",
    "        quantile_loss = (torch.abs(quantiles.unsqueeze(-1) - (td_errors.detach() < 0).float()) * huber_loss).mean()\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        quantile_loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # Update target network periodically\n",
    "        self.update_counter += 1\n",
    "        if self.update_counter % self.target_update_freq == 0:\n",
    "            self.update_target_network()\n",
    "\n",
    "        return quantile_loss.item()\n",
    "\n",
    "    def update_target_network(self):\n",
    "        self.model_target.load_state_dict(self.model.state_dict())\n",
    "\n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        self.replay_buffer.push(state, action, reward, next_state, done)\n",
    "\n",
    "    def save(self, path):\n",
    "        torch.save(self.model.state_dict(), path)\n",
    "\n",
    "    def load(self, path):\n",
    "        self.model.load_state_dict(torch.load(path))\n",
    "        self.model.eval()\n",
    "\n",
    "    def reset(self):\n",
    "        self.steps_done = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simplify_state(state):\n",
    "    # Flatten the grid to create a fixed-size input vector\n",
    "    agent_x, agent_y = get_agent_position(state)\n",
    "    obstacles = get_nearby_obstacles(state, agent_x, agent_y)\n",
    "    timbers = get_nearby_timbers(state, agent_x, agent_y)\n",
    "    \n",
    "    \n",
    "    \n",
    "    simplified_state = ((agent_x, agent_y), tuple(set(obstacles)), tuple(set(timbers)))\n",
    "    \n",
    "    return simplified_state\n",
    "\n",
    "def get_agent_position(state):\n",
    "    for i, row in enumerate(state):\n",
    "        for j, cell in enumerate(row):\n",
    "            if cell == 1:\n",
    "                return j, i\n",
    "    return state.shape[1] // 2, state.shape[0] - 1\n",
    "    \n",
    "            \n",
    "def get_nearby_obstacles(state, agent_x, agent_y):\n",
    "    obstacles = []\n",
    "    for i, row in enumerate(state):\n",
    "        for j, cell in enumerate(row):\n",
    "            if cell == 2:\n",
    "                obstacles.append((j, i))\n",
    "    return obstacles\n",
    "\n",
    "def get_nearby_timbers(state, agent_x, agent_y):\n",
    "    timbers = []\n",
    "    for i, row in enumerate(state):\n",
    "        for j, cell in enumerate(row):\n",
    "            if cell == 3:\n",
    "                timbers.append((j, i))\n",
    "    return timbers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model is ready to train\n",
      "Step: 0, Action: 2, Reward: 0.25, Total Reward: 0.25, Loss: None\n",
      "Step: 1, Action: 0, Reward: 10.0, Total Reward: 10.25, Loss: None\n",
      "Step: 2, Action: 3, Reward: 0.25, Total Reward: 10.5, Loss: None\n",
      "Step: 3, Action: 1, Reward: -5.0, Total Reward: 5.5, Loss: None\n",
      "Step: 4, Action: 3, Reward: 0.25, Total Reward: 5.75, Loss: None\n",
      "Step: 5, Action: 4, Reward: 0.2, Total Reward: 5.95, Loss: None\n",
      "Step: 6, Action: 1, Reward: -5.0, Total Reward: 0.9500000000000002, Loss: None\n",
      "Step: 7, Action: 4, Reward: 0.2, Total Reward: 1.1500000000000001, Loss: None\n",
      "Step: 8, Action: 1, Reward: -5.0, Total Reward: -3.8499999999999996, Loss: None\n",
      "Step: 9, Action: 1, Reward: -5.0, Total Reward: -8.85, Loss: None\n",
      "Step: 10, Action: 2, Reward: 0.25, Total Reward: -8.6, Loss: None\n",
      "Step: 11, Action: 4, Reward: 0.2, Total Reward: -8.4, Loss: None\n",
      "Step: 12, Action: 1, Reward: -5.0, Total Reward: -13.4, Loss: None\n",
      "Step: 13, Action: 3, Reward: 0.25, Total Reward: -13.15, Loss: None\n",
      "Step: 14, Action: 1, Reward: -5.0, Total Reward: -18.15, Loss: None\n",
      "\n",
      "episode: 0, reward: -118.15\n",
      "Model saved\n",
      "Step: 0, Action: 1, Reward: -5.0, Total Reward: -5.0, Loss: None\n",
      "Step: 1, Action: 4, Reward: 0.2, Total Reward: -4.8, Loss: None\n",
      "Step: 2, Action: 0, Reward: 10.0, Total Reward: 5.2, Loss: None\n",
      "Step: 3, Action: 2, Reward: 0.25, Total Reward: 5.45, Loss: None\n",
      "Step: 4, Action: 4, Reward: 0.2, Total Reward: 5.65, Loss: None\n",
      "Step: 5, Action: 3, Reward: 0.25, Total Reward: 5.9, Loss: None\n",
      "Step: 6, Action: 2, Reward: 0.25, Total Reward: 6.15, Loss: None\n",
      "Step: 7, Action: 2, Reward: 0.25, Total Reward: 6.4, Loss: None\n",
      "Model acting\n",
      "Step: 8, Action: 2, Reward: 0.25, Total Reward: 6.65, Loss: None\n",
      "Step: 9, Action: 4, Reward: 0.2, Total Reward: 6.8500000000000005, Loss: None\n",
      "Step: 10, Action: 4, Reward: -9.8, Total Reward: -2.95, Loss: None\n",
      "Step: 11, Action: 4, Reward: -9.8, Total Reward: -12.75, Loss: None\n",
      "Step: 12, Action: 0, Reward: 10.0, Total Reward: -2.75, Loss: None\n",
      "Step: 13, Action: 2, Reward: 0.25, Total Reward: -2.5, Loss: None\n",
      "Step: 14, Action: 0, Reward: 10.0, Total Reward: 7.5, Loss: None\n",
      "Optimizing model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ikkiren\\AppData\\Local\\Temp\\ipykernel_29512\\3007061189.py:98: UserWarning: Using a target size (torch.Size([32, 1, 32])) that is different to the input size (torch.Size([32, 32])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  huber_loss = F.smooth_l1_loss(current_q, target_q.unsqueeze(1), reduction='none')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 15, Action: 4, Reward: 0.2, Total Reward: 7.7, Loss: 2.8623082637786865\n",
      "Optimizing model...\n",
      "Step: 16, Action: 3, Reward: 0.25, Total Reward: 7.95, Loss: 2.9446310997009277\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 17, Action: 0, Reward: 10.0, Total Reward: 17.95, Loss: 2.831799030303955\n",
      "Optimizing model...\n",
      "Step: 18, Action: 2, Reward: 0.25, Total Reward: 18.2, Loss: 2.999375104904175\n",
      "Optimizing model...\n",
      "Step: 19, Action: 3, Reward: 0.25, Total Reward: 18.45, Loss: 3.105541467666626\n",
      "Optimizing model...\n",
      "Step: 20, Action: 1, Reward: -5.0, Total Reward: 13.45, Loss: 3.0252881050109863\n",
      "Optimizing model...\n",
      "Step: 21, Action: 2, Reward: 0.25, Total Reward: 13.7, Loss: 2.8815200328826904\n",
      "Optimizing model...\n",
      "Step: 22, Action: 4, Reward: 0.2, Total Reward: 13.899999999999999, Loss: 2.856795310974121\n",
      "Optimizing model...\n",
      "Step: 23, Action: 1, Reward: -5.0, Total Reward: 8.899999999999999, Loss: 1.6173111200332642\n",
      "Optimizing model...\n",
      "Step: 24, Action: 1, Reward: -15.0, Total Reward: -6.100000000000001, Loss: 2.917781114578247\n",
      "Optimizing model...\n",
      "Step: 25, Action: 2, Reward: 0.25, Total Reward: -5.850000000000001, Loss: 1.8020951747894287\n",
      "Optimizing model...\n",
      "Step: 26, Action: 0, Reward: 10.0, Total Reward: 4.149999999999999, Loss: 3.0387673377990723\n",
      "Optimizing model...\n",
      "Step: 27, Action: 4, Reward: 0.2, Total Reward: 4.349999999999999, Loss: 2.9071860313415527\n",
      "Optimizing model...\n",
      "Step: 28, Action: 0, Reward: 10.0, Total Reward: 14.349999999999998, Loss: 3.041501998901367\n",
      "Optimizing model...\n",
      "Step: 29, Action: 0, Reward: 0.0, Total Reward: 14.349999999999998, Loss: 3.281963348388672\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 30, Action: 4, Reward: 0.2, Total Reward: 14.549999999999997, Loss: 3.205296516418457\n",
      "\n",
      "episode: 1, reward: -85.45\n",
      "Optimizing model...\n",
      "Step: 0, Action: 3, Reward: 0.25, Total Reward: 0.25, Loss: 3.6340513229370117\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: 10.0, Total Reward: 10.25, Loss: 2.9241092205047607\n",
      "Optimizing model...\n",
      "Step: 2, Action: 3, Reward: 0.25, Total Reward: 10.5, Loss: 4.634794235229492\n",
      "Optimizing model...\n",
      "Step: 3, Action: 1, Reward: -5.0, Total Reward: 5.5, Loss: 2.9799845218658447\n",
      "Optimizing model...\n",
      "Step: 4, Action: 3, Reward: 0.25, Total Reward: 5.75, Loss: 2.609783172607422\n",
      "Optimizing model...\n",
      "Step: 5, Action: 1, Reward: -5.0, Total Reward: 0.75, Loss: 3.2794618606567383\n",
      "Optimizing model...\n",
      "Step: 6, Action: 0, Reward: 10.0, Total Reward: 10.75, Loss: 5.3835601806640625\n",
      "Optimizing model...\n",
      "Step: 7, Action: 1, Reward: -5.0, Total Reward: 5.75, Loss: 2.91878604888916\n",
      "Optimizing model...\n",
      "Step: 8, Action: 1, Reward: -15.0, Total Reward: -9.25, Loss: 4.710127353668213\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 9, Action: 2, Reward: 0.25, Total Reward: -9.0, Loss: 4.859112739562988\n",
      "Optimizing model...\n",
      "Step: 10, Action: 1, Reward: -5.0, Total Reward: -14.0, Loss: 2.811764717102051\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 11, Action: 4, Reward: 0.2, Total Reward: -13.8, Loss: 2.7086448669433594\n",
      "Optimizing model...\n",
      "Step: 12, Action: 3, Reward: 0.25, Total Reward: -13.55, Loss: 5.007763385772705\n",
      "Optimizing model...\n",
      "Step: 13, Action: 0, Reward: 10.0, Total Reward: -3.5500000000000007, Loss: 3.4783544540405273\n",
      "\n",
      "episode: 2, reward: -103.55\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: 10.0, Total Reward: 10.0, Loss: 4.714488983154297\n",
      "Optimizing model...\n",
      "Step: 1, Action: 1, Reward: -5.0, Total Reward: 5.0, Loss: 3.551927089691162\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 4, Reward: 0.2, Total Reward: 5.2, Loss: 2.964259624481201\n",
      "Optimizing model...\n",
      "Step: 3, Action: 4, Reward: 0.2, Total Reward: 5.4, Loss: 4.539052963256836\n",
      "Optimizing model...\n",
      "Step: 4, Action: 4, Reward: -9.8, Total Reward: -4.4, Loss: 5.911111831665039\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 5, Action: 2, Reward: 0.25, Total Reward: -4.15, Loss: 2.666529417037964\n",
      "Optimizing model...\n",
      "Step: 6, Action: 4, Reward: 0.2, Total Reward: -3.95, Loss: 1.7865052223205566\n",
      "Optimizing model...\n",
      "Step: 7, Action: 3, Reward: 0.25, Total Reward: -3.7, Loss: 4.403172492980957\n",
      "Optimizing model...\n",
      "Step: 8, Action: 2, Reward: 0.25, Total Reward: -3.45, Loss: 5.231405735015869\n",
      "Optimizing model...\n",
      "Step: 9, Action: 4, Reward: 0.2, Total Reward: -3.25, Loss: 3.0520389080047607\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 10, Action: 2, Reward: 0.25, Total Reward: -3.0, Loss: 2.9213032722473145\n",
      "Optimizing model...\n",
      "Step: 11, Action: 4, Reward: 0.2, Total Reward: -2.8, Loss: 2.354856014251709\n",
      "Optimizing model...\n",
      "Step: 12, Action: 2, Reward: 0.25, Total Reward: -2.55, Loss: 5.064969539642334\n",
      "Optimizing model...\n",
      "Step: 13, Action: 0, Reward: 10.0, Total Reward: 7.45, Loss: 2.882206439971924\n",
      "Optimizing model...\n",
      "Step: 14, Action: 3, Reward: 0.25, Total Reward: 7.7, Loss: 1.511094570159912\n",
      "Optimizing model...\n",
      "Step: 15, Action: 1, Reward: -5.0, Total Reward: 2.7, Loss: 2.927661180496216\n",
      "Optimizing model...\n",
      "Step: 16, Action: 2, Reward: 0.25, Total Reward: 2.95, Loss: 2.177269458770752\n",
      "Optimizing model...\n",
      "Step: 17, Action: 4, Reward: 0.2, Total Reward: 3.1500000000000004, Loss: 2.899477243423462\n",
      "Optimizing model...\n",
      "Step: 18, Action: 2, Reward: 0.25, Total Reward: 3.4000000000000004, Loss: 4.167463779449463\n",
      "Optimizing model...\n",
      "Step: 19, Action: 2, Reward: -9.75, Total Reward: -6.35, Loss: 4.2741899490356445\n",
      "Optimizing model...\n",
      "Step: 20, Action: 1, Reward: -5.0, Total Reward: -11.35, Loss: 3.156299591064453\n",
      "\n",
      "episode: 3, reward: -111.35\n",
      "Optimizing model...\n",
      "Step: 0, Action: 3, Reward: 0.25, Total Reward: 0.25, Loss: 5.03472900390625\n",
      "Optimizing model...\n",
      "Step: 1, Action: 2, Reward: 0.25, Total Reward: 0.5, Loss: 4.844753265380859\n",
      "Optimizing model...\n",
      "Step: 2, Action: 3, Reward: 0.25, Total Reward: 0.75, Loss: 4.830392837524414\n",
      "Optimizing model...\n",
      "Step: 3, Action: 3, Reward: 0.25, Total Reward: 1.0, Loss: 3.2430601119995117\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: 10.0, Total Reward: 11.0, Loss: 4.356437683105469\n",
      "Optimizing model...\n",
      "Step: 5, Action: 1, Reward: -5.0, Total Reward: 6.0, Loss: 3.1590142250061035\n",
      "Optimizing model...\n",
      "Step: 6, Action: 3, Reward: 0.25, Total Reward: 6.25, Loss: 2.9872841835021973\n",
      "Optimizing model...\n",
      "Step: 7, Action: 2, Reward: 0.25, Total Reward: 6.5, Loss: 1.4680721759796143\n",
      "Optimizing model...\n",
      "Step: 8, Action: 2, Reward: 0.25, Total Reward: 6.75, Loss: 1.587172269821167\n",
      "Optimizing model...\n",
      "Step: 9, Action: 3, Reward: 0.25, Total Reward: 7.0, Loss: 1.5932023525238037\n",
      "Optimizing model...\n",
      "Step: 10, Action: 0, Reward: 10.0, Total Reward: 17.0, Loss: 1.4421088695526123\n",
      "Optimizing model...\n",
      "Step: 11, Action: 0, Reward: 10.0, Total Reward: 27.0, Loss: 4.703156471252441\n",
      "Optimizing model...\n",
      "Step: 12, Action: 2, Reward: 0.25, Total Reward: 27.25, Loss: 5.374295234680176\n",
      "Optimizing model...\n",
      "Step: 13, Action: 1, Reward: -5.0, Total Reward: 22.25, Loss: 2.7033276557922363\n",
      "Optimizing model...\n",
      "Step: 14, Action: 3, Reward: 0.25, Total Reward: 22.5, Loss: 4.465939521789551\n",
      "Optimizing model...\n",
      "Step: 15, Action: 3, Reward: -9.75, Total Reward: 12.75, Loss: 4.214886665344238\n",
      "Optimizing model...\n",
      "Step: 16, Action: 3, Reward: -9.75, Total Reward: 3.0, Loss: 2.7785959243774414\n",
      "Optimizing model...\n",
      "Step: 17, Action: 2, Reward: 0.25, Total Reward: 3.25, Loss: 4.2310662269592285\n",
      "Optimizing model...\n",
      "Step: 18, Action: 0, Reward: 10.0, Total Reward: 13.25, Loss: 3.0981531143188477\n",
      "Optimizing model...\n",
      "Step: 19, Action: 4, Reward: 0.2, Total Reward: 13.45, Loss: 3.080446243286133\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 20, Action: 0, Reward: 10.0, Total Reward: 23.45, Loss: 1.2895599603652954\n",
      "Optimizing model...\n",
      "Step: 21, Action: 0, Reward: 10.0, Total Reward: 33.45, Loss: 4.5904998779296875\n",
      "Optimizing model...\n",
      "Step: 22, Action: 4, Reward: 0.2, Total Reward: 33.650000000000006, Loss: 5.289303779602051\n",
      "Optimizing model...\n",
      "Step: 23, Action: 4, Reward: -9.8, Total Reward: 23.850000000000005, Loss: 5.974337577819824\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 24, Action: 0, Reward: 10.0, Total Reward: 33.85000000000001, Loss: 1.8479002714157104\n",
      "Optimizing model...\n",
      "Step: 25, Action: 3, Reward: 0.25, Total Reward: 34.10000000000001, Loss: 3.158719301223755\n",
      "Optimizing model...\n",
      "Step: 26, Action: 4, Reward: 0.2, Total Reward: 34.30000000000001, Loss: 2.79394268989563\n",
      "Optimizing model...\n",
      "Step: 27, Action: 4, Reward: -9.8, Total Reward: 24.50000000000001, Loss: 3.046698570251465\n",
      "Optimizing model...\n",
      "Step: 28, Action: 0, Reward: 10.0, Total Reward: 34.500000000000014, Loss: 3.549905300140381\n",
      "Optimizing model...\n",
      "Step: 29, Action: 1, Reward: -5.0, Total Reward: 29.500000000000014, Loss: 5.263884544372559\n",
      "Optimizing model...\n",
      "Step: 30, Action: 1, Reward: -15.0, Total Reward: 14.500000000000014, Loss: 4.857144832611084\n",
      "Optimizing model...\n",
      "Step: 31, Action: 4, Reward: 0.2, Total Reward: 14.700000000000014, Loss: 3.0479893684387207\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 32, Action: 3, Reward: 0.25, Total Reward: 14.950000000000014, Loss: 1.9438846111297607\n",
      "Optimizing model...\n",
      "Step: 33, Action: 2, Reward: 0.25, Total Reward: 15.200000000000014, Loss: 3.0039072036743164\n",
      "Optimizing model...\n",
      "Step: 34, Action: 1, Reward: -5.0, Total Reward: 10.200000000000014, Loss: 1.6100729703903198\n",
      "\n",
      "episode: 4, reward: -89.79999999999998\n",
      "\n",
      "episode: 5, reward: -100\n",
      "Optimizing model...\n",
      "Step: 0, Action: 4, Reward: 0.2, Total Reward: 0.2, Loss: 5.878052711486816\n",
      "Optimizing model...\n",
      "Step: 1, Action: 4, Reward: -9.8, Total Reward: -9.600000000000001, Loss: 6.183846473693848\n",
      "Optimizing model...\n",
      "Step: 2, Action: 2, Reward: 0.25, Total Reward: -9.350000000000001, Loss: 3.6091673374176025\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 3, Reward: 0.25, Total Reward: -9.100000000000001, Loss: 2.7610490322113037\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 4, Action: 4, Reward: 0.2, Total Reward: -8.900000000000002, Loss: 1.7795428037643433\n",
      "Optimizing model...\n",
      "Step: 5, Action: 2, Reward: 0.25, Total Reward: -8.650000000000002, Loss: 3.3917949199676514\n",
      "Optimizing model...\n",
      "Step: 6, Action: 3, Reward: 0.25, Total Reward: -8.400000000000002, Loss: 7.366135120391846\n",
      "Optimizing model...\n",
      "Step: 7, Action: 0, Reward: 10.0, Total Reward: 1.5999999999999979, Loss: 4.974168300628662\n",
      "Optimizing model...\n",
      "Step: 8, Action: 2, Reward: 0.25, Total Reward: 1.8499999999999979, Loss: 5.121280670166016\n",
      "Optimizing model...\n",
      "Step: 9, Action: 2, Reward: 0.25, Total Reward: 2.099999999999998, Loss: 2.630763053894043\n",
      "Optimizing model...\n",
      "Step: 10, Action: 2, Reward: 0.25, Total Reward: 2.349999999999998, Loss: 4.647487640380859\n",
      "Optimizing model...\n",
      "Step: 11, Action: 1, Reward: -5.0, Total Reward: -2.650000000000002, Loss: 4.963366508483887\n",
      "Optimizing model...\n",
      "Step: 12, Action: 1, Reward: -15.0, Total Reward: -17.650000000000002, Loss: 4.109914779663086\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 13, Action: 4, Reward: 0.2, Total Reward: -17.450000000000003, Loss: 4.774435997009277\n",
      "Optimizing model...\n",
      "Step: 14, Action: 3, Reward: 0.25, Total Reward: -17.200000000000003, Loss: 3.4825735092163086\n",
      "Optimizing model...\n",
      "Step: 15, Action: 4, Reward: 0.2, Total Reward: -17.000000000000004, Loss: 1.7472083568572998\n",
      "Optimizing model...\n",
      "Step: 16, Action: 4, Reward: 0.2, Total Reward: -16.800000000000004, Loss: 6.201664924621582\n",
      "Optimizing model...\n",
      "Step: 17, Action: 2, Reward: 0.25, Total Reward: -16.550000000000004, Loss: 1.569617748260498\n",
      "Optimizing model...\n",
      "Step: 18, Action: 4, Reward: 0.2, Total Reward: -16.350000000000005, Loss: 4.919367790222168\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 19, Action: 0, Reward: 10.0, Total Reward: -6.350000000000005, Loss: 4.0154242515563965\n",
      "\n",
      "episode: 6, reward: -106.35000000000001\n",
      "Optimizing model...\n",
      "Step: 0, Action: 1, Reward: -5.0, Total Reward: -5.0, Loss: 2.824760675430298\n",
      "Optimizing model...\n",
      "Step: 1, Action: 2, Reward: 0.25, Total Reward: -4.75, Loss: 4.205724239349365\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: 10.0, Total Reward: 5.25, Loss: 4.505321025848389\n",
      "Optimizing model...\n",
      "Step: 3, Action: 2, Reward: 0.25, Total Reward: 5.5, Loss: 5.059523582458496\n",
      "Optimizing model...\n",
      "Step: 4, Action: 3, Reward: 0.25, Total Reward: 5.75, Loss: 5.1068267822265625\n",
      "Optimizing model...\n",
      "Step: 5, Action: 0, Reward: 10.0, Total Reward: 15.75, Loss: 5.27602481842041\n",
      "Optimizing model...\n",
      "Step: 6, Action: 0, Reward: 10.0, Total Reward: 25.75, Loss: 3.5281364917755127\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 7, Action: 0, Reward: 0.0, Total Reward: 25.75, Loss: 3.325305223464966\n",
      "Optimizing model...\n",
      "Step: 8, Action: 2, Reward: 0.25, Total Reward: 26.0, Loss: 1.1535592079162598\n",
      "Optimizing model...\n",
      "Step: 9, Action: 2, Reward: 0.25, Total Reward: 26.25, Loss: 6.317648887634277\n",
      "Optimizing model...\n",
      "Step: 10, Action: 2, Reward: 0.25, Total Reward: 26.5, Loss: 3.1690447330474854\n",
      "Optimizing model...\n",
      "Step: 11, Action: 2, Reward: -9.75, Total Reward: 16.75, Loss: 4.388185501098633\n",
      "Optimizing model...\n",
      "Step: 12, Action: 1, Reward: -5.0, Total Reward: 11.75, Loss: 3.394916534423828\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 13, Action: 0, Reward: 10.0, Total Reward: 21.75, Loss: 2.464301586151123\n",
      "Optimizing model...\n",
      "Step: 14, Action: 3, Reward: 0.25, Total Reward: 22.0, Loss: 6.081682205200195\n",
      "Optimizing model...\n",
      "Step: 15, Action: 3, Reward: 0.25, Total Reward: 22.25, Loss: 1.5056604146957397\n",
      "Optimizing model...\n",
      "Step: 16, Action: 1, Reward: -5.0, Total Reward: 17.25, Loss: 3.547020435333252\n",
      "Optimizing model...\n",
      "Step: 17, Action: 4, Reward: 0.2, Total Reward: 17.45, Loss: 3.11118221282959\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 18, Action: 0, Reward: 10.0, Total Reward: 27.45, Loss: 4.924842834472656\n",
      "Optimizing model...\n",
      "Step: 19, Action: 0, Reward: 0.0, Total Reward: 27.45, Loss: 2.2055022716522217\n",
      "Optimizing model...\n",
      "Step: 20, Action: 1, Reward: -5.0, Total Reward: 22.45, Loss: 6.104203224182129\n",
      "\n",
      "episode: 7, reward: -77.55\n",
      "Optimizing model...\n",
      "Step: 0, Action: 3, Reward: 0.25, Total Reward: 0.25, Loss: 1.486817479133606\n",
      "Optimizing model...\n",
      "Step: 1, Action: 2, Reward: 0.25, Total Reward: 0.5, Loss: 2.1011648178100586\n",
      "Optimizing model...\n",
      "Step: 2, Action: 2, Reward: 0.25, Total Reward: 0.75, Loss: 3.0233592987060547\n",
      "Optimizing model...\n",
      "Step: 3, Action: 3, Reward: 0.25, Total Reward: 1.0, Loss: 1.9055488109588623\n",
      "Optimizing model...\n",
      "Step: 4, Action: 4, Reward: 0.2, Total Reward: 1.2, Loss: 5.095688819885254\n",
      "Optimizing model...\n",
      "Step: 5, Action: 1, Reward: -5.0, Total Reward: -3.8, Loss: 3.04982328414917\n",
      "Optimizing model...\n",
      "Step: 6, Action: 3, Reward: 0.25, Total Reward: -3.55, Loss: 1.1593036651611328\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 7, Action: 0, Reward: 10.0, Total Reward: 6.45, Loss: 3.362053394317627\n",
      "Optimizing model...\n",
      "Step: 8, Action: 3, Reward: 0.25, Total Reward: 6.7, Loss: 2.999046802520752\n",
      "Optimizing model...\n",
      "Step: 9, Action: 2, Reward: 0.25, Total Reward: 6.95, Loss: 1.8361254930496216\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 10, Action: 0, Reward: 10.0, Total Reward: 16.95, Loss: 3.9600865840911865\n",
      "Optimizing model...\n",
      "Step: 11, Action: 2, Reward: 0.25, Total Reward: 17.2, Loss: 4.485125541687012\n",
      "Optimizing model...\n",
      "Step: 12, Action: 2, Reward: 0.25, Total Reward: 17.45, Loss: 4.919897079467773\n",
      "Optimizing model...\n",
      "Step: 13, Action: 4, Reward: 0.2, Total Reward: 17.65, Loss: 1.5624239444732666\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 14, Action: 0, Reward: 10.0, Total Reward: 27.65, Loss: 5.289671897888184\n",
      "Optimizing model...\n",
      "Step: 15, Action: 4, Reward: 0.2, Total Reward: 27.849999999999998, Loss: 3.233120918273926\n",
      "Optimizing model...\n",
      "Step: 16, Action: 4, Reward: -9.8, Total Reward: 18.049999999999997, Loss: 3.054234266281128\n",
      "Optimizing model...\n",
      "Step: 17, Action: 3, Reward: 0.25, Total Reward: 18.299999999999997, Loss: 1.783718466758728\n",
      "Optimizing model...\n",
      "Step: 18, Action: 2, Reward: 0.25, Total Reward: 18.549999999999997, Loss: 2.791142463684082\n",
      "Optimizing model...\n",
      "Step: 19, Action: 1, Reward: -5.0, Total Reward: 13.549999999999997, Loss: 3.0533406734466553\n",
      "Optimizing model...\n",
      "Step: 20, Action: 4, Reward: 0.2, Total Reward: 13.749999999999996, Loss: 4.959752082824707\n",
      "Optimizing model...\n",
      "Step: 21, Action: 1, Reward: -5.0, Total Reward: 8.749999999999996, Loss: 2.982067108154297\n",
      "Optimizing model...\n",
      "Step: 22, Action: 0, Reward: 10.0, Total Reward: 18.749999999999996, Loss: 2.8225111961364746\n",
      "Optimizing model...\n",
      "Step: 23, Action: 0, Reward: 0.0, Total Reward: 18.749999999999996, Loss: 2.7132816314697266\n",
      "Optimizing model...\n",
      "Step: 24, Action: 0, Reward: 0.0, Total Reward: 18.749999999999996, Loss: 3.012183666229248\n",
      "\n",
      "episode: 8, reward: -81.25\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: 10.0, Total Reward: 10.0, Loss: 6.938181400299072\n",
      "Optimizing model...\n",
      "Step: 1, Action: 3, Reward: 0.25, Total Reward: 10.25, Loss: 3.0535097122192383\n",
      "Optimizing model...\n",
      "Step: 2, Action: 1, Reward: -5.0, Total Reward: 5.25, Loss: 1.5523115396499634\n",
      "Optimizing model...\n",
      "Step: 3, Action: 4, Reward: 0.2, Total Reward: 5.45, Loss: 2.8532285690307617\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 4, Action: 3, Reward: 0.25, Total Reward: 5.7, Loss: 1.5115814208984375\n",
      "Optimizing model...\n",
      "Step: 5, Action: 0, Reward: 10.0, Total Reward: 15.7, Loss: 3.2957189083099365\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 6, Action: 3, Reward: 0.25, Total Reward: 15.95, Loss: 5.2350592613220215\n",
      "Optimizing model...\n",
      "Step: 7, Action: 3, Reward: -9.75, Total Reward: 6.199999999999999, Loss: 2.57818603515625\n",
      "Optimizing model...\n",
      "Step: 8, Action: 3, Reward: -9.75, Total Reward: -3.5500000000000007, Loss: 4.803569316864014\n",
      "Optimizing model...\n",
      "Step: 9, Action: 3, Reward: 0.25, Total Reward: -3.3000000000000007, Loss: 5.246412754058838\n",
      "Optimizing model...\n",
      "Step: 10, Action: 4, Reward: 0.2, Total Reward: -3.1000000000000005, Loss: 2.174149513244629\n",
      "Optimizing model...\n",
      "Step: 11, Action: 4, Reward: -9.8, Total Reward: -12.900000000000002, Loss: 5.041303634643555\n",
      "Optimizing model...\n",
      "Step: 12, Action: 3, Reward: 0.25, Total Reward: -12.650000000000002, Loss: 2.2059433460235596\n",
      "Optimizing model...\n",
      "Step: 13, Action: 4, Reward: 0.2, Total Reward: -12.450000000000003, Loss: 2.035186767578125\n",
      "Optimizing model...\n",
      "Step: 14, Action: 1, Reward: -5.0, Total Reward: -17.450000000000003, Loss: 2.8190183639526367\n",
      "Optimizing model...\n",
      "Step: 15, Action: 4, Reward: 0.2, Total Reward: -17.250000000000004, Loss: 2.903892993927002\n",
      "Optimizing model...\n",
      "Step: 16, Action: 0, Reward: 10.0, Total Reward: -7.2500000000000036, Loss: 6.150118827819824\n",
      "Optimizing model...\n",
      "Step: 17, Action: 4, Reward: 0.2, Total Reward: -7.050000000000003, Loss: 4.04624080657959\n",
      "Optimizing model...\n",
      "Step: 18, Action: 4, Reward: -9.8, Total Reward: -16.850000000000005, Loss: 4.438040733337402\n",
      "Optimizing model...\n",
      "Step: 19, Action: 3, Reward: 0.25, Total Reward: -16.600000000000005, Loss: 1.490631103515625\n",
      "Model acting\n",
      "\n",
      "episode: 9, reward: -116.60000000000001\n",
      "Optimizing model...\n",
      "Step: 0, Action: 2, Reward: 0.25, Total Reward: 0.25, Loss: 3.2738935947418213\n",
      "Optimizing model...\n",
      "Step: 1, Action: 2, Reward: 0.25, Total Reward: 0.5, Loss: 4.947858810424805\n",
      "Optimizing model...\n",
      "Step: 2, Action: 4, Reward: 0.2, Total Reward: 0.7, Loss: 5.32966423034668\n",
      "Optimizing model...\n",
      "Step: 3, Action: 1, Reward: -5.0, Total Reward: -4.3, Loss: 3.291306972503662\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: 10.0, Total Reward: 5.7, Loss: 1.0847806930541992\n",
      "Optimizing model...\n",
      "Step: 5, Action: 0, Reward: 10.0, Total Reward: 15.7, Loss: 2.9148902893066406\n",
      "Optimizing model...\n",
      "Step: 6, Action: 0, Reward: 10.0, Total Reward: 25.7, Loss: 1.351192831993103\n",
      "Optimizing model...\n",
      "Step: 7, Action: 3, Reward: 0.25, Total Reward: 25.95, Loss: 6.137289047241211\n",
      "Optimizing model...\n",
      "Step: 8, Action: 4, Reward: 0.2, Total Reward: 26.15, Loss: 1.4657177925109863\n",
      "Optimizing model...\n",
      "Step: 9, Action: 0, Reward: 10.0, Total Reward: 36.15, Loss: 1.9303597211837769\n",
      "Optimizing model...\n",
      "Step: 10, Action: 4, Reward: 0.2, Total Reward: 36.35, Loss: 3.0099854469299316\n",
      "\n",
      "episode: 10, reward: -63.65\n",
      "Model saved\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: 10.0, Total Reward: 10.0, Loss: 6.082833290100098\n",
      "Optimizing model...\n",
      "Step: 1, Action: 3, Reward: 0.25, Total Reward: 10.25, Loss: 4.68919563293457\n",
      "Optimizing model...\n",
      "Step: 2, Action: 3, Reward: -9.75, Total Reward: 0.5, Loss: 4.780352592468262\n",
      "Optimizing model...\n",
      "Step: 3, Action: 3, Reward: 0.25, Total Reward: 0.75, Loss: 4.820953845977783\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: 10.0, Total Reward: 10.75, Loss: 3.133962631225586\n",
      "Optimizing model...\n",
      "Step: 5, Action: 4, Reward: 0.2, Total Reward: 10.95, Loss: 1.66543447971344\n",
      "\n",
      "episode: 11, reward: -89.05\n",
      "Optimizing model...\n",
      "Step: 0, Action: 1, Reward: -5.0, Total Reward: -5.0, Loss: 1.5768933296203613\n",
      "Optimizing model...\n",
      "Step: 1, Action: 4, Reward: 0.2, Total Reward: -4.8, Loss: 3.189802885055542\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: 10.0, Total Reward: 5.2, Loss: 5.836464881896973\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: 10.0, Total Reward: 15.2, Loss: 6.5496826171875\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: 0.0, Total Reward: 15.2, Loss: 3.627009153366089\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 5, Action: 0, Reward: 10.0, Total Reward: 25.2, Loss: 5.2609992027282715\n",
      "Optimizing model...\n",
      "Step: 6, Action: 2, Reward: 0.25, Total Reward: 25.45, Loss: 4.902811050415039\n",
      "Optimizing model...\n",
      "Step: 7, Action: 2, Reward: 0.25, Total Reward: 25.7, Loss: 3.236694097518921\n",
      "Optimizing model...\n",
      "Step: 8, Action: 2, Reward: 0.25, Total Reward: 25.95, Loss: 5.267743110656738\n",
      "Optimizing model...\n",
      "Step: 9, Action: 3, Reward: 0.25, Total Reward: 26.2, Loss: 3.3461108207702637\n",
      "Optimizing model...\n",
      "Step: 10, Action: 0, Reward: 10.0, Total Reward: 36.2, Loss: 3.178433895111084\n",
      "Optimizing model...\n",
      "Step: 11, Action: 2, Reward: 0.25, Total Reward: 36.45, Loss: 6.70343017578125\n",
      "Optimizing model...\n",
      "Step: 12, Action: 3, Reward: 0.25, Total Reward: 36.7, Loss: 4.423714637756348\n",
      "Optimizing model...\n",
      "Step: 13, Action: 4, Reward: 0.2, Total Reward: 36.900000000000006, Loss: 2.7757091522216797\n",
      "Optimizing model...\n",
      "Step: 14, Action: 0, Reward: 10.0, Total Reward: 46.900000000000006, Loss: 3.248605966567993\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 15, Action: 0, Reward: 10.0, Total Reward: 56.900000000000006, Loss: 3.573000192642212\n",
      "Optimizing model...\n",
      "Step: 16, Action: 4, Reward: 0.2, Total Reward: 57.10000000000001, Loss: 3.1998534202575684\n",
      "Optimizing model...\n",
      "Step: 17, Action: 3, Reward: 0.25, Total Reward: 57.35000000000001, Loss: 5.391152858734131\n",
      "Optimizing model...\n",
      "Step: 18, Action: 0, Reward: 10.0, Total Reward: 67.35000000000001, Loss: 3.173602819442749\n",
      "Optimizing model...\n",
      "Step: 19, Action: 3, Reward: 0.25, Total Reward: 67.60000000000001, Loss: 5.048596382141113\n",
      "Optimizing model...\n",
      "Step: 20, Action: 0, Reward: 10.0, Total Reward: 77.60000000000001, Loss: 1.8089032173156738\n",
      "Optimizing model...\n",
      "Step: 21, Action: 3, Reward: 0.25, Total Reward: 77.85000000000001, Loss: 1.3538014888763428\n",
      "Optimizing model...\n",
      "Step: 22, Action: 1, Reward: -5.0, Total Reward: 72.85000000000001, Loss: 5.323481559753418\n",
      "Optimizing model...\n",
      "Step: 23, Action: 2, Reward: 0.25, Total Reward: 73.10000000000001, Loss: 6.584158897399902\n",
      "Optimizing model...\n",
      "Step: 24, Action: 1, Reward: -5.0, Total Reward: 68.10000000000001, Loss: 1.415113925933838\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 25, Action: 3, Reward: 0.25, Total Reward: 68.35000000000001, Loss: 3.2903361320495605\n",
      "Optimizing model...\n",
      "Step: 26, Action: 1, Reward: -5.0, Total Reward: 63.35000000000001, Loss: 6.112100601196289\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 27, Action: 2, Reward: 0.25, Total Reward: 63.60000000000001, Loss: 2.7150368690490723\n",
      "Optimizing model...\n",
      "Step: 28, Action: 3, Reward: 0.25, Total Reward: 63.85000000000001, Loss: 1.9084234237670898\n",
      "Optimizing model...\n",
      "Step: 29, Action: 2, Reward: 0.25, Total Reward: 64.10000000000001, Loss: 3.110383987426758\n",
      "Optimizing model...\n",
      "Step: 30, Action: 0, Reward: 10.0, Total Reward: 74.10000000000001, Loss: 2.7350172996520996\n",
      "Optimizing model...\n",
      "Step: 31, Action: 3, Reward: 0.25, Total Reward: 74.35000000000001, Loss: 3.8869152069091797\n",
      "Optimizing model...\n",
      "Step: 32, Action: 4, Reward: 0.2, Total Reward: 74.55000000000001, Loss: 4.487701416015625\n",
      "Optimizing model...\n",
      "Step: 33, Action: 0, Reward: 10.0, Total Reward: 84.55000000000001, Loss: 7.445477485656738\n",
      "\n",
      "episode: 12, reward: -15.449999999999989\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: 10.0, Total Reward: 10.0, Loss: 5.265413284301758\n",
      "Optimizing model...\n",
      "Step: 1, Action: 1, Reward: -5.0, Total Reward: 5.0, Loss: 4.62073278427124\n",
      "Optimizing model...\n",
      "Step: 2, Action: 4, Reward: 0.2, Total Reward: 5.2, Loss: 6.701417446136475\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: 10.0, Total Reward: 15.2, Loss: 4.377120018005371\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: 0.0, Total Reward: 15.2, Loss: 5.111536502838135\n",
      "Optimizing model...\n",
      "Step: 5, Action: 1, Reward: -5.0, Total Reward: 10.2, Loss: 2.440340757369995\n",
      "Optimizing model...\n",
      "Step: 6, Action: 2, Reward: 0.25, Total Reward: 10.45, Loss: 2.800614356994629\n",
      "Optimizing model...\n",
      "Step: 7, Action: 1, Reward: -5.0, Total Reward: 5.449999999999999, Loss: 3.061737537384033\n",
      "Optimizing model...\n",
      "Step: 8, Action: 2, Reward: 0.25, Total Reward: 5.699999999999999, Loss: 4.249698638916016\n",
      "Optimizing model...\n",
      "Step: 9, Action: 1, Reward: -5.0, Total Reward: 0.6999999999999993, Loss: 2.7301998138427734\n",
      "Optimizing model...\n",
      "Step: 10, Action: 2, Reward: 0.25, Total Reward: 0.9499999999999993, Loss: 1.80377197265625\n",
      "Optimizing model...\n",
      "Step: 11, Action: 2, Reward: -9.75, Total Reward: -8.8, Loss: 3.6823787689208984\n",
      "Optimizing model...\n",
      "Step: 12, Action: 3, Reward: 0.25, Total Reward: -8.55, Loss: 2.8933143615722656\n",
      "Optimizing model...\n",
      "Step: 13, Action: 1, Reward: -5.0, Total Reward: -13.55, Loss: 2.9170539379119873\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 14, Action: 3, Reward: 0.25, Total Reward: -13.3, Loss: 4.691004753112793\n",
      "Model acting\n",
      "\n",
      "episode: 13, reward: -113.3\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: 10.0, Total Reward: 10.0, Loss: 1.1554465293884277\n",
      "Optimizing model...\n",
      "Step: 1, Action: 1, Reward: -5.0, Total Reward: 5.0, Loss: 4.465597152709961\n",
      "Optimizing model...\n",
      "Step: 2, Action: 3, Reward: 0.25, Total Reward: 5.25, Loss: 3.5211129188537598\n",
      "Optimizing model...\n",
      "Step: 3, Action: 1, Reward: -5.0, Total Reward: 0.25, Loss: 2.493104934692383\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: 10.0, Total Reward: 10.25, Loss: 2.5900049209594727\n",
      "Optimizing model...\n",
      "Step: 5, Action: 4, Reward: 0.2, Total Reward: 10.45, Loss: 4.601049423217773\n",
      "Optimizing model...\n",
      "Step: 6, Action: 3, Reward: 0.25, Total Reward: 10.7, Loss: 4.410989761352539\n",
      "Optimizing model...\n",
      "Step: 7, Action: 3, Reward: -9.75, Total Reward: 0.9499999999999993, Loss: 3.2328882217407227\n",
      "Optimizing model...\n",
      "Step: 8, Action: 1, Reward: -5.0, Total Reward: -4.050000000000001, Loss: 6.154335975646973\n",
      "Optimizing model...\n",
      "Step: 9, Action: 3, Reward: 0.25, Total Reward: -3.8000000000000007, Loss: 5.142894744873047\n",
      "Optimizing model...\n",
      "Step: 10, Action: 2, Reward: 0.25, Total Reward: -3.5500000000000007, Loss: 3.756592273712158\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 11, Action: 3, Reward: 0.25, Total Reward: -3.3000000000000007, Loss: 1.5926965475082397\n",
      "Optimizing model...\n",
      "Step: 12, Action: 0, Reward: 10.0, Total Reward: 6.699999999999999, Loss: 11.52989387512207\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 13, Action: 3, Reward: 0.25, Total Reward: 6.949999999999999, Loss: 1.0366697311401367\n",
      "Optimizing model...\n",
      "Step: 14, Action: 1, Reward: -5.0, Total Reward: 1.9499999999999993, Loss: 5.22208309173584\n",
      "\n",
      "episode: 14, reward: -98.05\n",
      "Optimizing model...\n",
      "Step: 0, Action: 1, Reward: -5.0, Total Reward: -5.0, Loss: 1.5856599807739258\n",
      "Optimizing model...\n",
      "Step: 1, Action: 1, Reward: -5.0, Total Reward: -10.0, Loss: 6.570555210113525\n",
      "Optimizing model...\n",
      "Step: 2, Action: 3, Reward: 0.25, Total Reward: -9.75, Loss: 5.635451316833496\n",
      "Optimizing model...\n",
      "Step: 3, Action: 3, Reward: -9.75, Total Reward: -19.5, Loss: 3.623047351837158\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 4, Action: 3, Reward: -9.75, Total Reward: -29.25, Loss: 3.4124135971069336\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 5, Action: 3, Reward: -9.75, Total Reward: -39.0, Loss: 3.007838726043701\n",
      "Optimizing model...\n",
      "Step: 6, Action: 4, Reward: 0.2, Total Reward: -38.8, Loss: 6.494250297546387\n",
      "Optimizing model...\n",
      "Step: 7, Action: 1, Reward: -5.0, Total Reward: -43.8, Loss: 4.910721778869629\n",
      "Optimizing model...\n",
      "Step: 8, Action: 2, Reward: 0.25, Total Reward: -43.55, Loss: 3.408799648284912\n",
      "Optimizing model...\n",
      "Step: 9, Action: 3, Reward: 0.25, Total Reward: -43.3, Loss: 2.280785083770752\n",
      "Optimizing model...\n",
      "Step: 10, Action: 0, Reward: 10.0, Total Reward: -33.3, Loss: 2.903611898422241\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 11, Action: 3, Reward: 0.25, Total Reward: -33.05, Loss: 5.033236026763916\n",
      "\n",
      "episode: 15, reward: -133.05\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 3, Reward: 0.25, Total Reward: 0.25, Loss: 6.664751052856445\n",
      "Optimizing model...\n",
      "Step: 1, Action: 2, Reward: 0.25, Total Reward: 0.5, Loss: 2.5171210765838623\n",
      "Optimizing model...\n",
      "Step: 2, Action: 3, Reward: 0.25, Total Reward: 0.75, Loss: 4.734699249267578\n",
      "Optimizing model...\n",
      "Step: 3, Action: 1, Reward: -5.0, Total Reward: -4.25, Loss: 6.578409194946289\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 4, Action: 3, Reward: 0.25, Total Reward: -4.0, Loss: 1.1978459358215332\n",
      "Optimizing model...\n",
      "Step: 5, Action: 0, Reward: 10.0, Total Reward: 6.0, Loss: 3.007042407989502\n",
      "Optimizing model...\n",
      "Step: 6, Action: 2, Reward: 0.25, Total Reward: 6.25, Loss: 2.322219133377075\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 7, Action: 3, Reward: 0.25, Total Reward: 6.5, Loss: 3.2689356803894043\n",
      "Optimizing model...\n",
      "Step: 8, Action: 4, Reward: 0.2, Total Reward: 6.7, Loss: 3.3829033374786377\n",
      "Optimizing model...\n",
      "Step: 9, Action: 3, Reward: 0.25, Total Reward: 6.95, Loss: 4.453848838806152\n",
      "Optimizing model...\n",
      "Step: 10, Action: 2, Reward: 0.25, Total Reward: 7.2, Loss: 4.957991600036621\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 11, Action: 3, Reward: 0.25, Total Reward: 7.45, Loss: 4.757195949554443\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 12, Action: 3, Reward: -9.75, Total Reward: -2.3, Loss: 2.678523540496826\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 13, Action: 3, Reward: -9.75, Total Reward: -12.05, Loss: 5.54354190826416\n",
      "Optimizing model...\n",
      "Step: 14, Action: 1, Reward: -5.0, Total Reward: -17.05, Loss: 3.4802796840667725\n",
      "Optimizing model...\n",
      "Step: 15, Action: 0, Reward: 10.0, Total Reward: -7.050000000000001, Loss: 4.0912933349609375\n",
      "Optimizing model...\n",
      "Step: 16, Action: 0, Reward: 10.0, Total Reward: 2.9499999999999993, Loss: 6.062929153442383\n",
      "Optimizing model...\n",
      "Step: 17, Action: 2, Reward: 0.25, Total Reward: 3.1999999999999993, Loss: 4.766919136047363\n",
      "Optimizing model...\n",
      "Step: 18, Action: 2, Reward: -9.75, Total Reward: -6.550000000000001, Loss: 5.829314231872559\n",
      "Optimizing model...\n",
      "Step: 19, Action: 4, Reward: 0.2, Total Reward: -6.3500000000000005, Loss: 6.047095775604248\n",
      "\n",
      "episode: 16, reward: -106.35\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: 10.0, Total Reward: 10.0, Loss: 4.4297661781311035\n",
      "Optimizing model...\n",
      "Step: 1, Action: 4, Reward: 0.2, Total Reward: 10.2, Loss: 7.604987144470215\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: 10.0, Total Reward: 20.2, Loss: 9.641611099243164\n",
      "Optimizing model...\n",
      "Step: 3, Action: 1, Reward: -5.0, Total Reward: 15.2, Loss: 7.693435192108154\n",
      "Optimizing model...\n",
      "Step: 4, Action: 2, Reward: 0.25, Total Reward: 15.45, Loss: 5.272368431091309\n",
      "\n",
      "episode: 17, reward: -84.55\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: 10.0, Total Reward: 10.0, Loss: 7.1466145515441895\n",
      "Optimizing model...\n",
      "Step: 1, Action: 3, Reward: 0.25, Total Reward: 10.25, Loss: 4.5241899490356445\n",
      "Optimizing model...\n",
      "Step: 2, Action: 1, Reward: -5.0, Total Reward: 5.25, Loss: 8.110602378845215\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: 10.0, Total Reward: 15.25, Loss: 5.949923038482666\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: 10.0, Total Reward: 25.25, Loss: 6.5658087730407715\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 5, Action: 0, Reward: 0.0, Total Reward: 25.25, Loss: 3.01267147064209\n",
      "Optimizing model...\n",
      "Step: 6, Action: 1, Reward: -5.0, Total Reward: 20.25, Loss: 4.122077941894531\n",
      "Optimizing model...\n",
      "Step: 7, Action: 1, Reward: -5.0, Total Reward: 15.25, Loss: 6.411845684051514\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 8, Action: 0, Reward: 10.0, Total Reward: 25.25, Loss: 7.215208053588867\n",
      "Optimizing model...\n",
      "Step: 9, Action: 3, Reward: 0.25, Total Reward: 25.5, Loss: 3.250025749206543\n",
      "Optimizing model...\n",
      "Step: 10, Action: 1, Reward: -5.0, Total Reward: 20.5, Loss: 3.1833248138427734\n",
      "Optimizing model...\n",
      "Step: 11, Action: 0, Reward: 10.0, Total Reward: 30.5, Loss: 1.611028790473938\n",
      "Optimizing model...\n",
      "Step: 12, Action: 2, Reward: 0.25, Total Reward: 30.75, Loss: 2.7290711402893066\n",
      "Optimizing model...\n",
      "Step: 13, Action: 4, Reward: 0.2, Total Reward: 30.95, Loss: 5.218904495239258\n",
      "Optimizing model...\n",
      "Step: 14, Action: 0, Reward: 10.0, Total Reward: 40.95, Loss: 3.1443233489990234\n",
      "Optimizing model...\n",
      "Step: 15, Action: 3, Reward: 0.25, Total Reward: 41.2, Loss: 2.7838470935821533\n",
      "\n",
      "episode: 18, reward: -58.8\n",
      "Optimizing model...\n",
      "Step: 0, Action: 2, Reward: 0.25, Total Reward: 0.25, Loss: 4.895450592041016\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: 10.0, Total Reward: 10.25, Loss: 5.3911004066467285\n",
      "Optimizing model...\n",
      "Step: 2, Action: 4, Reward: 0.2, Total Reward: 10.45, Loss: 6.095566749572754\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: 10.0, Total Reward: 20.45, Loss: 4.629934310913086\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: 0.0, Total Reward: 20.45, Loss: 4.385639190673828\n",
      "Optimizing model...\n",
      "Step: 5, Action: 3, Reward: 0.25, Total Reward: 20.7, Loss: 5.094911098480225\n",
      "Optimizing model...\n",
      "Step: 6, Action: 1, Reward: -5.0, Total Reward: 15.7, Loss: 3.1920061111450195\n",
      "Optimizing model...\n",
      "Step: 7, Action: 4, Reward: 0.2, Total Reward: 15.899999999999999, Loss: 1.8196532726287842\n",
      "Optimizing model...\n",
      "Step: 8, Action: 1, Reward: -5.0, Total Reward: 10.899999999999999, Loss: 4.132143020629883\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 9, Action: 0, Reward: 10.0, Total Reward: 20.9, Loss: 4.563343524932861\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 10, Action: 0, Reward: 10.0, Total Reward: 30.9, Loss: 1.4643768072128296\n",
      "Optimizing model...\n",
      "Step: 11, Action: 3, Reward: 0.25, Total Reward: 31.15, Loss: 7.291776657104492\n",
      "\n",
      "episode: 19, reward: -68.85\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: 10.0, Total Reward: 10.0, Loss: 4.287309646606445\n",
      "Optimizing model...\n",
      "Step: 1, Action: 1, Reward: -5.0, Total Reward: 5.0, Loss: 5.459336280822754\n",
      "Optimizing model...\n",
      "Step: 2, Action: 2, Reward: 0.25, Total Reward: 5.25, Loss: 1.876979112625122\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: 10.0, Total Reward: 15.25, Loss: 6.4822211265563965\n",
      "Optimizing model...\n",
      "Step: 4, Action: 2, Reward: 0.25, Total Reward: 15.5, Loss: 2.571873188018799\n",
      "Optimizing model...\n",
      "Step: 5, Action: 0, Reward: 10.0, Total Reward: 25.5, Loss: 2.0352680683135986\n",
      "Optimizing model...\n",
      "Step: 6, Action: 0, Reward: 10.0, Total Reward: 35.5, Loss: 1.6558318138122559\n",
      "Optimizing model...\n",
      "Step: 7, Action: 3, Reward: 0.25, Total Reward: 35.75, Loss: 2.0835371017456055\n",
      "Optimizing model...\n",
      "Step: 8, Action: 4, Reward: 0.2, Total Reward: 35.95, Loss: 2.20971417427063\n",
      "Optimizing model...\n",
      "Step: 9, Action: 3, Reward: 0.25, Total Reward: 36.2, Loss: 4.93427848815918\n",
      "Optimizing model...\n",
      "Step: 10, Action: 0, Reward: 10.0, Total Reward: 46.2, Loss: 4.630091190338135\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 11, Action: 0, Reward: 10.0, Total Reward: 56.2, Loss: 2.0173940658569336\n",
      "Optimizing model...\n",
      "Step: 12, Action: 4, Reward: 0.2, Total Reward: 56.400000000000006, Loss: 2.903209924697876\n",
      "\n",
      "episode: 20, reward: -43.599999999999994\n",
      "Model saved\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: 10.0, Total Reward: 10.0, Loss: 5.400392055511475\n",
      "Optimizing model...\n",
      "Step: 1, Action: 3, Reward: 0.25, Total Reward: 10.25, Loss: 3.5586509704589844\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: 10.0, Total Reward: 20.25, Loss: 1.8149434328079224\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: 0.0, Total Reward: 20.25, Loss: 3.29461669921875\n",
      "Optimizing model...\n",
      "Step: 4, Action: 1, Reward: -5.0, Total Reward: 15.25, Loss: 9.08755111694336\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 5, Action: 0, Reward: 10.0, Total Reward: 25.25, Loss: 3.516181230545044\n",
      "Optimizing model...\n",
      "Step: 6, Action: 0, Reward: 10.0, Total Reward: 35.25, Loss: 5.15372371673584\n",
      "Optimizing model...\n",
      "Step: 7, Action: 0, Reward: 0.0, Total Reward: 35.25, Loss: 7.674607276916504\n",
      "Optimizing model...\n",
      "Step: 8, Action: 1, Reward: -5.0, Total Reward: 30.25, Loss: 1.9941213130950928\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 9, Action: 0, Reward: 10.0, Total Reward: 40.25, Loss: 1.5329850912094116\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 10, Action: 0, Reward: 10.0, Total Reward: 50.25, Loss: 1.8118035793304443\n",
      "Optimizing model...\n",
      "Step: 11, Action: 4, Reward: 0.2, Total Reward: 50.45, Loss: 1.839951515197754\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 12, Action: 0, Reward: 10.0, Total Reward: 60.45, Loss: 3.775670289993286\n",
      "Optimizing model...\n",
      "Step: 13, Action: 1, Reward: -5.0, Total Reward: 55.45, Loss: 1.9403640031814575\n",
      "Optimizing model...\n",
      "Step: 14, Action: 4, Reward: 0.2, Total Reward: 55.650000000000006, Loss: 4.803311824798584\n",
      "Optimizing model...\n",
      "Step: 15, Action: 1, Reward: -5.0, Total Reward: 50.650000000000006, Loss: 2.8796164989471436\n",
      "Optimizing model...\n",
      "Step: 16, Action: 2, Reward: 0.25, Total Reward: 50.900000000000006, Loss: 7.772801399230957\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 17, Action: 0, Reward: 10.0, Total Reward: 60.900000000000006, Loss: 6.091022491455078\n",
      "Optimizing model...\n",
      "Step: 18, Action: 4, Reward: 0.2, Total Reward: 61.10000000000001, Loss: 5.378499507904053\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 19, Action: 0, Reward: 10.0, Total Reward: 71.10000000000001, Loss: 4.784725189208984\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 20, Action: 0, Reward: 0.0, Total Reward: 71.10000000000001, Loss: 4.829901695251465\n",
      "\n",
      "episode: 21, reward: -28.89999999999999\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: 10.0, Total Reward: 10.0, Loss: 3.122119188308716\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: 10.0, Total Reward: 20.0, Loss: 5.432284355163574\n",
      "Optimizing model...\n",
      "Step: 2, Action: 4, Reward: 0.2, Total Reward: 20.2, Loss: 10.03013801574707\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: 10.0, Total Reward: 30.2, Loss: 1.9935221672058105\n",
      "Optimizing model...\n",
      "Step: 4, Action: 1, Reward: -5.0, Total Reward: 25.2, Loss: 5.068417072296143\n",
      "Optimizing model...\n",
      "Step: 5, Action: 2, Reward: 0.25, Total Reward: 25.45, Loss: 5.135546684265137\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 6, Action: 0, Reward: 10.0, Total Reward: 35.45, Loss: 4.77412748336792\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 7, Action: 0, Reward: 10.0, Total Reward: 45.45, Loss: 10.861778259277344\n",
      "Optimizing model...\n",
      "Step: 8, Action: 3, Reward: 0.25, Total Reward: 45.7, Loss: 6.955920219421387\n",
      "Optimizing model...\n",
      "Step: 9, Action: 2, Reward: 0.25, Total Reward: 45.95, Loss: 6.819659233093262\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 10, Action: 0, Reward: 10.0, Total Reward: 55.95, Loss: 4.558786392211914\n",
      "Optimizing model...\n",
      "Step: 11, Action: 3, Reward: 0.25, Total Reward: 56.2, Loss: 1.6659536361694336\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 12, Action: 0, Reward: 10.0, Total Reward: 66.2, Loss: 5.926856994628906\n",
      "\n",
      "episode: 22, reward: -33.8\n",
      "Optimizing model...\n",
      "Step: 0, Action: 4, Reward: 0.2, Total Reward: 0.2, Loss: 7.213497161865234\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: 10.0, Total Reward: 10.2, Loss: 5.962111473083496\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: 10.0, Total Reward: 20.2, Loss: 4.53769588470459\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: 0.0, Total Reward: 20.2, Loss: 1.5385499000549316\n",
      "Optimizing model...\n",
      "Step: 4, Action: 3, Reward: 0.25, Total Reward: 20.45, Loss: 4.582158088684082\n",
      "Optimizing model...\n",
      "Step: 5, Action: 4, Reward: 0.2, Total Reward: 20.65, Loss: 5.601964950561523\n",
      "Optimizing model...\n",
      "Step: 6, Action: 1, Reward: -5.0, Total Reward: 15.649999999999999, Loss: 4.226665496826172\n",
      "Optimizing model...\n",
      "Step: 7, Action: 4, Reward: 0.2, Total Reward: 15.849999999999998, Loss: 3.5303988456726074\n",
      "Optimizing model...\n",
      "Step: 8, Action: 2, Reward: 0.25, Total Reward: 16.099999999999998, Loss: 5.493561744689941\n",
      "Optimizing model...\n",
      "Step: 9, Action: 3, Reward: 0.25, Total Reward: 16.349999999999998, Loss: 5.0832977294921875\n",
      "Optimizing model...\n",
      "Step: 10, Action: 3, Reward: 0.25, Total Reward: 16.599999999999998, Loss: 4.479670524597168\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 11, Action: 0, Reward: 10.0, Total Reward: 26.599999999999998, Loss: 5.997526168823242\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 12, Action: 0, Reward: 0.0, Total Reward: 26.599999999999998, Loss: 5.50064754486084\n",
      "Optimizing model...\n",
      "Step: 13, Action: 4, Reward: 0.2, Total Reward: 26.799999999999997, Loss: 6.53571891784668\n",
      "Optimizing model...\n",
      "Step: 14, Action: 2, Reward: 0.25, Total Reward: 27.049999999999997, Loss: 2.1967763900756836\n",
      "Optimizing model...\n",
      "Step: 15, Action: 3, Reward: 0.25, Total Reward: 27.299999999999997, Loss: 1.586930751800537\n",
      "Optimizing model...\n",
      "Step: 16, Action: 2, Reward: 0.25, Total Reward: 27.549999999999997, Loss: 2.0382447242736816\n",
      "Optimizing model...\n",
      "Step: 17, Action: 1, Reward: -5.0, Total Reward: 22.549999999999997, Loss: 3.4000277519226074\n",
      "Optimizing model...\n",
      "Step: 18, Action: 2, Reward: 0.25, Total Reward: 22.799999999999997, Loss: 3.0491719245910645\n",
      "Optimizing model...\n",
      "Step: 19, Action: 1, Reward: -5.0, Total Reward: 17.799999999999997, Loss: 4.448391437530518\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 20, Action: 0, Reward: 10.0, Total Reward: 27.799999999999997, Loss: 4.182328224182129\n",
      "Optimizing model...\n",
      "Step: 21, Action: 3, Reward: 0.25, Total Reward: 28.049999999999997, Loss: 3.447906732559204\n",
      "\n",
      "episode: 23, reward: -71.95\n",
      "Optimizing model...\n",
      "Step: 0, Action: 1, Reward: -5.0, Total Reward: -5.0, Loss: 6.319039344787598\n",
      "Optimizing model...\n",
      "Step: 1, Action: 4, Reward: 0.2, Total Reward: -4.8, Loss: 1.5135235786437988\n",
      "Optimizing model...\n",
      "Step: 2, Action: 3, Reward: 0.25, Total Reward: -4.55, Loss: 2.090212345123291\n",
      "Optimizing model...\n",
      "Step: 3, Action: 3, Reward: 0.25, Total Reward: -4.3, Loss: 3.0376768112182617\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: 10.0, Total Reward: 5.7, Loss: 3.0783839225769043\n",
      "Optimizing model...\n",
      "Step: 5, Action: 1, Reward: -5.0, Total Reward: 0.7000000000000002, Loss: 4.979491710662842\n",
      "Optimizing model...\n",
      "Step: 6, Action: 2, Reward: 0.25, Total Reward: 0.9500000000000002, Loss: 5.633232116699219\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 7, Action: 0, Reward: 10.0, Total Reward: 10.95, Loss: 3.211357593536377\n",
      "Optimizing model...\n",
      "Step: 8, Action: 0, Reward: 10.0, Total Reward: 20.95, Loss: 7.492031097412109\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 9, Action: 4, Reward: 0.2, Total Reward: 21.15, Loss: 2.34371280670166\n",
      "Optimizing model...\n",
      "Step: 10, Action: 3, Reward: 0.25, Total Reward: 21.4, Loss: 7.005914211273193\n",
      "Optimizing model...\n",
      "Step: 11, Action: 4, Reward: 0.2, Total Reward: 21.599999999999998, Loss: 6.740853309631348\n",
      "Optimizing model...\n",
      "Step: 12, Action: 2, Reward: 0.25, Total Reward: 21.849999999999998, Loss: 7.975600242614746\n",
      "Optimizing model...\n",
      "Step: 13, Action: 0, Reward: 10.0, Total Reward: 31.849999999999998, Loss: 3.704559326171875\n",
      "Optimizing model...\n",
      "Step: 14, Action: 2, Reward: 0.25, Total Reward: 32.099999999999994, Loss: 5.003215789794922\n",
      "Optimizing model...\n",
      "Step: 15, Action: 2, Reward: 0.25, Total Reward: 32.349999999999994, Loss: 3.190326452255249\n",
      "Optimizing model...\n",
      "Step: 16, Action: 2, Reward: -9.75, Total Reward: 22.599999999999994, Loss: 1.7055636644363403\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 17, Action: 4, Reward: 0.2, Total Reward: 22.799999999999994, Loss: 2.122541904449463\n",
      "\n",
      "episode: 24, reward: -77.2\n",
      "Optimizing model...\n",
      "Step: 0, Action: 3, Reward: 0.25, Total Reward: 0.25, Loss: 5.474814414978027\n",
      "Optimizing model...\n",
      "Step: 1, Action: 3, Reward: 0.25, Total Reward: 0.5, Loss: 3.087094306945801\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 4, Reward: 0.2, Total Reward: 0.7, Loss: 2.972740411758423\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 4, Reward: -9.8, Total Reward: -9.100000000000001, Loss: 4.131397724151611\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: 10.0, Total Reward: 0.8999999999999986, Loss: 7.400032043457031\n",
      "Optimizing model...\n",
      "Step: 5, Action: 1, Reward: -5.0, Total Reward: -4.100000000000001, Loss: 5.914173603057861\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 6, Action: 0, Reward: 10.0, Total Reward: 5.899999999999999, Loss: 4.410204887390137\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 7, Action: 4, Reward: 0.2, Total Reward: 6.099999999999999, Loss: 3.5502841472625732\n",
      "Optimizing model...\n",
      "Step: 8, Action: 4, Reward: -9.8, Total Reward: -3.700000000000002, Loss: 3.072068214416504\n",
      "Optimizing model...\n",
      "Step: 9, Action: 1, Reward: -5.0, Total Reward: -8.700000000000003, Loss: 3.1605238914489746\n",
      "Optimizing model...\n",
      "Step: 10, Action: 3, Reward: 0.25, Total Reward: -8.450000000000003, Loss: 3.7274317741394043\n",
      "Optimizing model...\n",
      "Step: 11, Action: 2, Reward: 0.25, Total Reward: -8.200000000000003, Loss: 2.862168550491333\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 12, Action: 0, Reward: 10.0, Total Reward: 1.7999999999999972, Loss: 4.655782699584961\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 13, Action: 4, Reward: 0.2, Total Reward: 1.9999999999999971, Loss: 8.581947326660156\n",
      "Optimizing model...\n",
      "Step: 14, Action: 4, Reward: -9.8, Total Reward: -7.800000000000003, Loss: 1.9781490564346313\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 15, Action: 0, Reward: 10.0, Total Reward: 2.1999999999999966, Loss: 4.194540023803711\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 16, Action: 0, Reward: 10.0, Total Reward: 12.199999999999996, Loss: 1.4539034366607666\n",
      "Optimizing model...\n",
      "Step: 17, Action: 2, Reward: 0.25, Total Reward: 12.449999999999996, Loss: 2.6734752655029297\n",
      "Optimizing model...\n",
      "Step: 18, Action: 4, Reward: 0.2, Total Reward: 12.649999999999995, Loss: 3.3953776359558105\n",
      "Optimizing model...\n",
      "Step: 19, Action: 0, Reward: 10.0, Total Reward: 22.649999999999995, Loss: 4.869234085083008\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 20, Action: 0, Reward: 10.0, Total Reward: 32.64999999999999, Loss: 1.9980831146240234\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 21, Action: 0, Reward: 10.0, Total Reward: 42.64999999999999, Loss: 2.349318027496338\n",
      "Optimizing model...\n",
      "Step: 22, Action: 4, Reward: 0.2, Total Reward: 42.849999999999994, Loss: 3.7074172496795654\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 23, Action: 0, Reward: 10.0, Total Reward: 52.849999999999994, Loss: 5.021450996398926\n",
      "Optimizing model...\n",
      "Step: 24, Action: 1, Reward: -5.0, Total Reward: 47.849999999999994, Loss: 3.026388168334961\n",
      "Optimizing model...\n",
      "Step: 25, Action: 0, Reward: 10.0, Total Reward: 57.849999999999994, Loss: 5.960960388183594\n",
      "\n",
      "episode: 25, reward: -42.150000000000006\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: 10.0, Total Reward: 10.0, Loss: 4.806068420410156\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: 10.0, Total Reward: 20.0, Loss: 4.295909881591797\n",
      "Optimizing model...\n",
      "Step: 2, Action: 4, Reward: 0.2, Total Reward: 20.2, Loss: 4.843027114868164\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: 10.0, Total Reward: 30.2, Loss: 3.613203525543213\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: 0.0, Total Reward: 30.2, Loss: 2.1847641468048096\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 5, Action: 0, Reward: 10.0, Total Reward: 40.2, Loss: 4.5668840408325195\n",
      "Optimizing model...\n",
      "Step: 6, Action: 0, Reward: 10.0, Total Reward: 50.2, Loss: 1.6221082210540771\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 7, Action: 0, Reward: 0.0, Total Reward: 50.2, Loss: 6.275732040405273\n",
      "Optimizing model...\n",
      "Step: 8, Action: 4, Reward: 0.2, Total Reward: 50.400000000000006, Loss: 4.197439670562744\n",
      "Optimizing model...\n",
      "Step: 9, Action: 3, Reward: 0.25, Total Reward: 50.650000000000006, Loss: 4.884479522705078\n",
      "Optimizing model...\n",
      "Step: 10, Action: 4, Reward: 0.2, Total Reward: 50.85000000000001, Loss: 1.4296939373016357\n",
      "Optimizing model...\n",
      "Step: 11, Action: 0, Reward: 10.0, Total Reward: 60.85000000000001, Loss: 1.6144764423370361\n",
      "Optimizing model...\n",
      "Step: 12, Action: 0, Reward: 0.0, Total Reward: 60.85000000000001, Loss: 3.1296863555908203\n",
      "Optimizing model...\n",
      "Step: 13, Action: 0, Reward: 0.0, Total Reward: 60.85000000000001, Loss: 5.004117488861084\n",
      "Optimizing model...\n",
      "Step: 14, Action: 4, Reward: 0.2, Total Reward: 61.05000000000001, Loss: 5.153947830200195\n",
      "Optimizing model...\n",
      "Step: 15, Action: 1, Reward: -5.0, Total Reward: 56.05000000000001, Loss: 4.952962875366211\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 16, Action: 0, Reward: 10.0, Total Reward: 66.05000000000001, Loss: 5.674094200134277\n",
      "Model acting\n",
      "\n",
      "episode: 26, reward: -33.94999999999999\n",
      "Optimizing model...\n",
      "Step: 0, Action: 1, Reward: -5.0, Total Reward: -5.0, Loss: 1.7843422889709473\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: 10.0, Total Reward: 5.0, Loss: 2.453503131866455\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: 10.0, Total Reward: 15.0, Loss: 4.719404220581055\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: 10.0, Total Reward: 25.0, Loss: 4.245940208435059\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: 10.0, Total Reward: 35.0, Loss: 4.538902282714844\n",
      "Optimizing model...\n",
      "Step: 5, Action: 2, Reward: 0.25, Total Reward: 35.25, Loss: 3.385803699493408\n",
      "Optimizing model...\n",
      "Step: 6, Action: 0, Reward: 10.0, Total Reward: 45.25, Loss: 3.6922011375427246\n",
      "\n",
      "episode: 27, reward: -54.75\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: 10.0, Total Reward: 10.0, Loss: 3.362372398376465\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: 10.0, Total Reward: 20.0, Loss: 2.0924229621887207\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: 0.0, Total Reward: 20.0, Loss: 2.8313848972320557\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: 10.0, Total Reward: 30.0, Loss: 4.559450149536133\n",
      "Optimizing model...\n",
      "Step: 4, Action: 4, Reward: 0.2, Total Reward: 30.2, Loss: 3.4289441108703613\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 5, Action: 0, Reward: 10.0, Total Reward: 40.2, Loss: 5.21647834777832\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 6, Action: 0, Reward: 10.0, Total Reward: 50.2, Loss: 3.6392979621887207\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 7, Action: 0, Reward: 0.0, Total Reward: 50.2, Loss: 6.099090576171875\n",
      "Optimizing model...\n",
      "Step: 8, Action: 0, Reward: 0.0, Total Reward: 50.2, Loss: 4.046383857727051\n",
      "Optimizing model...\n",
      "Step: 9, Action: 2, Reward: 0.25, Total Reward: 50.45, Loss: 3.01053786277771\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 10, Action: 0, Reward: 10.0, Total Reward: 60.45, Loss: 2.0054984092712402\n",
      "Optimizing model...\n",
      "Step: 11, Action: 2, Reward: 0.25, Total Reward: 60.7, Loss: 3.908635139465332\n",
      "Optimizing model...\n",
      "Step: 12, Action: 3, Reward: 0.25, Total Reward: 60.95, Loss: 3.627283811569214\n",
      "Optimizing model...\n",
      "Step: 13, Action: 2, Reward: 0.25, Total Reward: 61.2, Loss: 4.742340087890625\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 14, Action: 0, Reward: 10.0, Total Reward: 71.2, Loss: 6.602413654327393\n",
      "Model acting\n",
      "\n",
      "episode: 28, reward: -28.799999999999997\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: 10.0, Total Reward: 10.0, Loss: 2.859447717666626\n",
      "Optimizing model...\n",
      "Step: 1, Action: 3, Reward: 0.25, Total Reward: 10.25, Loss: 5.867764472961426\n",
      "Optimizing model...\n",
      "Step: 2, Action: 2, Reward: 0.25, Total Reward: 10.5, Loss: 1.4963006973266602\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: 10.0, Total Reward: 20.5, Loss: 4.710336208343506\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: 10.0, Total Reward: 30.5, Loss: 3.30452561378479\n",
      "Optimizing model...\n",
      "Step: 5, Action: 2, Reward: 0.25, Total Reward: 30.75, Loss: 7.988402366638184\n",
      "Optimizing model...\n",
      "Step: 6, Action: 3, Reward: 0.25, Total Reward: 31.0, Loss: 5.703129291534424\n",
      "Optimizing model...\n",
      "Step: 7, Action: 4, Reward: 0.2, Total Reward: 31.2, Loss: 3.1120126247406006\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 8, Action: 0, Reward: 10.0, Total Reward: 41.2, Loss: 5.347830772399902\n",
      "Optimizing model...\n",
      "Step: 9, Action: 0, Reward: 0.0, Total Reward: 41.2, Loss: 3.3591675758361816\n",
      "Optimizing model...\n",
      "Step: 10, Action: 2, Reward: 0.25, Total Reward: 41.45, Loss: 3.2172605991363525\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 11, Action: 0, Reward: 10.0, Total Reward: 51.45, Loss: 4.495489120483398\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 12, Action: 0, Reward: 10.0, Total Reward: 61.45, Loss: 4.522920608520508\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 13, Action: 0, Reward: 0.0, Total Reward: 61.45, Loss: 4.711927890777588\n",
      "\n",
      "episode: 29, reward: -38.55\n",
      "Optimizing model...\n",
      "Step: 0, Action: 4, Reward: 0.2, Total Reward: 0.2, Loss: 6.118636131286621\n",
      "Optimizing model...\n",
      "Step: 1, Action: 1, Reward: -5.0, Total Reward: -4.8, Loss: 4.488107681274414\n",
      "Optimizing model...\n",
      "Step: 2, Action: 2, Reward: 0.25, Total Reward: -4.55, Loss: 1.4646538496017456\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: 10.0, Total Reward: 5.45, Loss: 3.650038003921509\n",
      "Optimizing model...\n",
      "Step: 4, Action: 2, Reward: 0.25, Total Reward: 5.7, Loss: 4.396282196044922\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 5, Action: 0, Reward: 10.0, Total Reward: 15.7, Loss: 4.641969680786133\n",
      "Optimizing model...\n",
      "Step: 6, Action: 2, Reward: 0.25, Total Reward: 15.95, Loss: 3.465212821960449\n",
      "Optimizing model...\n",
      "Step: 7, Action: 3, Reward: 0.25, Total Reward: 16.2, Loss: 4.343738555908203\n",
      "Optimizing model...\n",
      "Step: 8, Action: 3, Reward: 0.25, Total Reward: 16.45, Loss: 5.2414093017578125\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 9, Action: 0, Reward: 10.0, Total Reward: 26.45, Loss: 4.676578998565674\n",
      "Optimizing model...\n",
      "Step: 10, Action: 4, Reward: 0.2, Total Reward: 26.65, Loss: 6.322206020355225\n",
      "Optimizing model...\n",
      "Step: 11, Action: 2, Reward: 0.25, Total Reward: 26.9, Loss: 6.323409080505371\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 12, Action: 0, Reward: 10.0, Total Reward: 36.9, Loss: 6.292621612548828\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 13, Action: 0, Reward: 0.0, Total Reward: 36.9, Loss: 4.136665344238281\n",
      "Optimizing model...\n",
      "Step: 14, Action: 4, Reward: 0.2, Total Reward: 37.1, Loss: 5.017604827880859\n",
      "Optimizing model...\n",
      "Step: 15, Action: 4, Reward: -9.8, Total Reward: 27.3, Loss: 3.0617599487304688\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 16, Action: 0, Reward: 10.0, Total Reward: 37.3, Loss: 4.719630241394043\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 17, Action: 0, Reward: 0.0, Total Reward: 37.3, Loss: 5.060752868652344\n",
      "Optimizing model...\n",
      "Step: 18, Action: 3, Reward: 0.25, Total Reward: 37.55, Loss: 4.499587059020996\n",
      "Optimizing model...\n",
      "Step: 19, Action: 3, Reward: -9.75, Total Reward: 27.799999999999997, Loss: 6.242393493652344\n",
      "Optimizing model...\n",
      "Step: 20, Action: 0, Reward: 10.0, Total Reward: 37.8, Loss: 6.3139824867248535\n",
      "Optimizing model...\n",
      "Step: 21, Action: 4, Reward: 0.2, Total Reward: 38.0, Loss: 4.026956558227539\n",
      "Optimizing model...\n",
      "Step: 22, Action: 3, Reward: 0.25, Total Reward: 38.25, Loss: 2.4495296478271484\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 23, Action: 0, Reward: 10.0, Total Reward: 48.25, Loss: 3.8966729640960693\n",
      "Optimizing model...\n",
      "Step: 24, Action: 2, Reward: 0.25, Total Reward: 48.5, Loss: 3.3697285652160645\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 25, Action: 0, Reward: 10.0, Total Reward: 58.5, Loss: 4.664109706878662\n",
      "Optimizing model...\n",
      "Step: 26, Action: 4, Reward: 0.2, Total Reward: 58.7, Loss: 1.8059232234954834\n",
      "Optimizing model...\n",
      "Step: 27, Action: 0, Reward: 10.0, Total Reward: 68.7, Loss: 1.5839177370071411\n",
      "Model acting\n",
      "\n",
      "episode: 30, reward: -31.299999999999997\n",
      "Model saved\n",
      "Optimizing model...\n",
      "Step: 0, Action: 4, Reward: 0.2, Total Reward: 0.2, Loss: 3.398623466491699\n",
      "Optimizing model...\n",
      "Step: 1, Action: 3, Reward: 0.25, Total Reward: 0.45, Loss: 6.923079967498779\n",
      "Optimizing model...\n",
      "Step: 2, Action: 3, Reward: 0.25, Total Reward: 0.7, Loss: 7.504945755004883\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: 10.0, Total Reward: 10.7, Loss: 2.6523804664611816\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: 0.0, Total Reward: 10.7, Loss: 3.7178163528442383\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 5, Action: 0, Reward: 10.0, Total Reward: 20.7, Loss: 1.7951682806015015\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 6, Action: 0, Reward: 0.0, Total Reward: 20.7, Loss: 1.733382225036621\n",
      "Optimizing model...\n",
      "Step: 7, Action: 0, Reward: 10.0, Total Reward: 30.7, Loss: 4.729686737060547\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 8, Action: 0, Reward: 0.0, Total Reward: 30.7, Loss: 5.766116619110107\n",
      "\n",
      "episode: 31, reward: -69.3\n",
      "Optimizing model...\n",
      "Step: 0, Action: 3, Reward: 0.25, Total Reward: 0.25, Loss: 2.251857280731201\n",
      "Optimizing model...\n",
      "Step: 1, Action: 4, Reward: 0.2, Total Reward: 0.45, Loss: 4.871866226196289\n",
      "Optimizing model...\n",
      "Step: 2, Action: 1, Reward: -5.0, Total Reward: -4.55, Loss: 3.806870460510254\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: 10.0, Total Reward: 5.45, Loss: 4.6798295974731445\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: 0.0, Total Reward: 5.45, Loss: 1.2089557647705078\n",
      "Optimizing model...\n",
      "Step: 5, Action: 4, Reward: 0.2, Total Reward: 5.65, Loss: 3.7238612174987793\n",
      "Optimizing model...\n",
      "Step: 6, Action: 1, Reward: -5.0, Total Reward: 0.6500000000000004, Loss: 4.742425918579102\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 7, Action: 0, Reward: 10.0, Total Reward: 10.65, Loss: 4.379847049713135\n",
      "Optimizing model...\n",
      "Step: 8, Action: 1, Reward: -5.0, Total Reward: 5.65, Loss: 5.823299884796143\n",
      "Optimizing model...\n",
      "Step: 9, Action: 4, Reward: 0.2, Total Reward: 5.8500000000000005, Loss: 4.824728965759277\n",
      "Optimizing model...\n",
      "Step: 10, Action: 0, Reward: 10.0, Total Reward: 15.850000000000001, Loss: 3.1709675788879395\n",
      "Optimizing model...\n",
      "Step: 11, Action: 4, Reward: 0.2, Total Reward: 16.05, Loss: 3.621262550354004\n",
      "Optimizing model...\n",
      "Step: 12, Action: 1, Reward: -5.0, Total Reward: 11.05, Loss: 2.172480583190918\n",
      "Optimizing model...\n",
      "Step: 13, Action: 3, Reward: 0.25, Total Reward: 11.3, Loss: 2.1332502365112305\n",
      "Optimizing model...\n",
      "Step: 14, Action: 0, Reward: 10.0, Total Reward: 21.3, Loss: 3.662959575653076\n",
      "Optimizing model...\n",
      "Step: 15, Action: 3, Reward: 0.25, Total Reward: 21.55, Loss: 3.9097535610198975\n",
      "Optimizing model...\n",
      "Step: 16, Action: 4, Reward: 0.2, Total Reward: 21.75, Loss: 6.428938865661621\n",
      "Optimizing model...\n",
      "Step: 17, Action: 4, Reward: -9.8, Total Reward: 11.95, Loss: 6.276354789733887\n",
      "Optimizing model...\n",
      "Step: 18, Action: 0, Reward: 10.0, Total Reward: 21.95, Loss: 5.3404083251953125\n",
      "Optimizing model...\n",
      "Step: 19, Action: 3, Reward: 0.25, Total Reward: 22.2, Loss: 2.811262607574463\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 20, Action: 0, Reward: 10.0, Total Reward: 32.2, Loss: 4.857677459716797\n",
      "Optimizing model...\n",
      "Step: 21, Action: 1, Reward: -5.0, Total Reward: 27.200000000000003, Loss: 4.784129619598389\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 22, Action: 0, Reward: 10.0, Total Reward: 37.2, Loss: 7.248301029205322\n",
      "Optimizing model...\n",
      "Step: 23, Action: 1, Reward: -5.0, Total Reward: 32.2, Loss: 4.773500442504883\n",
      "\n",
      "episode: 32, reward: -67.8\n",
      "Optimizing model...\n",
      "Step: 0, Action: 1, Reward: -5.0, Total Reward: -5.0, Loss: 5.376645088195801\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: 10.0, Total Reward: 5.0, Loss: 4.615184783935547\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: 10.0, Total Reward: 15.0, Loss: 4.803862571716309\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: 0.0, Total Reward: 15.0, Loss: 1.7512719631195068\n",
      "Optimizing model...\n",
      "Step: 4, Action: 2, Reward: 0.25, Total Reward: 15.25, Loss: 3.1821413040161133\n",
      "Optimizing model...\n",
      "Step: 5, Action: 2, Reward: -9.75, Total Reward: 5.5, Loss: 4.520330429077148\n",
      "\n",
      "episode: 33, reward: -94.5\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: 10.0, Total Reward: 10.0, Loss: 3.189866065979004\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: 10.0, Total Reward: 20.0, Loss: 7.1212263107299805\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: 0.0, Total Reward: 20.0, Loss: 9.993175506591797\n",
      "Optimizing model...\n",
      "Step: 3, Action: 1, Reward: -5.0, Total Reward: 15.0, Loss: 4.470614910125732\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: 10.0, Total Reward: 25.0, Loss: 5.359780788421631\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 5, Action: 0, Reward: 0.0, Total Reward: 25.0, Loss: 7.015105247497559\n",
      "Optimizing model...\n",
      "Step: 6, Action: 4, Reward: 0.2, Total Reward: 25.2, Loss: 6.256372451782227\n",
      "Optimizing model...\n",
      "Step: 7, Action: 2, Reward: 0.25, Total Reward: 25.45, Loss: 3.3066787719726562\n",
      "Optimizing model...\n",
      "Step: 8, Action: 1, Reward: -5.0, Total Reward: 20.45, Loss: 7.910356521606445\n",
      "Optimizing model...\n",
      "Step: 9, Action: 1, Reward: -5.0, Total Reward: 15.45, Loss: 3.577235698699951\n",
      "Optimizing model...\n",
      "Step: 10, Action: 0, Reward: 10.0, Total Reward: 25.45, Loss: 7.408112525939941\n",
      "Optimizing model...\n",
      "Step: 11, Action: 4, Reward: 0.2, Total Reward: 25.65, Loss: 1.665245771408081\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 12, Action: 0, Reward: 10.0, Total Reward: 35.65, Loss: 5.102359771728516\n",
      "Model acting\n",
      "\n",
      "episode: 34, reward: -64.35\n",
      "Optimizing model...\n",
      "Step: 0, Action: 1, Reward: -5.0, Total Reward: -5.0, Loss: 5.435146808624268\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: 10.0, Total Reward: 5.0, Loss: 4.628353595733643\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: 10.0, Total Reward: 15.0, Loss: 8.400383949279785\n",
      "Optimizing model...\n",
      "Step: 3, Action: 1, Reward: -5.0, Total Reward: 10.0, Loss: 2.118610143661499\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: 10.0, Total Reward: 20.0, Loss: 4.938323020935059\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 5, Action: 0, Reward: 10.0, Total Reward: 30.0, Loss: 4.047074317932129\n",
      "Optimizing model...\n",
      "Step: 6, Action: 1, Reward: -5.0, Total Reward: 25.0, Loss: 4.233826160430908\n",
      "Optimizing model...\n",
      "Step: 7, Action: 0, Reward: 10.0, Total Reward: 35.0, Loss: 4.731418609619141\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 8, Action: 0, Reward: 0.0, Total Reward: 35.0, Loss: 4.813075542449951\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 9, Action: 0, Reward: 10.0, Total Reward: 45.0, Loss: 2.060751438140869\n",
      "Optimizing model...\n",
      "Step: 10, Action: 1, Reward: -5.0, Total Reward: 40.0, Loss: 6.011373996734619\n",
      "\n",
      "episode: 35, reward: -60.0\n",
      "Optimizing model...\n",
      "Step: 0, Action: 2, Reward: 0.25, Total Reward: 0.25, Loss: 3.274681568145752\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: 10.0, Total Reward: 10.25, Loss: 4.735428333282471\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: 10.0, Total Reward: 20.25, Loss: 3.1729512214660645\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: 10.0, Total Reward: 30.25, Loss: 3.6699843406677246\n",
      "Optimizing model...\n",
      "Step: 4, Action: 3, Reward: 0.25, Total Reward: 30.5, Loss: 4.826341152191162\n",
      "Model acting\n",
      "\n",
      "episode: 36, reward: -69.5\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: 10.0, Total Reward: 10.0, Loss: 5.562689781188965\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: 10.0, Total Reward: 20.0, Loss: 3.927799701690674\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: 10.0, Total Reward: 30.0, Loss: 2.866091728210449\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: 10.0, Total Reward: 40.0, Loss: 5.5588765144348145\n",
      "Model acting\n",
      "\n",
      "episode: 37, reward: -60.0\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: 10.0, Total Reward: 10.0, Loss: 2.9862735271453857\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: 10.0, Total Reward: 20.0, Loss: 6.522325038909912\n",
      "Optimizing model...\n",
      "Step: 2, Action: 3, Reward: 0.25, Total Reward: 20.25, Loss: 2.7771263122558594\n",
      "Optimizing model...\n",
      "Step: 3, Action: 1, Reward: -5.0, Total Reward: 15.25, Loss: 8.008111953735352\n",
      "Optimizing model...\n",
      "Step: 4, Action: 4, Reward: 0.2, Total Reward: 15.45, Loss: 9.104928016662598\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 5, Action: 0, Reward: 10.0, Total Reward: 25.45, Loss: 9.167811393737793\n",
      "Optimizing model...\n",
      "Step: 6, Action: 0, Reward: 10.0, Total Reward: 35.45, Loss: 5.244134902954102\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 7, Action: 0, Reward: 0.0, Total Reward: 35.45, Loss: 6.885125637054443\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 8, Action: 0, Reward: 10.0, Total Reward: 45.45, Loss: 3.686197280883789\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 9, Action: 0, Reward: 0.0, Total Reward: 45.45, Loss: 3.226938009262085\n",
      "Model acting\n",
      "\n",
      "episode: 38, reward: -54.55\n",
      "Optimizing model...\n",
      "Step: 0, Action: 4, Reward: 0.2, Total Reward: 0.2, Loss: 4.898629665374756\n",
      "Optimizing model...\n",
      "Step: 1, Action: 1, Reward: -5.0, Total Reward: -4.8, Loss: 1.8980238437652588\n",
      "Optimizing model...\n",
      "Step: 2, Action: 3, Reward: 0.25, Total Reward: -4.55, Loss: 3.9909751415252686\n",
      "Optimizing model...\n",
      "Step: 3, Action: 4, Reward: 0.2, Total Reward: -4.35, Loss: 3.881904125213623\n",
      "Optimizing model...\n",
      "Step: 4, Action: 4, Reward: -9.8, Total Reward: -14.15, Loss: 5.027639389038086\n",
      "Optimizing model...\n",
      "Step: 5, Action: 0, Reward: 10.0, Total Reward: -4.15, Loss: 2.0628066062927246\n",
      "Optimizing model...\n",
      "Step: 6, Action: 4, Reward: 0.2, Total Reward: -3.95, Loss: 2.397650957107544\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 7, Action: 0, Reward: 10.0, Total Reward: 6.05, Loss: 1.947615146636963\n",
      "Optimizing model...\n",
      "Step: 8, Action: 4, Reward: 0.2, Total Reward: 6.25, Loss: 3.52868914604187\n",
      "Optimizing model...\n",
      "Step: 9, Action: 1, Reward: -5.0, Total Reward: 1.25, Loss: 8.331235885620117\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 10, Action: 0, Reward: 10.0, Total Reward: 11.25, Loss: 2.062201499938965\n",
      "Optimizing model...\n",
      "Step: 11, Action: 0, Reward: 0.0, Total Reward: 11.25, Loss: 6.06125545501709\n",
      "Optimizing model...\n",
      "Step: 12, Action: 3, Reward: 0.25, Total Reward: 11.5, Loss: 3.254507064819336\n",
      "\n",
      "episode: 39, reward: -88.5\n",
      "Optimizing model...\n",
      "Step: 0, Action: 3, Reward: 0.25, Total Reward: 0.25, Loss: 6.067109107971191\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: 10.0, Total Reward: 10.25, Loss: 4.614959716796875\n",
      "Optimizing model...\n",
      "Step: 2, Action: 2, Reward: 0.25, Total Reward: 10.5, Loss: 8.494128227233887\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: 10.0, Total Reward: 20.5, Loss: 6.014791011810303\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: 10.0, Total Reward: 30.5, Loss: 3.5494213104248047\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 5, Action: 0, Reward: 10.0, Total Reward: 40.5, Loss: 3.6623337268829346\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 6, Action: 0, Reward: 10.0, Total Reward: 50.5, Loss: 3.4078285694122314\n",
      "Optimizing model...\n",
      "Step: 7, Action: 4, Reward: 0.2, Total Reward: 50.7, Loss: 7.474900245666504\n",
      "Model acting\n",
      "\n",
      "episode: 40, reward: -49.3\n",
      "Model saved\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: 10.0, Total Reward: 10.0, Loss: 3.491518020629883\n",
      "Optimizing model...\n",
      "Step: 1, Action: 3, Reward: 0.25, Total Reward: 10.25, Loss: 7.561105728149414\n",
      "Optimizing model...\n",
      "Step: 2, Action: 1, Reward: -5.0, Total Reward: 5.25, Loss: 7.606574058532715\n",
      "Optimizing model...\n",
      "Step: 3, Action: 1, Reward: -5.0, Total Reward: 0.25, Loss: 3.359454870223999\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: 10.0, Total Reward: 10.25, Loss: 4.015100002288818\n",
      "Optimizing model...\n",
      "Step: 5, Action: 3, Reward: 0.25, Total Reward: 10.5, Loss: 2.247342586517334\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 6, Action: 0, Reward: 10.0, Total Reward: 20.5, Loss: 2.371762990951538\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 7, Action: 0, Reward: 10.0, Total Reward: 30.5, Loss: 5.236682415008545\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 8, Action: 0, Reward: 0.0, Total Reward: 30.5, Loss: 4.238748550415039\n",
      "Optimizing model...\n",
      "Step: 9, Action: 2, Reward: 0.25, Total Reward: 30.75, Loss: 1.8362712860107422\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 10, Action: 0, Reward: 10.0, Total Reward: 40.75, Loss: 1.7099603414535522\n",
      "Optimizing model...\n",
      "Step: 11, Action: 3, Reward: 0.25, Total Reward: 41.0, Loss: 3.421677589416504\n",
      "Optimizing model...\n",
      "Step: 12, Action: 0, Reward: 10.0, Total Reward: 51.0, Loss: 4.748478412628174\n",
      "Optimizing model...\n",
      "Step: 13, Action: 4, Reward: 0.2, Total Reward: 51.2, Loss: 2.694735050201416\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 14, Action: 0, Reward: 10.0, Total Reward: 61.2, Loss: 5.414109230041504\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 15, Action: 0, Reward: 10.0, Total Reward: 71.2, Loss: 4.733585357666016\n",
      "\n",
      "episode: 41, reward: -28.799999999999997\n",
      "Optimizing model...\n",
      "Step: 0, Action: 4, Reward: 0.2, Total Reward: 0.2, Loss: 7.810758113861084\n",
      "Optimizing model...\n",
      "Step: 1, Action: 2, Reward: 0.25, Total Reward: 0.45, Loss: 3.6144909858703613\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: 10.0, Total Reward: 10.45, Loss: 5.247839450836182\n",
      "Optimizing model...\n",
      "Step: 3, Action: 4, Reward: 0.2, Total Reward: 10.649999999999999, Loss: 6.900864124298096\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: 10.0, Total Reward: 20.65, Loss: 5.706845283508301\n",
      "Optimizing model...\n",
      "Step: 5, Action: 1, Reward: -5.0, Total Reward: 15.649999999999999, Loss: 8.696269989013672\n",
      "Optimizing model...\n",
      "Step: 6, Action: 1, Reward: -5.0, Total Reward: 10.649999999999999, Loss: 5.639829635620117\n",
      "Optimizing model...\n",
      "Step: 7, Action: 2, Reward: 0.25, Total Reward: 10.899999999999999, Loss: 4.497113227844238\n",
      "Optimizing model...\n",
      "Step: 8, Action: 4, Reward: 0.2, Total Reward: 11.099999999999998, Loss: 4.480048179626465\n",
      "Optimizing model...\n",
      "Step: 9, Action: 4, Reward: -9.8, Total Reward: 1.2999999999999972, Loss: 8.664040565490723\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 10, Action: 0, Reward: 10.0, Total Reward: 11.299999999999997, Loss: 4.5889129638671875\n",
      "Optimizing model...\n",
      "Step: 11, Action: 0, Reward: 10.0, Total Reward: 21.299999999999997, Loss: 7.661600112915039\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 12, Action: 0, Reward: 0.0, Total Reward: 21.299999999999997, Loss: 6.802596092224121\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 13, Action: 0, Reward: 10.0, Total Reward: 31.299999999999997, Loss: 7.929296493530273\n",
      "Optimizing model...\n",
      "Step: 14, Action: 2, Reward: 0.25, Total Reward: 31.549999999999997, Loss: 5.873044967651367\n",
      "Optimizing model...\n",
      "Step: 15, Action: 4, Reward: 0.2, Total Reward: 31.749999999999996, Loss: 5.020456790924072\n",
      "Optimizing model...\n",
      "Step: 16, Action: 2, Reward: 0.25, Total Reward: 31.999999999999996, Loss: 4.989808082580566\n",
      "Optimizing model...\n",
      "Step: 17, Action: 1, Reward: -5.0, Total Reward: 26.999999999999996, Loss: 1.696122169494629\n",
      "Optimizing model...\n",
      "Step: 18, Action: 4, Reward: 0.2, Total Reward: 27.199999999999996, Loss: 7.90699577331543\n",
      "Optimizing model...\n",
      "Step: 19, Action: 1, Reward: -5.0, Total Reward: 22.199999999999996, Loss: 1.7557939291000366\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 20, Action: 0, Reward: 10.0, Total Reward: 32.199999999999996, Loss: 6.826228141784668\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 21, Action: 0, Reward: 0.0, Total Reward: 32.199999999999996, Loss: 1.8655664920806885\n",
      "Optimizing model...\n",
      "Step: 22, Action: 1, Reward: -5.0, Total Reward: 27.199999999999996, Loss: 9.505264282226562\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 23, Action: 0, Reward: 10.0, Total Reward: 37.199999999999996, Loss: 6.702280044555664\n",
      "Model acting\n",
      "\n",
      "episode: 42, reward: -62.800000000000004\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: 10.0, Total Reward: 10.0, Loss: 6.509514808654785\n",
      "Optimizing model...\n",
      "Step: 1, Action: 3, Reward: 0.25, Total Reward: 10.25, Loss: 6.002361297607422\n",
      "Optimizing model...\n",
      "Step: 2, Action: 4, Reward: 0.2, Total Reward: 10.45, Loss: 6.504929542541504\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: 10.0, Total Reward: 20.45, Loss: 4.09596061706543\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: 10.0, Total Reward: 30.45, Loss: 7.83986759185791\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 5, Action: 0, Reward: 0.0, Total Reward: 30.45, Loss: 5.462124824523926\n",
      "Optimizing model...\n",
      "Step: 6, Action: 0, Reward: 10.0, Total Reward: 40.45, Loss: 3.7004342079162598\n",
      "\n",
      "episode: 43, reward: -59.55\n",
      "Optimizing model...\n",
      "Step: 0, Action: 4, Reward: 0.2, Total Reward: 0.2, Loss: 6.348885536193848\n",
      "Optimizing model...\n",
      "Step: 1, Action: 3, Reward: 0.25, Total Reward: 0.45, Loss: 2.787421703338623\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: 10.0, Total Reward: 10.45, Loss: 1.654566764831543\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: 10.0, Total Reward: 20.45, Loss: 5.938940048217773\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: 0.0, Total Reward: 20.45, Loss: 7.837950706481934\n",
      "Optimizing model...\n",
      "Step: 5, Action: 2, Reward: 0.25, Total Reward: 20.7, Loss: 4.769058704376221\n",
      "Optimizing model...\n",
      "Step: 6, Action: 2, Reward: -9.75, Total Reward: 10.95, Loss: 9.60114574432373\n",
      "Optimizing model...\n",
      "Step: 7, Action: 0, Reward: 10.0, Total Reward: 20.95, Loss: 3.7801074981689453\n",
      "Optimizing model...\n",
      "Step: 8, Action: 3, Reward: 0.25, Total Reward: 21.2, Loss: 3.4143829345703125\n",
      "Optimizing model...\n",
      "Step: 9, Action: 3, Reward: -9.75, Total Reward: 11.45, Loss: 5.596901893615723\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 10, Action: 0, Reward: 10.0, Total Reward: 21.45, Loss: 9.709455490112305\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 11, Action: 0, Reward: 0.0, Total Reward: 21.45, Loss: 3.983304738998413\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 12, Action: 0, Reward: 0.0, Total Reward: 21.45, Loss: 6.59979772567749\n",
      "Optimizing model...\n",
      "Step: 13, Action: 2, Reward: 0.25, Total Reward: 21.7, Loss: 3.3764638900756836\n",
      "Optimizing model...\n",
      "Step: 14, Action: 0, Reward: 10.0, Total Reward: 31.7, Loss: 7.53232479095459\n",
      "Optimizing model...\n",
      "Step: 15, Action: 3, Reward: 0.25, Total Reward: 31.95, Loss: 10.483945846557617\n",
      "Optimizing model...\n",
      "Step: 16, Action: 3, Reward: -9.75, Total Reward: 22.2, Loss: 4.334460735321045\n",
      "Optimizing model...\n",
      "Step: 17, Action: 0, Reward: 10.0, Total Reward: 32.2, Loss: 3.5893149375915527\n",
      "Optimizing model...\n",
      "Step: 18, Action: 2, Reward: 0.25, Total Reward: 32.45, Loss: 6.9145965576171875\n",
      "Optimizing model...\n",
      "Step: 19, Action: 0, Reward: 10.0, Total Reward: 42.45, Loss: 1.5799190998077393\n",
      "Optimizing model...\n",
      "Step: 20, Action: 3, Reward: 0.25, Total Reward: 42.7, Loss: 4.666192054748535\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 21, Action: 0, Reward: 10.0, Total Reward: 52.7, Loss: 7.422712326049805\n",
      "Model acting\n",
      "\n",
      "episode: 44, reward: -47.3\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: 10.0, Total Reward: 10.0, Loss: 4.209099769592285\n",
      "Optimizing model...\n",
      "Step: 1, Action: 2, Reward: 0.25, Total Reward: 10.25, Loss: 3.6628966331481934\n",
      "Optimizing model...\n",
      "Step: 2, Action: 1, Reward: -5.0, Total Reward: 5.25, Loss: 3.569631814956665\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: 10.0, Total Reward: 15.25, Loss: 6.17015266418457\n",
      "Optimizing model...\n",
      "Step: 4, Action: 4, Reward: 0.2, Total Reward: 15.45, Loss: 5.464792251586914\n",
      "Optimizing model...\n",
      "Step: 5, Action: 4, Reward: -9.8, Total Reward: 5.649999999999999, Loss: 5.578290939331055\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 6, Action: 0, Reward: 10.0, Total Reward: 15.649999999999999, Loss: 6.276808261871338\n",
      "Optimizing model...\n",
      "Step: 7, Action: 4, Reward: 0.2, Total Reward: 15.849999999999998, Loss: 5.123880386352539\n",
      "Model acting\n",
      "\n",
      "episode: 45, reward: -84.15\n",
      "Optimizing model...\n",
      "Step: 0, Action: 1, Reward: -5.0, Total Reward: -5.0, Loss: 4.609287261962891\n",
      "Optimizing model...\n",
      "Step: 1, Action: 3, Reward: 0.25, Total Reward: -4.75, Loss: 1.4823752641677856\n",
      "Optimizing model...\n",
      "Step: 2, Action: 1, Reward: -5.0, Total Reward: -9.75, Loss: 2.739335775375366\n",
      "Optimizing model...\n",
      "Step: 3, Action: 3, Reward: 0.25, Total Reward: -9.5, Loss: 8.729290008544922\n",
      "Optimizing model...\n",
      "Step: 4, Action: 3, Reward: -9.75, Total Reward: -19.25, Loss: 2.2339844703674316\n",
      "Optimizing model...\n",
      "Step: 5, Action: 2, Reward: 0.25, Total Reward: -19.0, Loss: 1.4588027000427246\n",
      "Optimizing model...\n",
      "Step: 6, Action: 4, Reward: 0.2, Total Reward: -18.8, Loss: 4.540534973144531\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 7, Action: 0, Reward: 10.0, Total Reward: -8.8, Loss: 8.324054718017578\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 8, Action: 0, Reward: 0.0, Total Reward: -8.8, Loss: 3.612051248550415\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 9, Action: 0, Reward: 10.0, Total Reward: 1.1999999999999993, Loss: 5.048803329467773\n",
      "Optimizing model...\n",
      "Step: 10, Action: 0, Reward: 0.0, Total Reward: 1.1999999999999993, Loss: 1.9646878242492676\n",
      "Optimizing model...\n",
      "Step: 11, Action: 4, Reward: 0.2, Total Reward: 1.3999999999999992, Loss: 5.170571327209473\n",
      "Optimizing model...\n",
      "Step: 12, Action: 3, Reward: 0.25, Total Reward: 1.6499999999999992, Loss: 5.486874103546143\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 13, Action: 0, Reward: 10.0, Total Reward: 11.649999999999999, Loss: 3.6872894763946533\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 14, Action: 0, Reward: 10.0, Total Reward: 21.65, Loss: 6.42742919921875\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 15, Action: 4, Reward: 0.2, Total Reward: 21.849999999999998, Loss: 7.694202423095703\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 16, Action: 4, Reward: -9.8, Total Reward: 12.049999999999997, Loss: 7.898792266845703\n",
      "Optimizing model...\n",
      "Step: 17, Action: 4, Reward: -9.8, Total Reward: 2.2499999999999964, Loss: 2.046510696411133\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 18, Action: 0, Reward: 10.0, Total Reward: 12.249999999999996, Loss: 4.048480987548828\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 19, Action: 0, Reward: 0.0, Total Reward: 12.249999999999996, Loss: 1.8497569561004639\n",
      "Model acting\n",
      "\n",
      "episode: 46, reward: -87.75\n",
      "Optimizing model...\n",
      "Step: 0, Action: 3, Reward: 0.25, Total Reward: 0.25, Loss: 3.6535048484802246\n",
      "Optimizing model...\n",
      "Step: 1, Action: 1, Reward: -5.0, Total Reward: -4.75, Loss: 3.846583366394043\n",
      "Optimizing model...\n",
      "Step: 2, Action: 2, Reward: 0.25, Total Reward: -4.5, Loss: 2.695431709289551\n",
      "Optimizing model...\n",
      "Step: 3, Action: 1, Reward: -5.0, Total Reward: -9.5, Loss: 2.2821993827819824\n",
      "Optimizing model...\n",
      "Step: 4, Action: 1, Reward: -15.0, Total Reward: -24.5, Loss: 7.795830726623535\n",
      "Optimizing model...\n",
      "Step: 5, Action: 0, Reward: 10.0, Total Reward: -14.5, Loss: 6.1077985763549805\n",
      "Optimizing model...\n",
      "Step: 6, Action: 3, Reward: 0.25, Total Reward: -14.25, Loss: 9.167741775512695\n",
      "Optimizing model...\n",
      "Step: 7, Action: 2, Reward: 0.25, Total Reward: -14.0, Loss: 5.626664161682129\n",
      "Optimizing model...\n",
      "Step: 8, Action: 3, Reward: 0.25, Total Reward: -13.75, Loss: 8.570474624633789\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 9, Action: 0, Reward: 10.0, Total Reward: -3.75, Loss: 3.493485927581787\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 10, Action: 0, Reward: 0.0, Total Reward: -3.75, Loss: 6.340267181396484\n",
      "Optimizing model...\n",
      "Step: 11, Action: 0, Reward: 0.0, Total Reward: -3.75, Loss: 2.667144536972046\n",
      "Optimizing model...\n",
      "Step: 12, Action: 0, Reward: 0.0, Total Reward: -3.75, Loss: 8.467243194580078\n",
      "Optimizing model...\n",
      "Step: 13, Action: 1, Reward: -5.0, Total Reward: -8.75, Loss: 1.810150146484375\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 14, Action: 3, Reward: 0.25, Total Reward: -8.5, Loss: 5.142397403717041\n",
      "Optimizing model...\n",
      "Step: 15, Action: 1, Reward: -5.0, Total Reward: -13.5, Loss: 3.543456554412842\n",
      "Model acting\n",
      "\n",
      "episode: 47, reward: -113.5\n",
      "Optimizing model...\n",
      "Step: 0, Action: 2, Reward: 0.25, Total Reward: 0.25, Loss: 3.3520259857177734\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: 10.0, Total Reward: 10.25, Loss: 4.022864818572998\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: 0.0, Total Reward: 10.25, Loss: 6.223803520202637\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: 10.0, Total Reward: 20.25, Loss: 6.832239151000977\n",
      "Model acting\n",
      "\n",
      "episode: 48, reward: -79.75\n",
      "Optimizing model...\n",
      "Step: 0, Action: 3, Reward: 0.25, Total Reward: 0.25, Loss: 2.3179731369018555\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: 10.0, Total Reward: 10.25, Loss: 3.5924692153930664\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: 0.0, Total Reward: 10.25, Loss: 5.426703453063965\n",
      "Optimizing model...\n",
      "Step: 3, Action: 1, Reward: -5.0, Total Reward: 5.25, Loss: 1.704343318939209\n",
      "Optimizing model...\n",
      "Step: 4, Action: 2, Reward: 0.25, Total Reward: 5.5, Loss: 3.402866840362549\n",
      "Optimizing model...\n",
      "Step: 5, Action: 2, Reward: 0.25, Total Reward: 5.75, Loss: 7.151419162750244\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 6, Action: 4, Reward: 0.2, Total Reward: 5.95, Loss: 5.180556774139404\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 7, Action: 4, Reward: -9.8, Total Reward: -3.8500000000000005, Loss: 3.81587553024292\n",
      "Optimizing model...\n",
      "Step: 8, Action: 1, Reward: -5.0, Total Reward: -8.850000000000001, Loss: 3.3985233306884766\n",
      "Optimizing model...\n",
      "Step: 9, Action: 4, Reward: 0.2, Total Reward: -8.650000000000002, Loss: 8.226964950561523\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 10, Action: 4, Reward: -9.8, Total Reward: -18.450000000000003, Loss: 7.615897178649902\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 11, Action: 4, Reward: -9.8, Total Reward: -28.250000000000004, Loss: 4.490128993988037\n",
      "Optimizing model...\n",
      "Step: 12, Action: 3, Reward: 0.25, Total Reward: -28.000000000000004, Loss: 3.04060435295105\n",
      "Optimizing model...\n",
      "Step: 13, Action: 3, Reward: -9.75, Total Reward: -37.75, Loss: 4.958464622497559\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 14, Action: 4, Reward: 0.2, Total Reward: -37.55, Loss: 3.2214248180389404\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 15, Action: 0, Reward: 10.0, Total Reward: -27.549999999999997, Loss: 6.998677730560303\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 16, Action: 4, Reward: 0.2, Total Reward: -27.349999999999998, Loss: 5.450503349304199\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 17, Action: 4, Reward: -9.8, Total Reward: -37.15, Loss: 4.1043500900268555\n",
      "Optimizing model...\n",
      "Step: 18, Action: 4, Reward: -9.8, Total Reward: -46.95, Loss: 5.178495407104492\n",
      "Optimizing model...\n",
      "Step: 19, Action: 1, Reward: -5.0, Total Reward: -51.95, Loss: 2.5790815353393555\n",
      "\n",
      "episode: 49, reward: -151.95\n",
      "Optimizing model...\n",
      "Step: 0, Action: 2, Reward: 0.25, Total Reward: 0.25, Loss: 1.8540019989013672\n",
      "Optimizing model...\n",
      "Step: 1, Action: 3, Reward: 0.25, Total Reward: 0.5, Loss: 1.8548023700714111\n",
      "Optimizing model...\n",
      "Step: 2, Action: 1, Reward: -5.0, Total Reward: -4.5, Loss: 6.027030944824219\n",
      "Optimizing model...\n",
      "Step: 3, Action: 1, Reward: -5.0, Total Reward: -9.5, Loss: 4.704128265380859\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: 10.0, Total Reward: 0.5, Loss: 3.200532913208008\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 5, Action: 0, Reward: 10.0, Total Reward: 10.5, Loss: 6.566860198974609\n",
      "Optimizing model...\n",
      "Step: 6, Action: 2, Reward: 0.25, Total Reward: 10.75, Loss: 3.467654228210449\n",
      "Optimizing model...\n",
      "Step: 7, Action: 4, Reward: 0.2, Total Reward: 10.95, Loss: 4.561282634735107\n",
      "Optimizing model...\n",
      "Step: 8, Action: 0, Reward: 10.0, Total Reward: 20.95, Loss: 5.674659252166748\n",
      "Optimizing model...\n",
      "Step: 9, Action: 0, Reward: 0.0, Total Reward: 20.95, Loss: 1.2870367765426636\n",
      "Optimizing model...\n",
      "Step: 10, Action: 3, Reward: 0.25, Total Reward: 21.2, Loss: 6.183834075927734\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 11, Action: 0, Reward: 10.0, Total Reward: 31.2, Loss: 2.2293496131896973\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 12, Action: 0, Reward: 0.0, Total Reward: 31.2, Loss: 6.545924186706543\n",
      "Optimizing model...\n",
      "Step: 13, Action: 0, Reward: 10.0, Total Reward: 41.2, Loss: 2.667933940887451\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 14, Action: 0, Reward: 0.0, Total Reward: 41.2, Loss: 4.348061561584473\n",
      "Optimizing model...\n",
      "Step: 15, Action: 2, Reward: 0.25, Total Reward: 41.45, Loss: 5.228173732757568\n",
      "Optimizing model...\n",
      "Step: 16, Action: 2, Reward: 0.25, Total Reward: 41.7, Loss: 5.182290077209473\n",
      "Optimizing model...\n",
      "Step: 17, Action: 4, Reward: 0.2, Total Reward: 41.900000000000006, Loss: 4.597701072692871\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 18, Action: 0, Reward: 10.0, Total Reward: 51.900000000000006, Loss: 4.173029899597168\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 19, Action: 0, Reward: 0.0, Total Reward: 51.900000000000006, Loss: 3.2307558059692383\n",
      "Model acting\n",
      "\n",
      "episode: 50, reward: -48.099999999999994\n",
      "Model saved\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: 10.0, Total Reward: 10.0, Loss: 3.7270381450653076\n",
      "Optimizing model...\n",
      "Step: 1, Action: 4, Reward: 0.2, Total Reward: 10.2, Loss: 8.217385292053223\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: 10.0, Total Reward: 20.2, Loss: 6.0960893630981445\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 4, Reward: 0.2, Total Reward: 20.4, Loss: 4.13453483581543\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: 10.0, Total Reward: 30.4, Loss: 4.310919761657715\n",
      "Optimizing model...\n",
      "Step: 5, Action: 3, Reward: 0.25, Total Reward: 30.65, Loss: 4.590444564819336\n",
      "\n",
      "episode: 51, reward: -69.35\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: 10.0, Total Reward: 10.0, Loss: 2.2665066719055176\n",
      "Optimizing model...\n",
      "Step: 1, Action: 3, Reward: 0.25, Total Reward: 10.25, Loss: 4.202988624572754\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: 10.0, Total Reward: 20.25, Loss: 3.2899458408355713\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: 10.0, Total Reward: 30.25, Loss: 6.232204437255859\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: 0.0, Total Reward: 30.25, Loss: 6.66708517074585\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 5, Action: 3, Reward: 0.25, Total Reward: 30.5, Loss: 4.712276458740234\n",
      "Model acting\n",
      "\n",
      "episode: 52, reward: -69.5\n",
      "Optimizing model...\n",
      "Step: 0, Action: 3, Reward: 0.25, Total Reward: 0.25, Loss: 1.790111780166626\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: 10.0, Total Reward: 10.25, Loss: 3.747894287109375\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: 10.0, Total Reward: 20.25, Loss: 3.329195737838745\n",
      "Optimizing model...\n",
      "Step: 3, Action: 3, Reward: 0.25, Total Reward: 20.5, Loss: 5.979784965515137\n",
      "Optimizing model...\n",
      "Step: 4, Action: 4, Reward: 0.2, Total Reward: 20.7, Loss: 4.562558174133301\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 5, Action: 0, Reward: 10.0, Total Reward: 30.7, Loss: 5.152456283569336\n",
      "Optimizing model...\n",
      "Step: 6, Action: 0, Reward: 10.0, Total Reward: 40.7, Loss: 8.479673385620117\n",
      "Optimizing model...\n",
      "Step: 7, Action: 3, Reward: 0.25, Total Reward: 40.95, Loss: 3.529555320739746\n",
      "Model acting\n",
      "\n",
      "episode: 53, reward: -59.05\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: 10.0, Total Reward: 10.0, Loss: 1.6672956943511963\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: 10.0, Total Reward: 20.0, Loss: 4.985157012939453\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: 10.0, Total Reward: 30.0, Loss: 5.461821556091309\n",
      "Optimizing model...\n",
      "Step: 3, Action: 1, Reward: -5.0, Total Reward: 25.0, Loss: 7.887338161468506\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: 10.0, Total Reward: 35.0, Loss: 6.956752300262451\n",
      "Optimizing model...\n",
      "Step: 5, Action: 0, Reward: 10.0, Total Reward: 45.0, Loss: 1.9593346118927002\n",
      "Optimizing model...\n",
      "Step: 6, Action: 1, Reward: -5.0, Total Reward: 40.0, Loss: 1.9610488414764404\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 7, Action: 0, Reward: 10.0, Total Reward: 50.0, Loss: 5.2263078689575195\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 8, Action: 4, Reward: 0.2, Total Reward: 50.2, Loss: 1.585124135017395\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 9, Action: 4, Reward: 0.2, Total Reward: 50.400000000000006, Loss: 4.864896297454834\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 10, Action: 4, Reward: -9.8, Total Reward: 40.60000000000001, Loss: 10.596946716308594\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 11, Action: 0, Reward: 10.0, Total Reward: 50.60000000000001, Loss: 3.88432240486145\n",
      "Optimizing model...\n",
      "Step: 12, Action: 1, Reward: -5.0, Total Reward: 45.60000000000001, Loss: 3.806222438812256\n",
      "Model acting\n",
      "\n",
      "episode: 54, reward: -54.39999999999999\n",
      "Optimizing model...\n",
      "Step: 0, Action: 1, Reward: -5.0, Total Reward: -5.0, Loss: 2.6318271160125732\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: 10.0, Total Reward: 5.0, Loss: 4.348838806152344\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: 10.0, Total Reward: 15.0, Loss: 4.750186920166016\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: 0.0, Total Reward: 15.0, Loss: 7.462262153625488\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: 10.0, Total Reward: 25.0, Loss: 4.8568010330200195\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 5, Action: 0, Reward: 10.0, Total Reward: 35.0, Loss: 3.6819262504577637\n",
      "Model acting\n",
      "\n",
      "episode: 55, reward: -65.0\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: 10.0, Total Reward: 10.0, Loss: 1.596315622329712\n",
      "Optimizing model...\n",
      "Step: 1, Action: 1, Reward: -5.0, Total Reward: 5.0, Loss: 8.127230644226074\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: 10.0, Total Reward: 15.0, Loss: 5.736660003662109\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: 10.0, Total Reward: 25.0, Loss: 3.737820625305176\n",
      "Optimizing model...\n",
      "Step: 4, Action: 3, Reward: 0.25, Total Reward: 25.25, Loss: 3.727752447128296\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 5, Action: 0, Reward: 10.0, Total Reward: 35.25, Loss: 10.69617748260498\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 6, Action: 0, Reward: 0.0, Total Reward: 35.25, Loss: 5.608920097351074\n",
      "Optimizing model...\n",
      "Step: 7, Action: 1, Reward: -5.0, Total Reward: 30.25, Loss: 8.276342391967773\n",
      "Optimizing model...\n",
      "Step: 8, Action: 4, Reward: 0.2, Total Reward: 30.45, Loss: 3.4451005458831787\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 9, Action: 0, Reward: 10.0, Total Reward: 40.45, Loss: 3.574143886566162\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 10, Action: 0, Reward: 0.0, Total Reward: 40.45, Loss: 8.195978164672852\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 11, Action: 0, Reward: 0.0, Total Reward: 40.45, Loss: 6.9082441329956055\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 12, Action: 0, Reward: 10.0, Total Reward: 50.45, Loss: 3.6235878467559814\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 13, Action: 0, Reward: 0.0, Total Reward: 50.45, Loss: 4.7668561935424805\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 14, Action: 0, Reward: 0.0, Total Reward: 50.45, Loss: 4.203432083129883\n",
      "Optimizing model...\n",
      "Step: 15, Action: 0, Reward: 0.0, Total Reward: 50.45, Loss: 4.150434494018555\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 16, Action: 0, Reward: 0.0, Total Reward: 50.45, Loss: 8.388376235961914\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 17, Action: 0, Reward: 0.0, Total Reward: 50.45, Loss: 3.5794739723205566\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 18, Action: 0, Reward: 0.0, Total Reward: 50.45, Loss: 1.7707347869873047\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 19, Action: 0, Reward: 0.0, Total Reward: 50.45, Loss: 8.023527145385742\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 20, Action: 0, Reward: 10.0, Total Reward: 60.45, Loss: 6.480780124664307\n",
      "Optimizing model...\n",
      "Step: 21, Action: 3, Reward: 0.25, Total Reward: 60.7, Loss: 3.493767261505127\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 22, Action: 0, Reward: 10.0, Total Reward: 70.7, Loss: 7.417157173156738\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 23, Action: 0, Reward: 10.0, Total Reward: 80.7, Loss: 5.670746803283691\n",
      "Model acting\n",
      "\n",
      "episode: 56, reward: -19.299999999999997\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: 10.0, Total Reward: 10.0, Loss: 7.418312072753906\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: 10.0, Total Reward: 20.0, Loss: 3.2512409687042236\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: 10.0, Total Reward: 30.0, Loss: 5.120751857757568\n",
      "Optimizing model...\n",
      "Step: 3, Action: 3, Reward: 0.25, Total Reward: 30.25, Loss: 2.692568302154541\n",
      "Optimizing model...\n",
      "Step: 4, Action: 1, Reward: -5.0, Total Reward: 25.25, Loss: 4.928699016571045\n",
      "Optimizing model...\n",
      "Step: 5, Action: 0, Reward: 10.0, Total Reward: 35.25, Loss: 5.061491966247559\n",
      "Optimizing model...\n",
      "Step: 6, Action: 1, Reward: -5.0, Total Reward: 30.25, Loss: 6.375925540924072\n",
      "Optimizing model...\n",
      "Step: 7, Action: 0, Reward: 10.0, Total Reward: 40.25, Loss: 3.878251552581787\n",
      "Optimizing model...\n",
      "Step: 8, Action: 4, Reward: 0.2, Total Reward: 40.45, Loss: 4.910327911376953\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 9, Action: 0, Reward: 10.0, Total Reward: 50.45, Loss: 6.7737836837768555\n",
      "Optimizing model...\n",
      "Step: 10, Action: 1, Reward: -5.0, Total Reward: 45.45, Loss: 4.5436530113220215\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 11, Action: 0, Reward: 10.0, Total Reward: 55.45, Loss: 7.1982741355896\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 12, Action: 0, Reward: 0.0, Total Reward: 55.45, Loss: 7.595428943634033\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 13, Action: 0, Reward: 0.0, Total Reward: 55.45, Loss: 6.165094375610352\n",
      "Model acting\n",
      "\n",
      "episode: 57, reward: -44.55\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: 10.0, Total Reward: 10.0, Loss: 3.506796360015869\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: 10.0, Total Reward: 20.0, Loss: 4.660146713256836\n",
      "Optimizing model...\n",
      "Step: 2, Action: 2, Reward: 0.25, Total Reward: 20.25, Loss: 3.67440128326416\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: 10.0, Total Reward: 30.25, Loss: 3.3771305084228516\n",
      "Optimizing model...\n",
      "Step: 4, Action: 2, Reward: 0.25, Total Reward: 30.5, Loss: 7.668009281158447\n",
      "Optimizing model...\n",
      "Step: 5, Action: 4, Reward: 0.2, Total Reward: 30.7, Loss: 3.1493067741394043\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 6, Action: 0, Reward: 10.0, Total Reward: 40.7, Loss: 4.6331400871276855\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 7, Action: 0, Reward: 10.0, Total Reward: 50.7, Loss: 3.9934606552124023\n",
      "Model acting\n",
      "\n",
      "episode: 58, reward: -49.3\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: 10.0, Total Reward: 10.0, Loss: 5.735692501068115\n",
      "Optimizing model...\n",
      "Step: 1, Action: 3, Reward: 0.25, Total Reward: 10.25, Loss: 5.566337585449219\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: 10.0, Total Reward: 20.25, Loss: 5.683518409729004\n",
      "Optimizing model...\n",
      "Step: 3, Action: 1, Reward: -5.0, Total Reward: 15.25, Loss: 4.114898681640625\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: 10.0, Total Reward: 25.25, Loss: 3.6048450469970703\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 5, Action: 0, Reward: 10.0, Total Reward: 35.25, Loss: 3.500108003616333\n",
      "Optimizing model...\n",
      "Step: 6, Action: 0, Reward: 10.0, Total Reward: 45.25, Loss: 6.436014175415039\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 7, Action: 0, Reward: 0.0, Total Reward: 45.25, Loss: 3.8291120529174805\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 8, Action: 0, Reward: 0.0, Total Reward: 45.25, Loss: 5.61496639251709\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 9, Action: 0, Reward: 10.0, Total Reward: 55.25, Loss: 6.919410705566406\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 10, Action: 0, Reward: 10.0, Total Reward: 65.25, Loss: 3.803877830505371\n",
      "\n",
      "episode: 59, reward: -34.75\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: 10.0, Total Reward: 10.0, Loss: 4.05430269241333\n",
      "Optimizing model...\n",
      "Step: 1, Action: 2, Reward: 0.25, Total Reward: 10.25, Loss: 3.544198989868164\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: 10.0, Total Reward: 20.25, Loss: 6.98680305480957\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: 0.0, Total Reward: 20.25, Loss: 3.6224145889282227\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: 0.0, Total Reward: 20.25, Loss: 5.188961982727051\n",
      "Optimizing model...\n",
      "Step: 5, Action: 3, Reward: 0.25, Total Reward: 20.5, Loss: 5.258510112762451\n",
      "Optimizing model...\n",
      "Step: 6, Action: 0, Reward: 10.0, Total Reward: 30.5, Loss: 3.386892795562744\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 7, Action: 0, Reward: 10.0, Total Reward: 40.5, Loss: 6.783746719360352\n",
      "Optimizing model...\n",
      "Step: 8, Action: 1, Reward: -5.0, Total Reward: 35.5, Loss: 6.203375339508057\n",
      "Optimizing model...\n",
      "Step: 9, Action: 3, Reward: 0.25, Total Reward: 35.75, Loss: 3.4803574085235596\n",
      "Optimizing model...\n",
      "Step: 10, Action: 3, Reward: 0.25, Total Reward: 36.0, Loss: 5.17616081237793\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 11, Action: 0, Reward: 10.0, Total Reward: 46.0, Loss: 3.5160393714904785\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 12, Action: 0, Reward: 10.0, Total Reward: 56.0, Loss: 3.482821226119995\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 13, Action: 0, Reward: 0.0, Total Reward: 56.0, Loss: 4.558847904205322\n",
      "Model acting\n",
      "\n",
      "episode: 60, reward: -44.0\n",
      "Model saved\n",
      "Optimizing model...\n",
      "Step: 0, Action: 1, Reward: -5.0, Total Reward: -5.0, Loss: 2.9750852584838867\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: 10.0, Total Reward: 5.0, Loss: 2.5937650203704834\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: 10.0, Total Reward: 15.0, Loss: 3.6006877422332764\n",
      "Optimizing model...\n",
      "Step: 3, Action: 3, Reward: 0.25, Total Reward: 15.25, Loss: 5.243961334228516\n",
      "Optimizing model...\n",
      "Step: 4, Action: 4, Reward: 0.2, Total Reward: 15.45, Loss: 4.260613441467285\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 5, Action: 0, Reward: 10.0, Total Reward: 25.45, Loss: 2.493818759918213\n",
      "Optimizing model...\n",
      "Step: 6, Action: 1, Reward: -5.0, Total Reward: 20.45, Loss: 6.9993486404418945\n",
      "Model acting\n",
      "\n",
      "episode: 61, reward: -79.55\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: 10.0, Total Reward: 10.0, Loss: 7.9069108963012695\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: 10.0, Total Reward: 20.0, Loss: 3.789290428161621\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: 10.0, Total Reward: 30.0, Loss: 4.915470123291016\n",
      "Optimizing model...\n",
      "Step: 3, Action: 4, Reward: 0.2, Total Reward: 30.2, Loss: 3.81577205657959\n",
      "Optimizing model...\n",
      "Step: 4, Action: 1, Reward: -5.0, Total Reward: 25.2, Loss: 1.7985211610794067\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 5, Action: 0, Reward: 10.0, Total Reward: 35.2, Loss: 3.504805564880371\n",
      "\n",
      "episode: 62, reward: -64.8\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: 10.0, Total Reward: 10.0, Loss: 5.173338413238525\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: 10.0, Total Reward: 20.0, Loss: 3.143131732940674\n",
      "Optimizing model...\n",
      "Step: 2, Action: 3, Reward: 0.25, Total Reward: 20.25, Loss: 5.037679672241211\n",
      "Optimizing model...\n",
      "Step: 3, Action: 2, Reward: 0.25, Total Reward: 20.5, Loss: 6.189267635345459\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: 10.0, Total Reward: 30.5, Loss: 3.8401947021484375\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 5, Action: 0, Reward: 10.0, Total Reward: 40.5, Loss: 6.709404945373535\n",
      "Optimizing model...\n",
      "Step: 6, Action: 4, Reward: 0.2, Total Reward: 40.7, Loss: 3.964752197265625\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 7, Action: 0, Reward: 10.0, Total Reward: 50.7, Loss: 5.15640926361084\n",
      "Optimizing model...\n",
      "Step: 8, Action: 2, Reward: 0.25, Total Reward: 50.95, Loss: 4.4716715812683105\n",
      "Model acting\n",
      "\n",
      "episode: 63, reward: -49.05\n",
      "Optimizing model...\n",
      "Step: 0, Action: 3, Reward: 0.25, Total Reward: 0.25, Loss: 5.9802374839782715\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: 10.0, Total Reward: 10.25, Loss: 6.7513108253479\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: 10.0, Total Reward: 20.25, Loss: 5.2930731773376465\n",
      "Optimizing model...\n",
      "Step: 3, Action: 1, Reward: -5.0, Total Reward: 15.25, Loss: 5.022668838500977\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: 10.0, Total Reward: 25.25, Loss: 4.457167625427246\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 5, Action: 0, Reward: 0.0, Total Reward: 25.25, Loss: 8.262983322143555\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 6, Action: 0, Reward: 10.0, Total Reward: 35.25, Loss: 7.584288597106934\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 7, Action: 0, Reward: 0.0, Total Reward: 35.25, Loss: 3.3525655269622803\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 8, Action: 0, Reward: 10.0, Total Reward: 45.25, Loss: 4.943469047546387\n",
      "\n",
      "episode: 64, reward: -54.75\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: 10.0, Total Reward: 10.0, Loss: 1.728332281112671\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: 10.0, Total Reward: 20.0, Loss: 5.1098408699035645\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: 0.0, Total Reward: 20.0, Loss: 7.05667781829834\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: 10.0, Total Reward: 30.0, Loss: 6.646143436431885\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: 10.0, Total Reward: 40.0, Loss: 4.042995452880859\n",
      "\n",
      "episode: 65, reward: -60.0\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: 10.0, Total Reward: 10.0, Loss: 5.586231231689453\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: 10.0, Total Reward: 20.0, Loss: 5.303460121154785\n",
      "Optimizing model...\n",
      "Step: 2, Action: 3, Reward: 0.25, Total Reward: 20.25, Loss: 5.143754959106445\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: 10.0, Total Reward: 30.25, Loss: 4.155977249145508\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: 10.0, Total Reward: 40.25, Loss: 8.160846710205078\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 5, Action: 0, Reward: 0.0, Total Reward: 40.25, Loss: 3.673900604248047\n",
      "\n",
      "episode: 66, reward: -59.75\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: 10.0, Total Reward: 10.0, Loss: 2.738553047180176\n",
      "Optimizing model...\n",
      "Step: 1, Action: 3, Reward: 0.25, Total Reward: 10.25, Loss: 1.8061097860336304\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: 10.0, Total Reward: 20.25, Loss: 3.147318124771118\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: 10.0, Total Reward: 30.25, Loss: 3.3742692470550537\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: 0.0, Total Reward: 30.25, Loss: 1.2492082118988037\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 5, Action: 0, Reward: 0.0, Total Reward: 30.25, Loss: 2.311145067214966\n",
      "Optimizing model...\n",
      "Step: 6, Action: 2, Reward: 0.25, Total Reward: 30.5, Loss: 6.706506729125977\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 7, Action: 0, Reward: 10.0, Total Reward: 40.5, Loss: 2.252413749694824\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 8, Action: 0, Reward: 10.0, Total Reward: 50.5, Loss: 7.198127746582031\n",
      "Optimizing model...\n",
      "Step: 9, Action: 2, Reward: 0.25, Total Reward: 50.75, Loss: 6.567688941955566\n",
      "\n",
      "episode: 67, reward: -49.25\n",
      "Optimizing model...\n",
      "Step: 0, Action: 1, Reward: -5.0, Total Reward: -5.0, Loss: 3.356466770172119\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: 10.0, Total Reward: 5.0, Loss: 4.869354248046875\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: 10.0, Total Reward: 15.0, Loss: 6.933596611022949\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: 0.0, Total Reward: 15.0, Loss: 1.6214076280593872\n",
      "Optimizing model...\n",
      "Step: 4, Action: 3, Reward: 0.25, Total Reward: 15.25, Loss: 4.065369129180908\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 5, Action: 0, Reward: 10.0, Total Reward: 25.25, Loss: 7.115250587463379\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 6, Action: 0, Reward: 10.0, Total Reward: 35.25, Loss: 5.290093421936035\n",
      "Model acting\n",
      "\n",
      "episode: 68, reward: -64.75\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: 10.0, Total Reward: 10.0, Loss: 6.58253812789917\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: 10.0, Total Reward: 20.0, Loss: 1.6034674644470215\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: 10.0, Total Reward: 30.0, Loss: 7.2405242919921875\n",
      "Optimizing model...\n",
      "Step: 3, Action: 1, Reward: -5.0, Total Reward: 25.0, Loss: 1.4650025367736816\n",
      "\n",
      "episode: 69, reward: -75.0\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: 10.0, Total Reward: 10.0, Loss: 3.416243076324463\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: 10.0, Total Reward: 20.0, Loss: 8.680534362792969\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: 0.0, Total Reward: 20.0, Loss: 9.886415481567383\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: 0.0, Total Reward: 20.0, Loss: 6.944758415222168\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: 10.0, Total Reward: 30.0, Loss: 7.996523380279541\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 5, Action: 0, Reward: 0.0, Total Reward: 30.0, Loss: 6.514798164367676\n",
      "\n",
      "episode: 70, reward: -70.0\n",
      "Model saved\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: 10.0, Total Reward: 10.0, Loss: 5.908984661102295\n",
      "Optimizing model...\n",
      "Step: 1, Action: 2, Reward: 0.25, Total Reward: 10.25, Loss: 4.07568359375\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: 10.0, Total Reward: 20.25, Loss: 6.670697212219238\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: 0.0, Total Reward: 20.25, Loss: 3.9223766326904297\n",
      "Model acting\n",
      "\n",
      "episode: 71, reward: -79.75\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: 10.0, Total Reward: 10.0, Loss: 5.762534141540527\n",
      "Optimizing model...\n",
      "Step: 1, Action: 4, Reward: 0.2, Total Reward: 10.2, Loss: 3.7849111557006836\n",
      "Optimizing model...\n",
      "Step: 2, Action: 4, Reward: 0.2, Total Reward: 10.399999999999999, Loss: 4.652159690856934\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: 10.0, Total Reward: 20.4, Loss: 6.3240647315979\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: 10.0, Total Reward: 30.4, Loss: 7.149122714996338\n",
      "Optimizing model...\n",
      "Step: 5, Action: 1, Reward: -5.0, Total Reward: 25.4, Loss: 3.3010149002075195\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 6, Action: 0, Reward: 10.0, Total Reward: 35.4, Loss: 13.62535572052002\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 7, Action: 0, Reward: 10.0, Total Reward: 45.4, Loss: 8.328899383544922\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 8, Action: 0, Reward: 0.0, Total Reward: 45.4, Loss: 4.611789226531982\n",
      "Model acting\n",
      "\n",
      "episode: 72, reward: -54.6\n",
      "Optimizing model...\n",
      "Step: 0, Action: 2, Reward: 0.25, Total Reward: 0.25, Loss: 6.9460649490356445\n",
      "Optimizing model...\n",
      "Step: 1, Action: 2, Reward: 0.25, Total Reward: 0.5, Loss: 3.7684872150421143\n",
      "Optimizing model...\n",
      "Step: 2, Action: 3, Reward: 0.25, Total Reward: 0.75, Loss: 1.2702850103378296\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: 10.0, Total Reward: 10.75, Loss: 7.833797454833984\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: 10.0, Total Reward: 20.75, Loss: 7.2597551345825195\n",
      "Optimizing model...\n",
      "Step: 5, Action: 2, Reward: 0.25, Total Reward: 21.0, Loss: 3.1788253784179688\n",
      "Model acting\n",
      "\n",
      "episode: 73, reward: -79.0\n",
      "Optimizing model...\n",
      "Step: 0, Action: 4, Reward: 0.2, Total Reward: 0.2, Loss: 4.610404014587402\n",
      "Optimizing model...\n",
      "Step: 1, Action: 1, Reward: -5.0, Total Reward: -4.8, Loss: 8.006040573120117\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: 10.0, Total Reward: 5.2, Loss: 4.822131633758545\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: 10.0, Total Reward: 15.2, Loss: 4.934557914733887\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: 0.0, Total Reward: 15.2, Loss: 3.786351442337036\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 5, Action: 0, Reward: 10.0, Total Reward: 25.2, Loss: 6.009994029998779\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 6, Action: 0, Reward: 0.0, Total Reward: 25.2, Loss: 5.444263458251953\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 7, Action: 0, Reward: 10.0, Total Reward: 35.2, Loss: 5.377574920654297\n",
      "Optimizing model...\n",
      "Step: 8, Action: 1, Reward: -5.0, Total Reward: 30.200000000000003, Loss: 3.883138656616211\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 9, Action: 0, Reward: 10.0, Total Reward: 40.2, Loss: 5.542791366577148\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 10, Action: 0, Reward: 0.0, Total Reward: 40.2, Loss: 6.570355415344238\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 11, Action: 0, Reward: 10.0, Total Reward: 50.2, Loss: 5.083290100097656\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 12, Action: 0, Reward: 10.0, Total Reward: 60.2, Loss: 7.355771541595459\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 13, Action: 0, Reward: 10.0, Total Reward: 70.2, Loss: 6.250856399536133\n",
      "\n",
      "episode: 74, reward: -29.799999999999997\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: 10.0, Total Reward: 10.0, Loss: 2.908623695373535\n",
      "Optimizing model...\n",
      "Step: 1, Action: 1, Reward: -5.0, Total Reward: 5.0, Loss: 5.349241733551025\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: 10.0, Total Reward: 15.0, Loss: 5.2769012451171875\n",
      "Optimizing model...\n",
      "Step: 3, Action: 1, Reward: -5.0, Total Reward: 10.0, Loss: 3.431574583053589\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: 10.0, Total Reward: 20.0, Loss: 3.714869976043701\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 5, Action: 0, Reward: 10.0, Total Reward: 30.0, Loss: 5.461768627166748\n",
      "Optimizing model...\n",
      "Step: 6, Action: 1, Reward: -5.0, Total Reward: 25.0, Loss: 3.54811692237854\n",
      "Optimizing model...\n",
      "Step: 7, Action: 2, Reward: 0.25, Total Reward: 25.25, Loss: 4.62857723236084\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 8, Action: 0, Reward: 10.0, Total Reward: 35.25, Loss: 4.6654815673828125\n",
      "Optimizing model...\n",
      "Step: 9, Action: 4, Reward: 0.2, Total Reward: 35.45, Loss: 7.814541816711426\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 10, Action: 0, Reward: 10.0, Total Reward: 45.45, Loss: 3.822984218597412\n",
      "Optimizing model...\n",
      "Step: 11, Action: 1, Reward: -5.0, Total Reward: 40.45, Loss: 4.567584991455078\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 12, Action: 0, Reward: 10.0, Total Reward: 50.45, Loss: 7.792201995849609\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 13, Action: 0, Reward: 0.0, Total Reward: 50.45, Loss: 1.7137432098388672\n",
      "Optimizing model...\n",
      "Step: 14, Action: 4, Reward: 0.2, Total Reward: 50.650000000000006, Loss: 2.5053019523620605\n",
      "Optimizing model...\n",
      "Step: 15, Action: 3, Reward: 0.25, Total Reward: 50.900000000000006, Loss: 4.807000637054443\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 16, Action: 0, Reward: 10.0, Total Reward: 60.900000000000006, Loss: 2.2888500690460205\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 17, Action: 0, Reward: 0.0, Total Reward: 60.900000000000006, Loss: 3.8423080444335938\n",
      "Optimizing model...\n",
      "Step: 18, Action: 1, Reward: -5.0, Total Reward: 55.900000000000006, Loss: 5.069145202636719\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 19, Action: 0, Reward: 10.0, Total Reward: 65.9, Loss: 5.722720146179199\n",
      "Optimizing model...\n",
      "Step: 20, Action: 0, Reward: 0.0, Total Reward: 65.9, Loss: 6.912632942199707\n",
      "Model acting\n",
      "\n",
      "episode: 75, reward: -34.099999999999994\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: 10.0, Total Reward: 10.0, Loss: 7.651301383972168\n",
      "Optimizing model...\n",
      "Step: 1, Action: 2, Reward: 0.25, Total Reward: 10.25, Loss: 5.174663066864014\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: 10.0, Total Reward: 20.25, Loss: 6.529122829437256\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: 10.0, Total Reward: 30.25, Loss: 1.9905472993850708\n",
      "Optimizing model...\n",
      "Step: 4, Action: 2, Reward: 0.25, Total Reward: 30.5, Loss: 2.879275321960449\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 5, Action: 0, Reward: 10.0, Total Reward: 40.5, Loss: 3.659531354904175\n",
      "Optimizing model...\n",
      "Step: 6, Action: 0, Reward: 0.0, Total Reward: 40.5, Loss: 3.3965420722961426\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 7, Action: 0, Reward: 0.0, Total Reward: 40.5, Loss: 6.494294166564941\n",
      "Optimizing model...\n",
      "Step: 8, Action: 4, Reward: 0.2, Total Reward: 40.7, Loss: 6.92936897277832\n",
      "Optimizing model...\n",
      "Step: 9, Action: 3, Reward: 0.25, Total Reward: 40.95, Loss: 3.65372896194458\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 10, Action: 0, Reward: 10.0, Total Reward: 50.95, Loss: 2.7474489212036133\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 11, Action: 0, Reward: 0.0, Total Reward: 50.95, Loss: 1.886122226715088\n",
      "Optimizing model...\n",
      "Step: 12, Action: 2, Reward: 0.25, Total Reward: 51.2, Loss: 3.850520133972168\n",
      "Optimizing model...\n",
      "Step: 13, Action: 0, Reward: 10.0, Total Reward: 61.2, Loss: 3.8988289833068848\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 14, Action: 0, Reward: 10.0, Total Reward: 71.2, Loss: 9.717804908752441\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 15, Action: 0, Reward: 0.0, Total Reward: 71.2, Loss: 6.387051105499268\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 16, Action: 0, Reward: 10.0, Total Reward: 81.2, Loss: 10.54101276397705\n",
      "Optimizing model...\n",
      "Step: 17, Action: 4, Reward: 0.2, Total Reward: 81.4, Loss: 5.377533435821533\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 18, Action: 0, Reward: 10.0, Total Reward: 91.4, Loss: 1.6827185153961182\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 19, Action: 0, Reward: 0.0, Total Reward: 91.4, Loss: 4.652421951293945\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 20, Action: 0, Reward: 0.0, Total Reward: 91.4, Loss: 1.9388823509216309\n",
      "Optimizing model...\n",
      "Step: 21, Action: 3, Reward: 0.25, Total Reward: 91.65, Loss: 7.7861328125\n",
      "Optimizing model...\n",
      "Step: 22, Action: 0, Reward: 10.0, Total Reward: 101.65, Loss: 11.511548042297363\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 23, Action: 0, Reward: 0.0, Total Reward: 101.65, Loss: 4.650059700012207\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 24, Action: 0, Reward: 10.0, Total Reward: 111.65, Loss: 6.128227233886719\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 25, Action: 0, Reward: 0.0, Total Reward: 111.65, Loss: 2.3534927368164062\n",
      "Optimizing model...\n",
      "Step: 26, Action: 1, Reward: -5.0, Total Reward: 106.65, Loss: 2.5238897800445557\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 27, Action: 0, Reward: 10.0, Total Reward: 116.65, Loss: 8.030179977416992\n",
      "Optimizing model...\n",
      "Step: 28, Action: 1, Reward: -5.0, Total Reward: 111.65, Loss: 3.770975112915039\n",
      "\n",
      "episode: 76, reward: 11.650000000000006\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: 10.0, Total Reward: 10.0, Loss: 1.4323382377624512\n",
      "Optimizing model...\n",
      "Step: 1, Action: 2, Reward: 0.25, Total Reward: 10.25, Loss: 4.359865188598633\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: 10.0, Total Reward: 20.25, Loss: 8.202184677124023\n",
      "Optimizing model...\n",
      "Step: 3, Action: 1, Reward: -5.0, Total Reward: 15.25, Loss: 1.6683412790298462\n",
      "Optimizing model...\n",
      "Step: 4, Action: 3, Reward: 0.25, Total Reward: 15.5, Loss: 4.553197383880615\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 5, Action: 0, Reward: 10.0, Total Reward: 25.5, Loss: 3.28518009185791\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 6, Action: 0, Reward: 10.0, Total Reward: 35.5, Loss: 4.902726173400879\n",
      "Optimizing model...\n",
      "Step: 7, Action: 3, Reward: 0.25, Total Reward: 35.75, Loss: 6.4879865646362305\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 8, Action: 3, Reward: 0.25, Total Reward: 36.0, Loss: 8.559427261352539\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 9, Action: 3, Reward: 0.25, Total Reward: 36.25, Loss: 4.896293640136719\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 10, Action: 3, Reward: -9.75, Total Reward: 26.5, Loss: 8.010309219360352\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 11, Action: 0, Reward: 10.0, Total Reward: 36.5, Loss: 5.384512424468994\n",
      "Model acting\n",
      "\n",
      "episode: 77, reward: -63.5\n",
      "Optimizing model...\n",
      "Step: 0, Action: 3, Reward: 0.25, Total Reward: 0.25, Loss: 6.66167688369751\n",
      "Optimizing model...\n",
      "Step: 1, Action: 1, Reward: -5.0, Total Reward: -4.75, Loss: 6.311206817626953\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: 10.0, Total Reward: 5.25, Loss: 4.7606892585754395\n",
      "Optimizing model...\n",
      "Step: 3, Action: 2, Reward: 0.25, Total Reward: 5.5, Loss: 3.100094795227051\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: 10.0, Total Reward: 15.5, Loss: 8.163525581359863\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 5, Action: 0, Reward: 10.0, Total Reward: 25.5, Loss: 4.000044822692871\n",
      "Optimizing model...\n",
      "Step: 6, Action: 0, Reward: 10.0, Total Reward: 35.5, Loss: 6.749923229217529\n",
      "Optimizing model...\n",
      "Step: 7, Action: 1, Reward: -5.0, Total Reward: 30.5, Loss: 5.517265319824219\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 8, Action: 0, Reward: 10.0, Total Reward: 40.5, Loss: 5.124405384063721\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 9, Action: 0, Reward: 10.0, Total Reward: 50.5, Loss: 5.032836437225342\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 10, Action: 0, Reward: 10.0, Total Reward: 60.5, Loss: 5.124030113220215\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 11, Action: 0, Reward: 10.0, Total Reward: 70.5, Loss: 6.062479496002197\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 12, Action: 0, Reward: 10.0, Total Reward: 80.5, Loss: 4.814217567443848\n",
      "Model acting\n",
      "\n",
      "episode: 78, reward: -19.5\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: 10.0, Total Reward: 10.0, Loss: 6.693260192871094\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: 10.0, Total Reward: 20.0, Loss: 4.025707244873047\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: 0.0, Total Reward: 20.0, Loss: 3.9704372882843018\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: 10.0, Total Reward: 30.0, Loss: 2.794215202331543\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: 10.0, Total Reward: 40.0, Loss: 3.8474433422088623\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 5, Action: 0, Reward: 0.0, Total Reward: 40.0, Loss: 4.524245738983154\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 6, Action: 0, Reward: 0.0, Total Reward: 40.0, Loss: 8.849164009094238\n",
      "Optimizing model...\n",
      "Step: 7, Action: 4, Reward: 0.2, Total Reward: 40.2, Loss: 4.7437968254089355\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 8, Action: 0, Reward: 10.0, Total Reward: 50.2, Loss: 5.123149871826172\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 9, Action: 0, Reward: 10.0, Total Reward: 60.2, Loss: 4.322770118713379\n",
      "Optimizing model...\n",
      "Step: 10, Action: 0, Reward: 0.0, Total Reward: 60.2, Loss: 6.809807777404785\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 11, Action: 0, Reward: 0.0, Total Reward: 60.2, Loss: 4.6183271408081055\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 12, Action: 0, Reward: 0.0, Total Reward: 60.2, Loss: 9.656238555908203\n",
      "Optimizing model...\n",
      "Step: 13, Action: 2, Reward: 0.25, Total Reward: 60.45, Loss: 8.660869598388672\n",
      "Optimizing model...\n",
      "Step: 14, Action: 3, Reward: 0.25, Total Reward: 60.7, Loss: 1.8223788738250732\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 15, Action: 0, Reward: 10.0, Total Reward: 70.7, Loss: 7.4253129959106445\n",
      "Optimizing model...\n",
      "Step: 16, Action: 0, Reward: 10.0, Total Reward: 80.7, Loss: 4.815748691558838\n",
      "\n",
      "episode: 79, reward: -19.299999999999997\n",
      "Optimizing model...\n",
      "Step: 0, Action: 3, Reward: 0.25, Total Reward: 0.25, Loss: 5.529767036437988\n",
      "Optimizing model...\n",
      "Step: 1, Action: 2, Reward: 0.25, Total Reward: 0.5, Loss: 6.257488250732422\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: 10.0, Total Reward: 10.5, Loss: 5.28474760055542\n",
      "Optimizing model...\n",
      "Step: 3, Action: 3, Reward: 0.25, Total Reward: 10.75, Loss: 3.073482036590576\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: 10.0, Total Reward: 20.75, Loss: 9.711788177490234\n",
      "Optimizing model...\n",
      "Step: 5, Action: 3, Reward: 0.25, Total Reward: 21.0, Loss: 3.8594045639038086\n",
      "Optimizing model...\n",
      "Step: 6, Action: 0, Reward: 10.0, Total Reward: 31.0, Loss: 7.425774574279785\n",
      "Optimizing model...\n",
      "Step: 7, Action: 2, Reward: 0.25, Total Reward: 31.25, Loss: 10.966632843017578\n",
      "\n",
      "episode: 80, reward: -68.75\n",
      "Model saved\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: 10.0, Total Reward: 10.0, Loss: 3.99790620803833\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: 10.0, Total Reward: 20.0, Loss: 2.402557373046875\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: 0.0, Total Reward: 20.0, Loss: 12.43301773071289\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: 0.0, Total Reward: 20.0, Loss: 6.9832258224487305\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: 10.0, Total Reward: 30.0, Loss: 5.081048965454102\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 5, Action: 0, Reward: 0.0, Total Reward: 30.0, Loss: 6.502742290496826\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 6, Action: 0, Reward: 0.0, Total Reward: 30.0, Loss: 5.179350852966309\n",
      "Optimizing model...\n",
      "Step: 7, Action: 1, Reward: -5.0, Total Reward: 25.0, Loss: 4.020564556121826\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 8, Action: 0, Reward: 10.0, Total Reward: 35.0, Loss: 6.339277267456055\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 9, Action: 0, Reward: 10.0, Total Reward: 45.0, Loss: 8.466455459594727\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 10, Action: 0, Reward: 10.0, Total Reward: 55.0, Loss: 7.963980674743652\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 11, Action: 0, Reward: 0.0, Total Reward: 55.0, Loss: 5.034448146820068\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 12, Action: 0, Reward: 0.0, Total Reward: 55.0, Loss: 3.227156639099121\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 13, Action: 0, Reward: 10.0, Total Reward: 65.0, Loss: 2.579263210296631\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 14, Action: 0, Reward: 10.0, Total Reward: 75.0, Loss: 8.127795219421387\n",
      "Model acting\n",
      "\n",
      "episode: 81, reward: -25.0\n",
      "Optimizing model...\n",
      "Step: 0, Action: 3, Reward: 0.25, Total Reward: 0.25, Loss: 4.270844459533691\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: 10.0, Total Reward: 10.25, Loss: 3.3291306495666504\n",
      "Optimizing model...\n",
      "Step: 2, Action: 3, Reward: 0.25, Total Reward: 10.5, Loss: 4.658531188964844\n",
      "Optimizing model...\n",
      "Step: 3, Action: 3, Reward: 0.25, Total Reward: 10.75, Loss: 6.51552677154541\n",
      "Optimizing model...\n",
      "Step: 4, Action: 1, Reward: -5.0, Total Reward: 5.75, Loss: 6.27457332611084\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 5, Action: 0, Reward: 10.0, Total Reward: 15.75, Loss: 4.865691184997559\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 6, Action: 0, Reward: 10.0, Total Reward: 25.75, Loss: 4.954237937927246\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 7, Action: 0, Reward: 10.0, Total Reward: 35.75, Loss: 6.516160011291504\n",
      "Optimizing model...\n",
      "Step: 8, Action: 4, Reward: 0.2, Total Reward: 35.95, Loss: 3.4121265411376953\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 9, Action: 0, Reward: 10.0, Total Reward: 45.95, Loss: 4.052697658538818\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 10, Action: 0, Reward: 10.0, Total Reward: 55.95, Loss: 3.2761759757995605\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 11, Action: 0, Reward: 0.0, Total Reward: 55.95, Loss: 3.344987630844116\n",
      "Optimizing model...\n",
      "Step: 12, Action: 0, Reward: 0.0, Total Reward: 55.95, Loss: 3.9127421379089355\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 13, Action: 0, Reward: 10.0, Total Reward: 65.95, Loss: 8.4847412109375\n",
      "Optimizing model...\n",
      "Step: 14, Action: 1, Reward: -5.0, Total Reward: 60.95, Loss: 5.659505844116211\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 15, Action: 0, Reward: 10.0, Total Reward: 70.95, Loss: 8.175089836120605\n",
      "Optimizing model...\n",
      "Step: 16, Action: 1, Reward: -5.0, Total Reward: 65.95, Loss: 6.18847131729126\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 17, Action: 0, Reward: 10.0, Total Reward: 75.95, Loss: 1.9221550226211548\n",
      "Model acting\n",
      "\n",
      "episode: 82, reward: -24.049999999999997\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: 10.0, Total Reward: 10.0, Loss: 6.848050117492676\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: 10.0, Total Reward: 20.0, Loss: 3.4676032066345215\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: 0.0, Total Reward: 20.0, Loss: 5.348230361938477\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: 10.0, Total Reward: 30.0, Loss: 5.474829196929932\n",
      "\n",
      "episode: 83, reward: -70.0\n",
      "Optimizing model...\n",
      "Step: 0, Action: 3, Reward: 0.25, Total Reward: 0.25, Loss: 4.546926498413086\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: 10.0, Total Reward: 10.25, Loss: 3.8352315425872803\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: 10.0, Total Reward: 20.25, Loss: 4.237344264984131\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: 0.0, Total Reward: 20.25, Loss: 6.664147853851318\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: 0.0, Total Reward: 20.25, Loss: 5.713624954223633\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 5, Action: 0, Reward: 0.0, Total Reward: 20.25, Loss: 9.063116073608398\n",
      "Optimizing model...\n",
      "Step: 6, Action: 0, Reward: 10.0, Total Reward: 30.25, Loss: 6.352235794067383\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 7, Action: 0, Reward: 10.0, Total Reward: 40.25, Loss: 6.44284200668335\n",
      "\n",
      "episode: 84, reward: -59.75\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: 10.0, Total Reward: 10.0, Loss: 4.139389991760254\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: 10.0, Total Reward: 20.0, Loss: 3.913719654083252\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: 0.0, Total Reward: 20.0, Loss: 4.8786115646362305\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: 10.0, Total Reward: 30.0, Loss: 4.735795497894287\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: 10.0, Total Reward: 40.0, Loss: 6.49328088760376\n",
      "\n",
      "episode: 85, reward: -60.0\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: 10.0, Total Reward: 10.0, Loss: 5.361252307891846\n",
      "Optimizing model...\n",
      "Step: 1, Action: 4, Reward: 0.2, Total Reward: 10.2, Loss: 3.531243324279785\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: 10.0, Total Reward: 20.2, Loss: 5.749176979064941\n",
      "Optimizing model...\n",
      "Step: 3, Action: 3, Reward: 0.25, Total Reward: 20.45, Loss: 7.097316265106201\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: 10.0, Total Reward: 30.45, Loss: 8.125044822692871\n",
      "Optimizing model...\n",
      "Step: 5, Action: 1, Reward: -5.0, Total Reward: 25.45, Loss: 6.991697788238525\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 6, Action: 0, Reward: 10.0, Total Reward: 35.45, Loss: 3.812826633453369\n",
      "Optimizing model...\n",
      "Step: 7, Action: 3, Reward: 0.25, Total Reward: 35.7, Loss: 5.241209983825684\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 8, Action: 0, Reward: 10.0, Total Reward: 45.7, Loss: 3.210401773452759\n",
      "Optimizing model...\n",
      "Step: 9, Action: 1, Reward: -5.0, Total Reward: 40.7, Loss: 5.232809066772461\n",
      "Optimizing model...\n",
      "Step: 10, Action: 0, Reward: 10.0, Total Reward: 50.7, Loss: 8.724472045898438\n",
      "Optimizing model...\n",
      "Step: 11, Action: 2, Reward: 0.25, Total Reward: 50.95, Loss: 3.6900033950805664\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 12, Action: 0, Reward: 10.0, Total Reward: 60.95, Loss: 9.034271240234375\n",
      "Optimizing model...\n",
      "Step: 13, Action: 1, Reward: -5.0, Total Reward: 55.95, Loss: 3.6129491329193115\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 14, Action: 0, Reward: 10.0, Total Reward: 65.95, Loss: 2.5536928176879883\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 15, Action: 0, Reward: 0.0, Total Reward: 65.95, Loss: 7.1398115158081055\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 16, Action: 0, Reward: 10.0, Total Reward: 75.95, Loss: 1.8801045417785645\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 17, Action: 0, Reward: 0.0, Total Reward: 75.95, Loss: 2.70381498336792\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 18, Action: 0, Reward: 10.0, Total Reward: 85.95, Loss: 3.0435540676116943\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 19, Action: 0, Reward: 0.0, Total Reward: 85.95, Loss: 5.245731353759766\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 20, Action: 0, Reward: 10.0, Total Reward: 95.95, Loss: 3.4250946044921875\n",
      "Optimizing model...\n",
      "Step: 21, Action: 4, Reward: 0.2, Total Reward: 96.15, Loss: 5.297251224517822\n",
      "Model acting\n",
      "\n",
      "episode: 86, reward: -3.8499999999999943\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: 10.0, Total Reward: 10.0, Loss: 8.188634872436523\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: 10.0, Total Reward: 20.0, Loss: 2.3246121406555176\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: 0.0, Total Reward: 20.0, Loss: 3.5781331062316895\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: 0.0, Total Reward: 20.0, Loss: 3.572930335998535\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: 10.0, Total Reward: 30.0, Loss: 3.801081657409668\n",
      "Optimizing model...\n",
      "Step: 5, Action: 2, Reward: 0.25, Total Reward: 30.25, Loss: 4.2468671798706055\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 6, Action: 0, Reward: 10.0, Total Reward: 40.25, Loss: 5.451662063598633\n",
      "Optimizing model...\n",
      "Step: 7, Action: 0, Reward: 0.0, Total Reward: 40.25, Loss: 3.6439342498779297\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 8, Action: 0, Reward: 10.0, Total Reward: 50.25, Loss: 5.2863874435424805\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 9, Action: 0, Reward: 0.0, Total Reward: 50.25, Loss: 6.188070774078369\n",
      "Optimizing model...\n",
      "Step: 10, Action: 3, Reward: 0.25, Total Reward: 50.5, Loss: 9.418087005615234\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 11, Action: 0, Reward: 10.0, Total Reward: 60.5, Loss: 6.899011135101318\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 12, Action: 0, Reward: 0.0, Total Reward: 60.5, Loss: 6.605371475219727\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 13, Action: 0, Reward: 10.0, Total Reward: 70.5, Loss: 5.339328289031982\n",
      "Optimizing model...\n",
      "Step: 14, Action: 0, Reward: 0.0, Total Reward: 70.5, Loss: 7.173391819000244\n",
      "Optimizing model...\n",
      "Step: 15, Action: 1, Reward: -5.0, Total Reward: 65.5, Loss: 4.775424957275391\n",
      "Optimizing model...\n",
      "Step: 16, Action: 3, Reward: 0.25, Total Reward: 65.75, Loss: 6.395395755767822\n",
      "Optimizing model...\n",
      "Step: 17, Action: 1, Reward: -5.0, Total Reward: 60.75, Loss: 7.571225166320801\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 18, Action: 0, Reward: 10.0, Total Reward: 70.75, Loss: 6.456216335296631\n",
      "Optimizing model...\n",
      "Step: 19, Action: 2, Reward: 0.25, Total Reward: 71.0, Loss: 5.047305107116699\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 20, Action: 0, Reward: 10.0, Total Reward: 81.0, Loss: 4.121169090270996\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 21, Action: 0, Reward: 0.0, Total Reward: 81.0, Loss: 4.6453537940979\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 22, Action: 0, Reward: 0.0, Total Reward: 81.0, Loss: 6.719329357147217\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 23, Action: 0, Reward: 0.0, Total Reward: 81.0, Loss: 5.555794715881348\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 24, Action: 0, Reward: 0.0, Total Reward: 81.0, Loss: 3.406560182571411\n",
      "Optimizing model...\n",
      "Step: 25, Action: 4, Reward: 0.2, Total Reward: 81.2, Loss: 4.4296393394470215\n",
      "Optimizing model...\n",
      "Step: 26, Action: 3, Reward: 0.25, Total Reward: 81.45, Loss: 5.115035533905029\n",
      "Optimizing model...\n",
      "Step: 27, Action: 4, Reward: 0.2, Total Reward: 81.65, Loss: 4.759416580200195\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 28, Action: 0, Reward: 10.0, Total Reward: 91.65, Loss: 7.847973823547363\n",
      "Optimizing model...\n",
      "Step: 29, Action: 4, Reward: 0.2, Total Reward: 91.85000000000001, Loss: 6.819103717803955\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 30, Action: 4, Reward: -9.8, Total Reward: 82.05000000000001, Loss: 4.168632507324219\n",
      "Optimizing model...\n",
      "Step: 31, Action: 0, Reward: 10.0, Total Reward: 92.05000000000001, Loss: 7.331892013549805\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 32, Action: 2, Reward: 0.25, Total Reward: 92.30000000000001, Loss: 4.4526872634887695\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 33, Action: 2, Reward: -9.75, Total Reward: 82.55000000000001, Loss: 4.281786918640137\n",
      "Optimizing model...\n",
      "Step: 34, Action: 2, Reward: -9.75, Total Reward: 72.80000000000001, Loss: 6.081470489501953\n",
      "Model acting\n",
      "\n",
      "episode: 87, reward: -27.19999999999999\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: 10.0, Total Reward: 10.0, Loss: 2.824805736541748\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 1, Action: 2, Reward: 0.25, Total Reward: 10.25, Loss: 6.357809543609619\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: 10.0, Total Reward: 20.25, Loss: 3.8969545364379883\n",
      "Optimizing model...\n",
      "Step: 3, Action: 3, Reward: 0.25, Total Reward: 20.5, Loss: 5.2065582275390625\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: 10.0, Total Reward: 30.5, Loss: 6.562552452087402\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 5, Action: 0, Reward: 10.0, Total Reward: 40.5, Loss: 3.1287777423858643\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 6, Action: 0, Reward: 10.0, Total Reward: 50.5, Loss: 5.581892013549805\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 7, Action: 0, Reward: 10.0, Total Reward: 60.5, Loss: 6.162016868591309\n",
      "Model acting\n",
      "\n",
      "episode: 88, reward: -39.5\n",
      "Optimizing model...\n",
      "Step: 0, Action: 4, Reward: 0.2, Total Reward: 0.2, Loss: 6.520830154418945\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: 10.0, Total Reward: 10.2, Loss: 7.412309646606445\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: 0.0, Total Reward: 10.2, Loss: 3.26303768157959\n",
      "Optimizing model...\n",
      "Step: 3, Action: 1, Reward: -5.0, Total Reward: 5.199999999999999, Loss: 5.119662761688232\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: 10.0, Total Reward: 15.2, Loss: 4.838415145874023\n",
      "Optimizing model...\n",
      "Step: 5, Action: 2, Reward: 0.25, Total Reward: 15.45, Loss: 7.459856986999512\n",
      "Optimizing model...\n",
      "Step: 6, Action: 4, Reward: 0.2, Total Reward: 15.649999999999999, Loss: 5.17022705078125\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 7, Action: 0, Reward: 10.0, Total Reward: 25.65, Loss: 6.645458221435547\n",
      "Optimizing model...\n",
      "Step: 8, Action: 1, Reward: -5.0, Total Reward: 20.65, Loss: 7.140923976898193\n",
      "Optimizing model...\n",
      "Step: 9, Action: 2, Reward: 0.25, Total Reward: 20.9, Loss: 8.03965950012207\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 10, Action: 0, Reward: 10.0, Total Reward: 30.9, Loss: 10.471385955810547\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 11, Action: 0, Reward: 10.0, Total Reward: 40.9, Loss: 4.671750545501709\n",
      "Optimizing model...\n",
      "Step: 12, Action: 1, Reward: -5.0, Total Reward: 35.9, Loss: 10.237600326538086\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 13, Action: 2, Reward: 0.25, Total Reward: 36.15, Loss: 2.162760019302368\n",
      "Optimizing model...\n",
      "Step: 14, Action: 4, Reward: 0.2, Total Reward: 36.35, Loss: 4.986335754394531\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 15, Action: 0, Reward: 10.0, Total Reward: 46.35, Loss: 3.9182353019714355\n",
      "Optimizing model...\n",
      "Step: 16, Action: 1, Reward: -5.0, Total Reward: 41.35, Loss: 3.8658199310302734\n",
      "Optimizing model...\n",
      "Step: 17, Action: 3, Reward: 0.25, Total Reward: 41.6, Loss: 5.744577407836914\n",
      "Optimizing model...\n",
      "Step: 18, Action: 0, Reward: 10.0, Total Reward: 51.6, Loss: 6.149911880493164\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 19, Action: 0, Reward: 10.0, Total Reward: 61.6, Loss: 3.8367056846618652\n",
      "Optimizing model...\n",
      "Step: 20, Action: 4, Reward: 0.2, Total Reward: 61.800000000000004, Loss: 6.698905944824219\n",
      "Optimizing model...\n",
      "Step: 21, Action: 3, Reward: 0.25, Total Reward: 62.050000000000004, Loss: 4.882547855377197\n",
      "Optimizing model...\n",
      "Step: 22, Action: 2, Reward: 0.25, Total Reward: 62.300000000000004, Loss: 4.528655052185059\n",
      "Optimizing model...\n",
      "Step: 23, Action: 1, Reward: -5.0, Total Reward: 57.300000000000004, Loss: 7.138594150543213\n",
      "Optimizing model...\n",
      "Step: 24, Action: 4, Reward: 0.2, Total Reward: 57.50000000000001, Loss: 2.6788978576660156\n",
      "Optimizing model...\n",
      "Step: 25, Action: 2, Reward: 0.25, Total Reward: 57.75000000000001, Loss: 4.964862823486328\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 26, Action: 0, Reward: 10.0, Total Reward: 67.75, Loss: 4.2237348556518555\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 27, Action: 0, Reward: 0.0, Total Reward: 67.75, Loss: 5.486587047576904\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 28, Action: 0, Reward: 10.0, Total Reward: 77.75, Loss: 3.794224262237549\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 29, Action: 0, Reward: 0.0, Total Reward: 77.75, Loss: 8.100863456726074\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 30, Action: 0, Reward: 0.0, Total Reward: 77.75, Loss: 2.677921772003174\n",
      "Model acting\n",
      "\n",
      "episode: 89, reward: -22.25\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: 10.0, Total Reward: 10.0, Loss: 6.81285285949707\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: 10.0, Total Reward: 20.0, Loss: 8.248429298400879\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: 10.0, Total Reward: 30.0, Loss: 5.050234794616699\n",
      "\n",
      "episode: 90, reward: -70.0\n",
      "Model saved\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: 10.0, Total Reward: 10.0, Loss: 3.1087777614593506\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: 10.0, Total Reward: 20.0, Loss: 5.621805191040039\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: 0.0, Total Reward: 20.0, Loss: 10.203853607177734\n",
      "Optimizing model...\n",
      "Step: 3, Action: 1, Reward: -5.0, Total Reward: 15.0, Loss: 6.606185436248779\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: 10.0, Total Reward: 25.0, Loss: 6.419071197509766\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 5, Action: 0, Reward: 10.0, Total Reward: 35.0, Loss: 4.301130771636963\n",
      "Model acting\n",
      "\n",
      "episode: 91, reward: -65.0\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: 10.0, Total Reward: 10.0, Loss: 8.334376335144043\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: 10.0, Total Reward: 20.0, Loss: 2.8207180500030518\n",
      "Optimizing model...\n",
      "Step: 2, Action: 1, Reward: -5.0, Total Reward: 15.0, Loss: 2.0400280952453613\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: 10.0, Total Reward: 25.0, Loss: 11.839509963989258\n",
      "Optimizing model...\n",
      "Step: 4, Action: 3, Reward: 0.25, Total Reward: 25.25, Loss: 3.1526384353637695\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 5, Action: 0, Reward: 10.0, Total Reward: 35.25, Loss: 3.348653793334961\n",
      "Optimizing model...\n",
      "Step: 6, Action: 4, Reward: 0.2, Total Reward: 35.45, Loss: 3.118378162384033\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 7, Action: 0, Reward: 10.0, Total Reward: 45.45, Loss: 2.5609655380249023\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 8, Action: 0, Reward: 10.0, Total Reward: 55.45, Loss: 6.386354446411133\n",
      "Optimizing model...\n",
      "Step: 9, Action: 2, Reward: 0.25, Total Reward: 55.7, Loss: 3.844294548034668\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 10, Action: 0, Reward: 10.0, Total Reward: 65.7, Loss: 9.894343376159668\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 11, Action: 0, Reward: 10.0, Total Reward: 75.7, Loss: 9.040550231933594\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 12, Action: 0, Reward: 0.0, Total Reward: 75.7, Loss: 3.9578075408935547\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 13, Action: 0, Reward: 10.0, Total Reward: 85.7, Loss: 4.146368026733398\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 14, Action: 0, Reward: 0.0, Total Reward: 85.7, Loss: 7.902448654174805\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 15, Action: 0, Reward: 0.0, Total Reward: 85.7, Loss: 6.605329513549805\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 16, Action: 0, Reward: 10.0, Total Reward: 95.7, Loss: 5.005350112915039\n",
      "Optimizing model...\n",
      "Step: 17, Action: 3, Reward: 0.25, Total Reward: 95.95, Loss: 5.488155364990234\n",
      "Optimizing model...\n",
      "Step: 18, Action: 2, Reward: 0.25, Total Reward: 96.2, Loss: 5.060090065002441\n",
      "Optimizing model...\n",
      "Step: 19, Action: 1, Reward: -5.0, Total Reward: 91.2, Loss: 6.983645915985107\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 20, Action: 0, Reward: 10.0, Total Reward: 101.2, Loss: 5.590404033660889\n",
      "Optimizing model...\n",
      "Step: 21, Action: 2, Reward: 0.25, Total Reward: 101.45, Loss: 5.5244460105896\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 22, Action: 0, Reward: 10.0, Total Reward: 111.45, Loss: 3.7734193801879883\n",
      "Optimizing model...\n",
      "Step: 23, Action: 1, Reward: -5.0, Total Reward: 106.45, Loss: 5.498079299926758\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 24, Action: 0, Reward: 10.0, Total Reward: 116.45, Loss: 3.445650339126587\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 25, Action: 0, Reward: 0.0, Total Reward: 116.45, Loss: 5.132828235626221\n",
      "Model acting\n",
      "\n",
      "episode: 92, reward: 16.450000000000003\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: 10.0, Total Reward: 10.0, Loss: 6.464601516723633\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: 10.0, Total Reward: 20.0, Loss: 4.792047500610352\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: 10.0, Total Reward: 30.0, Loss: 6.475410461425781\n",
      "Optimizing model...\n",
      "Step: 3, Action: 2, Reward: 0.25, Total Reward: 30.25, Loss: 3.101381778717041\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: 10.0, Total Reward: 40.25, Loss: 2.2412614822387695\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 5, Action: 0, Reward: 0.0, Total Reward: 40.25, Loss: 5.576563835144043\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 6, Action: 0, Reward: 0.0, Total Reward: 40.25, Loss: 6.726280212402344\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 7, Action: 0, Reward: 10.0, Total Reward: 50.25, Loss: 3.521916389465332\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 8, Action: 0, Reward: 0.0, Total Reward: 50.25, Loss: 3.5813310146331787\n",
      "Optimizing model...\n",
      "Step: 9, Action: 2, Reward: 0.25, Total Reward: 50.5, Loss: 2.80159330368042\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 10, Action: 0, Reward: 10.0, Total Reward: 60.5, Loss: 3.505751609802246\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 11, Action: 0, Reward: 0.0, Total Reward: 60.5, Loss: 9.555717468261719\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 12, Action: 0, Reward: 10.0, Total Reward: 70.5, Loss: 6.210904121398926\n",
      "Optimizing model...\n",
      "Step: 13, Action: 2, Reward: 0.25, Total Reward: 70.75, Loss: 6.716352462768555\n",
      "Model acting\n",
      "\n",
      "episode: 93, reward: -29.25\n",
      "Optimizing model...\n",
      "Step: 0, Action: 2, Reward: 0.25, Total Reward: 0.25, Loss: 2.566282272338867\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: 10.0, Total Reward: 10.25, Loss: 5.365635395050049\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: 10.0, Total Reward: 20.25, Loss: 3.5333242416381836\n",
      "Optimizing model...\n",
      "Step: 3, Action: 4, Reward: 0.2, Total Reward: 20.45, Loss: 11.073238372802734\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: 10.0, Total Reward: 30.45, Loss: 3.059563159942627\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 5, Action: 0, Reward: 10.0, Total Reward: 40.45, Loss: 8.540912628173828\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 6, Action: 0, Reward: 10.0, Total Reward: 50.45, Loss: 6.899269104003906\n",
      "\n",
      "episode: 94, reward: -49.55\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: 10.0, Total Reward: 10.0, Loss: 2.5601460933685303\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: 10.0, Total Reward: 20.0, Loss: 7.889019012451172\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: 0.0, Total Reward: 20.0, Loss: 2.2054224014282227\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: 10.0, Total Reward: 30.0, Loss: 3.9071707725524902\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: 0.0, Total Reward: 30.0, Loss: 9.438785552978516\n",
      "Optimizing model...\n",
      "Step: 5, Action: 3, Reward: 0.25, Total Reward: 30.25, Loss: 8.968744277954102\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 6, Action: 0, Reward: 10.0, Total Reward: 40.25, Loss: 6.128236770629883\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 7, Action: 0, Reward: 0.0, Total Reward: 40.25, Loss: 7.763584613800049\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 8, Action: 0, Reward: 10.0, Total Reward: 50.25, Loss: 5.609382152557373\n",
      "Optimizing model...\n",
      "Step: 9, Action: 2, Reward: 0.25, Total Reward: 50.5, Loss: 2.2963857650756836\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 10, Action: 0, Reward: 10.0, Total Reward: 60.5, Loss: 3.8320231437683105\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 11, Action: 0, Reward: 10.0, Total Reward: 70.5, Loss: 7.04736852645874\n",
      "\n",
      "episode: 95, reward: -29.5\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: 10.0, Total Reward: 10.0, Loss: 3.9305803775787354\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: 10.0, Total Reward: 20.0, Loss: 4.846065044403076\n",
      "Optimizing model...\n",
      "Step: 2, Action: 1, Reward: -5.0, Total Reward: 15.0, Loss: 6.953058242797852\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: 10.0, Total Reward: 25.0, Loss: 3.897721290588379\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: 0.0, Total Reward: 25.0, Loss: 5.236245155334473\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 5, Action: 0, Reward: 10.0, Total Reward: 35.0, Loss: 6.341940879821777\n",
      "Optimizing model...\n",
      "Step: 6, Action: 3, Reward: 0.25, Total Reward: 35.25, Loss: 7.1662373542785645\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 7, Action: 0, Reward: 10.0, Total Reward: 45.25, Loss: 5.560938835144043\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 8, Action: 0, Reward: 10.0, Total Reward: 55.25, Loss: 5.16860294342041\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 9, Action: 0, Reward: 10.0, Total Reward: 65.25, Loss: 4.152349472045898\n",
      "Model acting\n",
      "\n",
      "episode: 96, reward: -34.75\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: 10.0, Total Reward: 10.0, Loss: 2.170074224472046\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: 10.0, Total Reward: 20.0, Loss: 5.387029647827148\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: 10.0, Total Reward: 30.0, Loss: 3.6278417110443115\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: 10.0, Total Reward: 40.0, Loss: 6.354427337646484\n",
      "Optimizing model...\n",
      "Step: 4, Action: 4, Reward: 0.2, Total Reward: 40.2, Loss: 7.526264190673828\n",
      "Optimizing model...\n",
      "Step: 5, Action: 1, Reward: -5.0, Total Reward: 35.2, Loss: 5.762729644775391\n",
      "\n",
      "episode: 97, reward: -64.8\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: 10.0, Total Reward: 10.0, Loss: 4.656793594360352\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: 10.0, Total Reward: 20.0, Loss: 6.858626842498779\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: 10.0, Total Reward: 30.0, Loss: 5.215195178985596\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: 10.0, Total Reward: 40.0, Loss: 6.408806324005127\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: 10.0, Total Reward: 50.0, Loss: 4.201390266418457\n",
      "Optimizing model...\n",
      "Step: 5, Action: 3, Reward: 0.25, Total Reward: 50.25, Loss: 2.0153768062591553\n",
      "Model acting\n",
      "\n",
      "episode: 98, reward: -49.75\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: 10.0, Total Reward: 10.0, Loss: 5.06054162979126\n",
      "Optimizing model...\n",
      "Step: 1, Action: 4, Reward: 0.2, Total Reward: 10.2, Loss: 7.350275039672852\n",
      "Optimizing model...\n",
      "Step: 2, Action: 2, Reward: 0.25, Total Reward: 10.45, Loss: 6.43987512588501\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: 10.0, Total Reward: 20.45, Loss: 4.760706901550293\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: 10.0, Total Reward: 30.45, Loss: 5.9886932373046875\n",
      "Model acting\n",
      "\n",
      "episode: 99, reward: -69.55\n",
      "Optimizing model...\n",
      "Step: 0, Action: 3, Reward: 0.25, Total Reward: 0.25, Loss: 6.743910789489746\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: 10.0, Total Reward: 10.25, Loss: 5.225715637207031\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: 0.0, Total Reward: 10.25, Loss: 10.461743354797363\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: 10.0, Total Reward: 20.25, Loss: 4.307282447814941\n",
      "Model acting\n",
      "\n",
      "episode: 100, reward: -79.75\n",
      "Model saved\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: 10.0, Total Reward: 10.0, Loss: 5.936049938201904\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: 10.0, Total Reward: 20.0, Loss: 3.9842538833618164\n",
      "Optimizing model...\n",
      "Step: 2, Action: 2, Reward: 0.25, Total Reward: 20.25, Loss: 6.784897804260254\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: 10.0, Total Reward: 30.25, Loss: 8.172088623046875\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: 10.0, Total Reward: 40.25, Loss: 2.3836987018585205\n",
      "Model acting\n",
      "\n",
      "episode: 101, reward: -59.75\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: 10.0, Total Reward: 10.0, Loss: 2.6936731338500977\n",
      "Optimizing model...\n",
      "Step: 1, Action: 3, Reward: 0.25, Total Reward: 10.25, Loss: 4.003833293914795\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: 10.0, Total Reward: 20.25, Loss: 9.964451789855957\n",
      "Optimizing model...\n",
      "Step: 3, Action: 3, Reward: 0.25, Total Reward: 20.5, Loss: 8.133048057556152\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: 10.0, Total Reward: 30.5, Loss: 5.084590911865234\n",
      "Model acting\n",
      "\n",
      "episode: 102, reward: -69.5\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: 10.0, Total Reward: 10.0, Loss: 7.37954568862915\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: 10.0, Total Reward: 20.0, Loss: 3.389676809310913\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: 10.0, Total Reward: 30.0, Loss: 2.7246270179748535\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: 0.0, Total Reward: 30.0, Loss: 3.4636940956115723\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: 10.0, Total Reward: 40.0, Loss: 4.103718280792236\n",
      "Optimizing model...\n",
      "Step: 5, Action: 4, Reward: 0.2, Total Reward: 40.2, Loss: 9.886249542236328\n",
      "Optimizing model...\n",
      "Step: 6, Action: 1, Reward: -5.0, Total Reward: 35.2, Loss: 3.6774024963378906\n",
      "Optimizing model...\n",
      "Step: 7, Action: 1, Reward: -5.0, Total Reward: 30.200000000000003, Loss: 7.364568710327148\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 8, Action: 0, Reward: 10.0, Total Reward: 40.2, Loss: 4.391561508178711\n",
      "Model acting\n",
      "\n",
      "episode: 103, reward: -59.8\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: 10.0, Total Reward: 10.0, Loss: 3.4484004974365234\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: 10.0, Total Reward: 20.0, Loss: 5.270530700683594\n",
      "Optimizing model...\n",
      "Step: 2, Action: 3, Reward: 0.25, Total Reward: 20.25, Loss: 5.578342914581299\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: 10.0, Total Reward: 30.25, Loss: 4.994814872741699\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: 0.0, Total Reward: 30.25, Loss: 2.2296581268310547\n",
      "Optimizing model...\n",
      "Step: 5, Action: 1, Reward: -5.0, Total Reward: 25.25, Loss: 8.854086875915527\n",
      "Optimizing model...\n",
      "Step: 6, Action: 4, Reward: 0.2, Total Reward: 25.45, Loss: 5.1878790855407715\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 7, Action: 0, Reward: 10.0, Total Reward: 35.45, Loss: 1.827918291091919\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 8, Action: 0, Reward: 10.0, Total Reward: 45.45, Loss: 1.556187629699707\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 9, Action: 0, Reward: 10.0, Total Reward: 55.45, Loss: 1.9944484233856201\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 10, Action: 0, Reward: 10.0, Total Reward: 65.45, Loss: 6.778685092926025\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 11, Action: 0, Reward: 0.0, Total Reward: 65.45, Loss: 6.5402727127075195\n",
      "Optimizing model...\n",
      "Step: 12, Action: 4, Reward: 0.2, Total Reward: 65.65, Loss: 8.142513275146484\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 13, Action: 0, Reward: 10.0, Total Reward: 75.65, Loss: 6.936943054199219\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 14, Action: 0, Reward: 10.0, Total Reward: 85.65, Loss: 8.793557167053223\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 15, Action: 0, Reward: 10.0, Total Reward: 95.65, Loss: 2.581291675567627\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 16, Action: 0, Reward: 0.0, Total Reward: 95.65, Loss: 6.162973880767822\n",
      "Model acting\n",
      "\n",
      "episode: 104, reward: -4.349999999999994\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: 10.0, Total Reward: 10.0, Loss: 4.639958381652832\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: 10.0, Total Reward: 20.0, Loss: 4.369143009185791\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: 0.0, Total Reward: 20.0, Loss: 3.9660561084747314\n",
      "Optimizing model...\n",
      "Step: 3, Action: 2, Reward: 0.25, Total Reward: 20.25, Loss: 4.087467670440674\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: 10.0, Total Reward: 30.25, Loss: 4.653048515319824\n",
      "Optimizing model...\n",
      "Step: 5, Action: 3, Reward: 0.25, Total Reward: 30.5, Loss: 3.2409842014312744\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 6, Action: 0, Reward: 10.0, Total Reward: 40.5, Loss: 5.11811637878418\n",
      "Model acting\n",
      "\n",
      "episode: 105, reward: -59.5\n",
      "Optimizing model...\n",
      "Step: 0, Action: 1, Reward: -5.0, Total Reward: -5.0, Loss: 12.728556632995605\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: 10.0, Total Reward: 5.0, Loss: 5.709531307220459\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: 10.0, Total Reward: 15.0, Loss: 6.3157548904418945\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: 0.0, Total Reward: 15.0, Loss: 11.423637390136719\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: 10.0, Total Reward: 25.0, Loss: 7.561666488647461\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 5, Action: 0, Reward: 0.0, Total Reward: 25.0, Loss: 3.58549165725708\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 6, Action: 0, Reward: 10.0, Total Reward: 35.0, Loss: 8.10287094116211\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 7, Action: 0, Reward: 10.0, Total Reward: 45.0, Loss: 2.439258575439453\n",
      "Optimizing model...\n",
      "Step: 8, Action: 1, Reward: -5.0, Total Reward: 40.0, Loss: 4.741991996765137\n",
      "Model acting\n",
      "\n",
      "episode: 106, reward: -60.0\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: 10.0, Total Reward: 10.0, Loss: 5.741646766662598\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: 10.0, Total Reward: 20.0, Loss: 3.7253737449645996\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: 0.0, Total Reward: 20.0, Loss: 5.721863746643066\n",
      "Optimizing model...\n",
      "Step: 3, Action: 1, Reward: -5.0, Total Reward: 15.0, Loss: 5.35279655456543\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: 10.0, Total Reward: 25.0, Loss: 3.4552981853485107\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 5, Action: 0, Reward: 10.0, Total Reward: 35.0, Loss: 6.5162353515625\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 6, Action: 0, Reward: 10.0, Total Reward: 45.0, Loss: 5.464577674865723\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 7, Action: 0, Reward: 0.0, Total Reward: 45.0, Loss: 6.074300765991211\n",
      "Optimizing model...\n",
      "Step: 8, Action: 2, Reward: 0.25, Total Reward: 45.25, Loss: 7.858515739440918\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 9, Action: 0, Reward: 10.0, Total Reward: 55.25, Loss: 5.36266565322876\n",
      "Optimizing model...\n",
      "Step: 10, Action: 2, Reward: 0.25, Total Reward: 55.5, Loss: 1.8725323677062988\n",
      "\n",
      "episode: 107, reward: -44.5\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: 10.0, Total Reward: 10.0, Loss: 9.11463737487793\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: 10.0, Total Reward: 20.0, Loss: 14.156330108642578\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: 10.0, Total Reward: 30.0, Loss: 4.947193145751953\n",
      "Model acting\n",
      "\n",
      "episode: 108, reward: -70.0\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: 10.0, Total Reward: 10.0, Loss: 8.795806884765625\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: 0.0, Total Reward: 10.0, Loss: 10.22299861907959\n",
      "Optimizing model...\n",
      "Step: 2, Action: 4, Reward: 0.2, Total Reward: 10.2, Loss: 3.9043169021606445\n",
      "Model acting\n",
      "\n",
      "episode: 109, reward: -89.8\n",
      "Optimizing model...\n",
      "Step: 0, Action: 1, Reward: -5.0, Total Reward: -5.0, Loss: 6.344922065734863\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: 10.0, Total Reward: 5.0, Loss: 4.842197895050049\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: 10.0, Total Reward: 15.0, Loss: 4.987480163574219\n",
      "Optimizing model...\n",
      "Step: 3, Action: 3, Reward: 0.25, Total Reward: 15.25, Loss: 8.903572082519531\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: 10.0, Total Reward: 25.25, Loss: 7.913792610168457\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 5, Action: 0, Reward: 10.0, Total Reward: 35.25, Loss: 6.128395080566406\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 6, Action: 0, Reward: 10.0, Total Reward: 45.25, Loss: 3.3815064430236816\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 7, Action: 0, Reward: 10.0, Total Reward: 55.25, Loss: 6.4922075271606445\n",
      "Optimizing model...\n",
      "Step: 8, Action: 1, Reward: -5.0, Total Reward: 50.25, Loss: 5.409965515136719\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 9, Action: 0, Reward: 10.0, Total Reward: 60.25, Loss: 5.765453815460205\n",
      "Model acting\n",
      "\n",
      "episode: 110, reward: -39.75\n",
      "Model saved\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: 10.0, Total Reward: 10.0, Loss: 6.360785007476807\n",
      "Optimizing model...\n",
      "Step: 1, Action: 4, Reward: 0.2, Total Reward: 10.2, Loss: 6.709394454956055\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: 10.0, Total Reward: 20.2, Loss: 5.2902703285217285\n",
      "Optimizing model...\n",
      "Step: 3, Action: 4, Reward: 0.2, Total Reward: 20.4, Loss: 5.371616363525391\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: 10.0, Total Reward: 30.4, Loss: 7.309173583984375\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 5, Action: 0, Reward: 10.0, Total Reward: 40.4, Loss: 5.563939094543457\n",
      "Model acting\n",
      "\n",
      "episode: 111, reward: -59.6\n",
      "Optimizing model...\n",
      "Step: 0, Action: 1, Reward: -5.0, Total Reward: -5.0, Loss: 8.230473518371582\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: 10.0, Total Reward: 5.0, Loss: 2.8179521560668945\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: 0.0, Total Reward: 5.0, Loss: 6.96076774597168\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: 10.0, Total Reward: 15.0, Loss: 7.146165370941162\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: 10.0, Total Reward: 25.0, Loss: 3.2456040382385254\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 5, Action: 0, Reward: 0.0, Total Reward: 25.0, Loss: 7.053262710571289\n",
      "Model acting\n",
      "\n",
      "episode: 112, reward: -75.0\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: 10.0, Total Reward: 10.0, Loss: 2.3866732120513916\n",
      "Optimizing model...\n",
      "Step: 1, Action: 3, Reward: 0.25, Total Reward: 10.25, Loss: 6.393485069274902\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: 10.0, Total Reward: 20.25, Loss: 6.482512950897217\n",
      "Optimizing model...\n",
      "Step: 3, Action: 4, Reward: 0.2, Total Reward: 20.45, Loss: 2.4496970176696777\n",
      "Optimizing model...\n",
      "Step: 4, Action: 1, Reward: -5.0, Total Reward: 15.45, Loss: 7.000051498413086\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 5, Action: 0, Reward: 10.0, Total Reward: 25.45, Loss: 1.393251895904541\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 6, Action: 0, Reward: 0.0, Total Reward: 25.45, Loss: 5.800670146942139\n",
      "Optimizing model...\n",
      "Step: 7, Action: 3, Reward: 0.25, Total Reward: 25.7, Loss: 8.59747314453125\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 8, Action: 0, Reward: 10.0, Total Reward: 35.7, Loss: 11.71418285369873\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 9, Action: 0, Reward: 10.0, Total Reward: 45.7, Loss: 6.153916358947754\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 10, Action: 0, Reward: 0.0, Total Reward: 45.7, Loss: 5.069411277770996\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 11, Action: 0, Reward: 10.0, Total Reward: 55.7, Loss: 5.459095478057861\n",
      "Model acting\n",
      "\n",
      "episode: 113, reward: -44.3\n",
      "Optimizing model...\n",
      "Step: 0, Action: 3, Reward: 0.25, Total Reward: 0.25, Loss: 8.920701026916504\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: 10.0, Total Reward: 10.25, Loss: 2.0490517616271973\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: 0.0, Total Reward: 10.25, Loss: 5.087644577026367\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: 0.0, Total Reward: 10.25, Loss: 2.404946804046631\n",
      "Optimizing model...\n",
      "Step: 4, Action: 4, Reward: 0.2, Total Reward: 10.45, Loss: 6.682428359985352\n",
      "\n",
      "episode: 114, reward: -89.55\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: 10.0, Total Reward: 10.0, Loss: 8.716569900512695\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: 10.0, Total Reward: 20.0, Loss: 5.10912561416626\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: 0.0, Total Reward: 20.0, Loss: 3.963352680206299\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: 10.0, Total Reward: 30.0, Loss: 3.9516854286193848\n",
      "Optimizing model...\n",
      "Step: 4, Action: 3, Reward: 0.25, Total Reward: 30.25, Loss: 4.858779430389404\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 5, Action: 0, Reward: 10.0, Total Reward: 40.25, Loss: 3.50805401802063\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 6, Action: 0, Reward: 10.0, Total Reward: 50.25, Loss: 3.807511329650879\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 7, Action: 0, Reward: 10.0, Total Reward: 60.25, Loss: 3.3887577056884766\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 8, Action: 0, Reward: 10.0, Total Reward: 70.25, Loss: 5.30515718460083\n",
      "Optimizing model...\n",
      "Step: 9, Action: 4, Reward: 0.2, Total Reward: 70.45, Loss: 8.700496673583984\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 10, Action: 0, Reward: 10.0, Total Reward: 80.45, Loss: 5.628840923309326\n",
      "Model acting\n",
      "\n",
      "episode: 115, reward: -19.549999999999997\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: 10.0, Total Reward: 10.0, Loss: 10.466073989868164\n",
      "Optimizing model...\n",
      "Step: 1, Action: 3, Reward: 0.25, Total Reward: 10.25, Loss: 8.009069442749023\n",
      "Optimizing model...\n",
      "Step: 2, Action: 3, Reward: -9.75, Total Reward: 0.5, Loss: 8.723392486572266\n",
      "Optimizing model...\n",
      "Step: 3, Action: 4, Reward: 0.2, Total Reward: 0.7, Loss: 5.276609420776367\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: 10.0, Total Reward: 10.7, Loss: 4.658398628234863\n",
      "Optimizing model...\n",
      "Step: 5, Action: 4, Reward: 0.2, Total Reward: 10.899999999999999, Loss: 5.6056365966796875\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 6, Action: 0, Reward: 10.0, Total Reward: 20.9, Loss: 6.627381801605225\n",
      "Optimizing model...\n",
      "Step: 7, Action: 4, Reward: 0.2, Total Reward: 21.099999999999998, Loss: 5.236186981201172\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 8, Action: 0, Reward: 10.0, Total Reward: 31.099999999999998, Loss: 6.248874187469482\n",
      "\n",
      "episode: 116, reward: -68.9\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: 10.0, Total Reward: 10.0, Loss: 5.445806503295898\n",
      "Optimizing model...\n",
      "Step: 1, Action: 2, Reward: 0.25, Total Reward: 10.25, Loss: 9.942468643188477\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: 10.0, Total Reward: 20.25, Loss: 4.958355903625488\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: 10.0, Total Reward: 30.25, Loss: 8.470001220703125\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: 0.0, Total Reward: 30.25, Loss: 6.182319164276123\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 5, Action: 0, Reward: 0.0, Total Reward: 30.25, Loss: 6.976806163787842\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 6, Action: 0, Reward: 0.0, Total Reward: 30.25, Loss: 9.03213119506836\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 7, Action: 0, Reward: 10.0, Total Reward: 40.25, Loss: 5.802426338195801\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 8, Action: 0, Reward: 10.0, Total Reward: 50.25, Loss: 5.68794584274292\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 9, Action: 0, Reward: 0.0, Total Reward: 50.25, Loss: 2.2294774055480957\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 10, Action: 0, Reward: 10.0, Total Reward: 60.25, Loss: 4.182292938232422\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 11, Action: 0, Reward: 0.0, Total Reward: 60.25, Loss: 10.344005584716797\n",
      "Optimizing model...\n",
      "Step: 12, Action: 1, Reward: -5.0, Total Reward: 55.25, Loss: 6.8976898193359375\n",
      "Optimizing model...\n",
      "Step: 13, Action: 4, Reward: 0.2, Total Reward: 55.45, Loss: 3.3414559364318848\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 14, Action: 0, Reward: 10.0, Total Reward: 65.45, Loss: 8.081255912780762\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 15, Action: 0, Reward: 10.0, Total Reward: 75.45, Loss: 3.9589710235595703\n",
      "Model acting\n",
      "\n",
      "episode: 117, reward: -24.549999999999997\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: 10.0, Total Reward: 10.0, Loss: 5.497130870819092\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: 10.0, Total Reward: 20.0, Loss: 3.664618968963623\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: 10.0, Total Reward: 30.0, Loss: 5.423180103302002\n",
      "Optimizing model...\n",
      "Step: 3, Action: 3, Reward: 0.25, Total Reward: 30.25, Loss: 5.109726428985596\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: 10.0, Total Reward: 40.25, Loss: 8.018538475036621\n",
      "Optimizing model...\n",
      "Step: 5, Action: 1, Reward: -5.0, Total Reward: 35.25, Loss: 9.725863456726074\n",
      "Optimizing model...\n",
      "Step: 6, Action: 4, Reward: 0.2, Total Reward: 35.45, Loss: 7.845800876617432\n",
      "Optimizing model...\n",
      "Step: 7, Action: 3, Reward: 0.25, Total Reward: 35.7, Loss: 5.557140350341797\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 8, Action: 0, Reward: 10.0, Total Reward: 45.7, Loss: 7.239109516143799\n",
      "Optimizing model...\n",
      "Step: 9, Action: 4, Reward: 0.2, Total Reward: 45.900000000000006, Loss: 9.21135139465332\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 10, Action: 0, Reward: 10.0, Total Reward: 55.900000000000006, Loss: 3.7933852672576904\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 11, Action: 0, Reward: 10.0, Total Reward: 65.9, Loss: 9.378874778747559\n",
      "Optimizing model...\n",
      "Step: 12, Action: 2, Reward: 0.25, Total Reward: 66.15, Loss: 5.668088912963867\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 13, Action: 0, Reward: 10.0, Total Reward: 76.15, Loss: 7.845767021179199\n",
      "Model acting\n",
      "\n",
      "episode: 118, reward: -23.849999999999994\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: 10.0, Total Reward: 10.0, Loss: 1.6815485954284668\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: 10.0, Total Reward: 20.0, Loss: 3.807493209838867\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: 0.0, Total Reward: 20.0, Loss: 1.870241403579712\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: 10.0, Total Reward: 30.0, Loss: 5.50566291809082\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: 10.0, Total Reward: 40.0, Loss: 7.342159748077393\n",
      "Model acting\n",
      "\n",
      "episode: 119, reward: -60.0\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: 10.0, Total Reward: 10.0, Loss: 6.457586288452148\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: 10.0, Total Reward: 20.0, Loss: 5.071934223175049\n",
      "Optimizing model...\n",
      "Step: 2, Action: 4, Reward: 0.2, Total Reward: 20.2, Loss: 9.355770111083984\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: 10.0, Total Reward: 30.2, Loss: 4.1233320236206055\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: 10.0, Total Reward: 40.2, Loss: 6.882579803466797\n",
      "Optimizing model...\n",
      "Step: 5, Action: 0, Reward: 10.0, Total Reward: 50.2, Loss: 5.359180450439453\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 6, Action: 0, Reward: 10.0, Total Reward: 60.2, Loss: 4.967555046081543\n",
      "Model acting\n",
      "\n",
      "episode: 120, reward: -39.8\n",
      "Model saved\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: 10.0, Total Reward: 10.0, Loss: 5.43895959854126\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: 10.0, Total Reward: 20.0, Loss: 2.909381151199341\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: 10.0, Total Reward: 30.0, Loss: 8.174324035644531\n",
      "Optimizing model...\n",
      "Step: 3, Action: 3, Reward: 0.25, Total Reward: 30.25, Loss: 5.4417195320129395\n",
      "Optimizing model...\n",
      "Step: 4, Action: 1, Reward: -5.0, Total Reward: 25.25, Loss: 7.969554901123047\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 5, Action: 0, Reward: 10.0, Total Reward: 35.25, Loss: 7.126083850860596\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 6, Action: 0, Reward: 0.0, Total Reward: 35.25, Loss: 4.683237075805664\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 7, Action: 0, Reward: 10.0, Total Reward: 45.25, Loss: 3.7767810821533203\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 8, Action: 0, Reward: 0.0, Total Reward: 45.25, Loss: 3.237990379333496\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 9, Action: 0, Reward: 0.0, Total Reward: 45.25, Loss: 2.493377208709717\n",
      "Optimizing model...\n",
      "Step: 10, Action: 0, Reward: 10.0, Total Reward: 55.25, Loss: 4.299837112426758\n",
      "Optimizing model...\n",
      "Step: 11, Action: 3, Reward: 0.25, Total Reward: 55.5, Loss: 6.974611282348633\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 12, Action: 0, Reward: 10.0, Total Reward: 65.5, Loss: 8.217853546142578\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 13, Action: 0, Reward: 10.0, Total Reward: 75.5, Loss: 2.6184592247009277\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 14, Action: 0, Reward: 0.0, Total Reward: 75.5, Loss: 8.749529838562012\n",
      "Model acting\n",
      "\n",
      "episode: 121, reward: -24.5\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: 10.0, Total Reward: 10.0, Loss: 10.591493606567383\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: 10.0, Total Reward: 20.0, Loss: 6.539972305297852\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: 0.0, Total Reward: 20.0, Loss: 6.86528205871582\n",
      "Optimizing model...\n",
      "Step: 3, Action: 1, Reward: -5.0, Total Reward: 15.0, Loss: 5.721566200256348\n",
      "Optimizing model...\n",
      "Step: 4, Action: 4, Reward: 0.2, Total Reward: 15.2, Loss: 5.103428840637207\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 5, Action: 0, Reward: 10.0, Total Reward: 25.2, Loss: 4.731495380401611\n",
      "Optimizing model...\n",
      "Step: 6, Action: 3, Reward: 0.25, Total Reward: 25.45, Loss: 3.831724166870117\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 7, Action: 0, Reward: 10.0, Total Reward: 35.45, Loss: 6.180702209472656\n",
      "Optimizing model...\n",
      "Step: 8, Action: 3, Reward: 0.25, Total Reward: 35.7, Loss: 5.999762535095215\n",
      "Optimizing model...\n",
      "Step: 9, Action: 0, Reward: 10.0, Total Reward: 45.7, Loss: 3.0854194164276123\n",
      "Optimizing model...\n",
      "Step: 10, Action: 1, Reward: -5.0, Total Reward: 40.7, Loss: 7.263077259063721\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 11, Action: 0, Reward: 10.0, Total Reward: 50.7, Loss: 10.319780349731445\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 12, Action: 0, Reward: 0.0, Total Reward: 50.7, Loss: 2.20845890045166\n",
      "Model acting\n",
      "\n",
      "episode: 122, reward: -49.3\n",
      "Optimizing model...\n",
      "Step: 0, Action: 3, Reward: 0.25, Total Reward: 0.25, Loss: 8.30174446105957\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: 10.0, Total Reward: 10.25, Loss: 8.320259094238281\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: 10.0, Total Reward: 20.25, Loss: 7.197690010070801\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: 0.0, Total Reward: 20.25, Loss: 3.982555866241455\n",
      "Optimizing model...\n",
      "Step: 4, Action: 3, Reward: 0.25, Total Reward: 20.5, Loss: 5.26296854019165\n",
      "Optimizing model...\n",
      "Step: 5, Action: 4, Reward: 0.2, Total Reward: 20.7, Loss: 7.0435943603515625\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 6, Action: 0, Reward: 10.0, Total Reward: 30.7, Loss: 5.3519463539123535\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 7, Action: 0, Reward: 0.0, Total Reward: 30.7, Loss: 7.627369403839111\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 8, Action: 0, Reward: 10.0, Total Reward: 40.7, Loss: 6.125993728637695\n",
      "Model acting\n",
      "\n",
      "episode: 123, reward: -59.3\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: 10.0, Total Reward: 10.0, Loss: 6.073997497558594\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: 10.0, Total Reward: 20.0, Loss: 4.070720195770264\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: 10.0, Total Reward: 30.0, Loss: 4.988136291503906\n",
      "Optimizing model...\n",
      "Step: 3, Action: 2, Reward: 0.25, Total Reward: 30.25, Loss: 4.444277763366699\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: 10.0, Total Reward: 40.25, Loss: 8.782625198364258\n",
      "Optimizing model...\n",
      "Step: 5, Action: 3, Reward: 0.25, Total Reward: 40.5, Loss: 3.890829086303711\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 6, Action: 0, Reward: 10.0, Total Reward: 50.5, Loss: 5.1625776290893555\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 7, Action: 0, Reward: 0.0, Total Reward: 50.5, Loss: 8.337672233581543\n",
      "Optimizing model...\n",
      "Step: 8, Action: 0, Reward: 0.0, Total Reward: 50.5, Loss: 8.394493103027344\n",
      "Optimizing model...\n",
      "Step: 9, Action: 3, Reward: 0.25, Total Reward: 50.75, Loss: 3.4453043937683105\n",
      "Model acting\n",
      "\n",
      "episode: 124, reward: -49.25\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: 10.0, Total Reward: 10.0, Loss: 9.315356254577637\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: 10.0, Total Reward: 20.0, Loss: 4.218391418457031\n",
      "Optimizing model...\n",
      "Step: 2, Action: 1, Reward: -5.0, Total Reward: 15.0, Loss: 4.156315803527832\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: 10.0, Total Reward: 25.0, Loss: 5.338624954223633\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: 0.0, Total Reward: 25.0, Loss: 4.436470985412598\n",
      "Model acting\n",
      "\n",
      "episode: 125, reward: -75.0\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: 10.0, Total Reward: 10.0, Loss: 5.26096248626709\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: 10.0, Total Reward: 20.0, Loss: 9.942391395568848\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: 0.0, Total Reward: 20.0, Loss: 5.170895576477051\n",
      "Optimizing model...\n",
      "Step: 3, Action: 3, Reward: 0.25, Total Reward: 20.25, Loss: 7.202769756317139\n",
      "Optimizing model...\n",
      "Step: 4, Action: 2, Reward: 0.25, Total Reward: 20.5, Loss: 5.267881393432617\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 5, Action: 0, Reward: 10.0, Total Reward: 30.5, Loss: 5.324129581451416\n",
      "Optimizing model...\n",
      "Step: 6, Action: 4, Reward: 0.2, Total Reward: 30.7, Loss: 7.954156398773193\n",
      "Optimizing model...\n",
      "Step: 7, Action: 3, Reward: 0.25, Total Reward: 30.95, Loss: 10.56668472290039\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 8, Action: 0, Reward: 10.0, Total Reward: 40.95, Loss: 10.965385437011719\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 9, Action: 0, Reward: 10.0, Total Reward: 50.95, Loss: 3.8794970512390137\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 10, Action: 0, Reward: 0.0, Total Reward: 50.95, Loss: 9.00011157989502\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 11, Action: 0, Reward: 0.0, Total Reward: 50.95, Loss: 11.059926986694336\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 12, Action: 0, Reward: 10.0, Total Reward: 60.95, Loss: 3.7379069328308105\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 13, Action: 0, Reward: 0.0, Total Reward: 60.95, Loss: 11.953573226928711\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 14, Action: 0, Reward: 0.0, Total Reward: 60.95, Loss: 4.998340606689453\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 15, Action: 0, Reward: 0.0, Total Reward: 60.95, Loss: 4.142396926879883\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 16, Action: 0, Reward: 0.0, Total Reward: 60.95, Loss: 5.360257148742676\n",
      "Optimizing model...\n",
      "Step: 17, Action: 1, Reward: -5.0, Total Reward: 55.95, Loss: 13.403576850891113\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 18, Action: 0, Reward: 10.0, Total Reward: 65.95, Loss: 1.5948305130004883\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 19, Action: 0, Reward: 10.0, Total Reward: 75.95, Loss: 3.7394862174987793\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 20, Action: 0, Reward: 10.0, Total Reward: 85.95, Loss: 5.006304740905762\n",
      "Optimizing model...\n",
      "Step: 21, Action: 2, Reward: 0.25, Total Reward: 86.2, Loss: 5.725740432739258\n",
      "\n",
      "episode: 126, reward: -13.799999999999997\n",
      "Optimizing model...\n",
      "Step: 0, Action: 4, Reward: 0.2, Total Reward: 0.2, Loss: 6.979195594787598\n",
      "Optimizing model...\n",
      "Step: 1, Action: 3, Reward: 0.25, Total Reward: 0.45, Loss: 12.35767936706543\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: 10.0, Total Reward: 10.45, Loss: 3.5955586433410645\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: 10.0, Total Reward: 20.45, Loss: 6.677586555480957\n",
      "Optimizing model...\n",
      "Step: 4, Action: 2, Reward: 0.25, Total Reward: 20.7, Loss: 6.019044399261475\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 5, Action: 0, Reward: 10.0, Total Reward: 30.7, Loss: 6.788496494293213\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 6, Action: 0, Reward: 10.0, Total Reward: 40.7, Loss: 3.288066864013672\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 7, Action: 0, Reward: 0.0, Total Reward: 40.7, Loss: 2.208042621612549\n",
      "Optimizing model...\n",
      "Step: 8, Action: 2, Reward: 0.25, Total Reward: 40.95, Loss: 9.968931198120117\n",
      "Optimizing model...\n",
      "Step: 9, Action: 2, Reward: 0.25, Total Reward: 41.2, Loss: 6.757554054260254\n",
      "Optimizing model...\n",
      "Step: 10, Action: 4, Reward: 0.2, Total Reward: 41.400000000000006, Loss: 7.041403770446777\n",
      "Optimizing model...\n",
      "Step: 11, Action: 2, Reward: 0.25, Total Reward: 41.650000000000006, Loss: 5.216472625732422\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 12, Action: 0, Reward: 10.0, Total Reward: 51.650000000000006, Loss: 1.621908187866211\n",
      "Model acting\n",
      "\n",
      "episode: 127, reward: -48.349999999999994\n",
      "Optimizing model...\n",
      "Step: 0, Action: 1, Reward: -5.0, Total Reward: -5.0, Loss: 8.229469299316406\n",
      "Optimizing model...\n",
      "Step: 1, Action: 1, Reward: -5.0, Total Reward: -10.0, Loss: 6.830551624298096\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: 10.0, Total Reward: 0.0, Loss: 4.171596527099609\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: 10.0, Total Reward: 10.0, Loss: 4.573002338409424\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: 0.0, Total Reward: 10.0, Loss: 3.9128665924072266\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 5, Action: 2, Reward: 0.25, Total Reward: 10.25, Loss: 4.466605186462402\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 6, Action: 0, Reward: 10.0, Total Reward: 20.25, Loss: 7.188603401184082\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 7, Action: 0, Reward: 0.0, Total Reward: 20.25, Loss: 4.646179676055908\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 8, Action: 0, Reward: 10.0, Total Reward: 30.25, Loss: 2.959425687789917\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 9, Action: 0, Reward: 0.0, Total Reward: 30.25, Loss: 7.791877269744873\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 10, Action: 0, Reward: 10.0, Total Reward: 40.25, Loss: 5.717021942138672\n",
      "\n",
      "episode: 128, reward: -59.75\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: 10.0, Total Reward: 10.0, Loss: 4.505537986755371\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: 10.0, Total Reward: 20.0, Loss: 4.245034694671631\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: 0.0, Total Reward: 20.0, Loss: 6.462162494659424\n",
      "Optimizing model...\n",
      "Step: 3, Action: 4, Reward: 0.2, Total Reward: 20.2, Loss: 9.138015747070312\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: 10.0, Total Reward: 30.2, Loss: 9.750875473022461\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 5, Action: 0, Reward: 0.0, Total Reward: 30.2, Loss: 4.040500640869141\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 6, Action: 0, Reward: 10.0, Total Reward: 40.2, Loss: 7.99140739440918\n",
      "Optimizing model...\n",
      "Step: 7, Action: 0, Reward: 0.0, Total Reward: 40.2, Loss: 5.231680393218994\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 8, Action: 0, Reward: 0.0, Total Reward: 40.2, Loss: 5.528232097625732\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 9, Action: 0, Reward: 0.0, Total Reward: 40.2, Loss: 5.106978893280029\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 10, Action: 0, Reward: 0.0, Total Reward: 40.2, Loss: 3.6387577056884766\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 11, Action: 0, Reward: 0.0, Total Reward: 40.2, Loss: 6.590712547302246\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 12, Action: 0, Reward: 0.0, Total Reward: 40.2, Loss: 5.8495073318481445\n",
      "Optimizing model...\n",
      "Step: 13, Action: 3, Reward: 0.25, Total Reward: 40.45, Loss: 8.492572784423828\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 14, Action: 0, Reward: 10.0, Total Reward: 50.45, Loss: 4.290068626403809\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 15, Action: 0, Reward: 10.0, Total Reward: 60.45, Loss: 3.8839402198791504\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 16, Action: 0, Reward: 0.0, Total Reward: 60.45, Loss: 6.270803451538086\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 17, Action: 0, Reward: 0.0, Total Reward: 60.45, Loss: 3.3559231758117676\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 18, Action: 0, Reward: 0.0, Total Reward: 60.45, Loss: 7.074265480041504\n",
      "Optimizing model...\n",
      "Step: 19, Action: 4, Reward: 0.2, Total Reward: 60.650000000000006, Loss: 6.232754707336426\n",
      "Optimizing model...\n",
      "Step: 20, Action: 4, Reward: 0.2, Total Reward: 60.85000000000001, Loss: 5.1588850021362305\n",
      "Optimizing model...\n",
      "Step: 21, Action: 0, Reward: 10.0, Total Reward: 70.85000000000001, Loss: 5.68985652923584\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 22, Action: 0, Reward: 10.0, Total Reward: 80.85000000000001, Loss: 2.1798529624938965\n",
      "\n",
      "episode: 129, reward: -19.14999999999999\n",
      "Optimizing model...\n",
      "Step: 0, Action: 1, Reward: -5.0, Total Reward: -5.0, Loss: 3.638946294784546\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: 10.0, Total Reward: 5.0, Loss: 2.085542678833008\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: 10.0, Total Reward: 15.0, Loss: 3.550940990447998\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: 0.0, Total Reward: 15.0, Loss: 10.726242065429688\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: 10.0, Total Reward: 25.0, Loss: 3.8747966289520264\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 5, Action: 0, Reward: 0.0, Total Reward: 25.0, Loss: 5.261272430419922\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 6, Action: 0, Reward: 10.0, Total Reward: 35.0, Loss: 2.4967589378356934\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 7, Action: 0, Reward: 0.0, Total Reward: 35.0, Loss: 5.650659084320068\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 8, Action: 0, Reward: 0.0, Total Reward: 35.0, Loss: 6.578311920166016\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 9, Action: 0, Reward: 10.0, Total Reward: 45.0, Loss: 4.941461563110352\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 10, Action: 0, Reward: 10.0, Total Reward: 55.0, Loss: 2.6743414402008057\n",
      "\n",
      "episode: 130, reward: -45.0\n",
      "Model saved\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: 10.0, Total Reward: 10.0, Loss: 4.774133682250977\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: 10.0, Total Reward: 20.0, Loss: 8.190253257751465\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: 10.0, Total Reward: 30.0, Loss: 6.298713207244873\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: 10.0, Total Reward: 40.0, Loss: 4.88525390625\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: 0.0, Total Reward: 40.0, Loss: 3.432518482208252\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 5, Action: 0, Reward: 0.0, Total Reward: 40.0, Loss: 10.73054313659668\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 6, Action: 0, Reward: 0.0, Total Reward: 40.0, Loss: 7.09873104095459\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 7, Action: 0, Reward: 0.0, Total Reward: 40.0, Loss: 7.07896614074707\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 8, Action: 0, Reward: 0.0, Total Reward: 40.0, Loss: 5.500233173370361\n",
      "Optimizing model...\n",
      "Step: 9, Action: 2, Reward: 0.25, Total Reward: 40.25, Loss: 3.4580931663513184\n",
      "Optimizing model...\n",
      "Step: 10, Action: 2, Reward: 0.25, Total Reward: 40.5, Loss: 10.00452709197998\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 11, Action: 0, Reward: 10.0, Total Reward: 50.5, Loss: 4.598164081573486\n",
      "Optimizing model...\n",
      "Step: 12, Action: 0, Reward: 10.0, Total Reward: 60.5, Loss: 5.877876281738281\n",
      "Optimizing model...\n",
      "Step: 13, Action: 2, Reward: 0.25, Total Reward: 60.75, Loss: 5.168375015258789\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 14, Action: 0, Reward: 10.0, Total Reward: 70.75, Loss: 4.6110711097717285\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 15, Action: 0, Reward: 10.0, Total Reward: 80.75, Loss: 8.727553367614746\n",
      "Optimizing model...\n",
      "Step: 16, Action: 3, Reward: 0.25, Total Reward: 81.0, Loss: 4.535921573638916\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 17, Action: 0, Reward: 10.0, Total Reward: 91.0, Loss: 10.333063125610352\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 18, Action: 0, Reward: 10.0, Total Reward: 101.0, Loss: 8.063531875610352\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 19, Action: 0, Reward: 0.0, Total Reward: 101.0, Loss: 7.218578338623047\n",
      "Model acting\n",
      "\n",
      "episode: 131, reward: 1.0\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: 10.0, Total Reward: 10.0, Loss: 4.9256439208984375\n",
      "Optimizing model...\n",
      "Step: 1, Action: 2, Reward: 0.25, Total Reward: 10.25, Loss: 3.4335427284240723\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: 10.0, Total Reward: 20.25, Loss: 5.775350570678711\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: 10.0, Total Reward: 30.25, Loss: 4.794364929199219\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: 10.0, Total Reward: 40.25, Loss: 5.527554512023926\n",
      "Model acting\n",
      "\n",
      "episode: 132, reward: -59.75\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: 10.0, Total Reward: 10.0, Loss: 6.604634761810303\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: 10.0, Total Reward: 20.0, Loss: 9.640393257141113\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: 0.0, Total Reward: 20.0, Loss: 3.5794601440429688\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: 10.0, Total Reward: 30.0, Loss: 6.873588562011719\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: 0.0, Total Reward: 30.0, Loss: 5.039575576782227\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 5, Action: 0, Reward: 0.0, Total Reward: 30.0, Loss: 4.887992858886719\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 6, Action: 0, Reward: 10.0, Total Reward: 40.0, Loss: 5.0231547355651855\n",
      "Optimizing model...\n",
      "Step: 7, Action: 3, Reward: 0.25, Total Reward: 40.25, Loss: 7.028443813323975\n",
      "Model acting\n",
      "\n",
      "episode: 133, reward: -59.75\n",
      "Optimizing model...\n",
      "Step: 0, Action: 4, Reward: 0.2, Total Reward: 0.2, Loss: 3.9346861839294434\n",
      "Optimizing model...\n",
      "Step: 1, Action: 3, Reward: 0.25, Total Reward: 0.45, Loss: 2.793405055999756\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: 10.0, Total Reward: 10.45, Loss: 3.310229778289795\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: 10.0, Total Reward: 20.45, Loss: 11.975975036621094\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: 0.0, Total Reward: 20.45, Loss: 5.033145904541016\n",
      "Optimizing model...\n",
      "Step: 5, Action: 4, Reward: 0.2, Total Reward: 20.65, Loss: 8.051944732666016\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 6, Action: 0, Reward: 10.0, Total Reward: 30.65, Loss: 4.216022968292236\n",
      "Optimizing model...\n",
      "Step: 7, Action: 1, Reward: -5.0, Total Reward: 25.65, Loss: 5.650609970092773\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 8, Action: 0, Reward: 10.0, Total Reward: 35.65, Loss: 6.66957950592041\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 9, Action: 0, Reward: 10.0, Total Reward: 45.65, Loss: 5.334588050842285\n",
      "Optimizing model...\n",
      "Step: 10, Action: 1, Reward: -5.0, Total Reward: 40.65, Loss: 4.236799240112305\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 11, Action: 0, Reward: 10.0, Total Reward: 50.65, Loss: 3.709850788116455\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 12, Action: 0, Reward: 0.0, Total Reward: 50.65, Loss: 8.360652923583984\n",
      "Optimizing model...\n",
      "Step: 13, Action: 1, Reward: -5.0, Total Reward: 45.65, Loss: 4.852117538452148\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 14, Action: 0, Reward: 10.0, Total Reward: 55.65, Loss: 3.642983913421631\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 15, Action: 0, Reward: 0.0, Total Reward: 55.65, Loss: 3.5852537155151367\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 16, Action: 0, Reward: 0.0, Total Reward: 55.65, Loss: 3.544638156890869\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 17, Action: 0, Reward: 0.0, Total Reward: 55.65, Loss: 8.538622856140137\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 18, Action: 0, Reward: 10.0, Total Reward: 65.65, Loss: 11.352890014648438\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 19, Action: 0, Reward: 10.0, Total Reward: 75.65, Loss: 7.796034812927246\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 20, Action: 0, Reward: 10.0, Total Reward: 85.65, Loss: 4.517449378967285\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 21, Action: 0, Reward: 0.0, Total Reward: 85.65, Loss: 3.820552349090576\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 22, Action: 0, Reward: 0.0, Total Reward: 85.65, Loss: 1.9850581884384155\n",
      "Optimizing model...\n",
      "Step: 23, Action: 4, Reward: 0.2, Total Reward: 85.85000000000001, Loss: 2.319690704345703\n",
      "Model acting\n",
      "\n",
      "episode: 134, reward: -14.149999999999991\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: 10.0, Total Reward: 10.0, Loss: 5.3542680740356445\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: 10.0, Total Reward: 20.0, Loss: 6.296025276184082\n",
      "Optimizing model...\n",
      "Step: 2, Action: 1, Reward: -5.0, Total Reward: 15.0, Loss: 3.6180899143218994\n",
      "Model acting\n",
      "\n",
      "episode: 135, reward: -85.0\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: 10.0, Total Reward: 10.0, Loss: 4.779668807983398\n",
      "Optimizing model...\n",
      "Step: 1, Action: 2, Reward: 0.25, Total Reward: 10.25, Loss: 6.071232318878174\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: 10.0, Total Reward: 20.25, Loss: 5.345113754272461\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: 0.0, Total Reward: 20.25, Loss: 7.1581902503967285\n",
      "Optimizing model...\n",
      "Step: 4, Action: 1, Reward: -5.0, Total Reward: 15.25, Loss: 2.881654739379883\n",
      "\n",
      "episode: 136, reward: -84.75\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: 10.0, Total Reward: 10.0, Loss: 4.324456214904785\n",
      "Optimizing model...\n",
      "Step: 1, Action: 4, Reward: 0.2, Total Reward: 10.2, Loss: 4.078482627868652\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: 10.0, Total Reward: 20.2, Loss: 6.370519638061523\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: 10.0, Total Reward: 30.2, Loss: 6.596959590911865\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: 0.0, Total Reward: 30.2, Loss: 6.568966388702393\n",
      "Model acting\n",
      "\n",
      "episode: 137, reward: -69.8\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: 10.0, Total Reward: 10.0, Loss: 4.679414749145508\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: 10.0, Total Reward: 20.0, Loss: 5.117433547973633\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: 0.0, Total Reward: 20.0, Loss: 10.048154830932617\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: 10.0, Total Reward: 30.0, Loss: 6.35486364364624\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: 10.0, Total Reward: 40.0, Loss: 5.2103471755981445\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 5, Action: 0, Reward: 0.0, Total Reward: 40.0, Loss: 7.141338348388672\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 6, Action: 0, Reward: 0.0, Total Reward: 40.0, Loss: 7.707108497619629\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 7, Action: 0, Reward: 0.0, Total Reward: 40.0, Loss: 7.861617088317871\n",
      "Optimizing model...\n",
      "Step: 8, Action: 3, Reward: 0.25, Total Reward: 40.25, Loss: 3.189439296722412\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 9, Action: 0, Reward: 10.0, Total Reward: 50.25, Loss: 8.807662010192871\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 10, Action: 0, Reward: 0.0, Total Reward: 50.25, Loss: 6.838946342468262\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 11, Action: 0, Reward: 10.0, Total Reward: 60.25, Loss: 6.697142601013184\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 12, Action: 0, Reward: 10.0, Total Reward: 70.25, Loss: 5.4523701667785645\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 13, Action: 0, Reward: 10.0, Total Reward: 80.25, Loss: 6.222142219543457\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 14, Action: 0, Reward: 10.0, Total Reward: 90.25, Loss: 6.2784013748168945\n",
      "Optimizing model...\n",
      "Step: 15, Action: 4, Reward: 0.2, Total Reward: 90.45, Loss: 4.345990180969238\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 16, Action: 0, Reward: 10.0, Total Reward: 100.45, Loss: 4.506387710571289\n",
      "Model acting\n",
      "\n",
      "episode: 138, reward: 0.45000000000000284\n",
      "Optimizing model...\n",
      "Step: 0, Action: 2, Reward: 0.25, Total Reward: 0.25, Loss: 9.988688468933105\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: 10.0, Total Reward: 10.25, Loss: 7.560246467590332\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: 10.0, Total Reward: 20.25, Loss: 5.546788215637207\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: 10.0, Total Reward: 30.25, Loss: 4.767699718475342\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: 10.0, Total Reward: 40.25, Loss: 5.931816101074219\n",
      "\n",
      "episode: 139, reward: -59.75\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: 10.0, Total Reward: 10.0, Loss: 2.2060089111328125\n",
      "Optimizing model...\n",
      "Step: 1, Action: 3, Reward: 0.25, Total Reward: 10.25, Loss: 5.22867488861084\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: 10.0, Total Reward: 20.25, Loss: 6.231481075286865\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: 0.0, Total Reward: 20.25, Loss: 2.8311004638671875\n",
      "Optimizing model...\n",
      "Step: 4, Action: 3, Reward: 0.25, Total Reward: 20.5, Loss: 8.11878490447998\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 5, Action: 0, Reward: 10.0, Total Reward: 30.5, Loss: 3.6728782653808594\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 6, Action: 0, Reward: 10.0, Total Reward: 40.5, Loss: 5.240810394287109\n",
      "Optimizing model...\n",
      "Step: 7, Action: 3, Reward: 0.25, Total Reward: 40.75, Loss: 5.630804061889648\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 8, Action: 0, Reward: 10.0, Total Reward: 50.75, Loss: 3.3749349117279053\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 9, Action: 0, Reward: 0.0, Total Reward: 50.75, Loss: 2.4233627319335938\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 10, Action: 0, Reward: 10.0, Total Reward: 60.75, Loss: 8.165273666381836\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 11, Action: 0, Reward: 10.0, Total Reward: 70.75, Loss: 7.88181209564209\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 12, Action: 0, Reward: 0.0, Total Reward: 70.75, Loss: 7.036502361297607\n",
      "Optimizing model...\n",
      "Step: 13, Action: 2, Reward: 0.25, Total Reward: 71.0, Loss: 5.5548014640808105\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 14, Action: 0, Reward: 10.0, Total Reward: 81.0, Loss: 7.485363006591797\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 15, Action: 0, Reward: 10.0, Total Reward: 91.0, Loss: 7.532665729522705\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 16, Action: 0, Reward: 10.0, Total Reward: 101.0, Loss: 2.5790443420410156\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 17, Action: 0, Reward: 0.0, Total Reward: 101.0, Loss: 8.305268287658691\n",
      "Optimizing model...\n",
      "Step: 18, Action: 3, Reward: 0.25, Total Reward: 101.25, Loss: 10.455892562866211\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 19, Action: 0, Reward: 10.0, Total Reward: 111.25, Loss: 7.4005537033081055\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 20, Action: 0, Reward: 10.0, Total Reward: 121.25, Loss: 2.4086172580718994\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 21, Action: 0, Reward: 10.0, Total Reward: 131.25, Loss: 4.155993461608887\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 22, Action: 0, Reward: 0.0, Total Reward: 131.25, Loss: 7.168926239013672\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 23, Action: 0, Reward: 10.0, Total Reward: 141.25, Loss: 4.419952392578125\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 24, Action: 0, Reward: 0.0, Total Reward: 141.25, Loss: 5.321159839630127\n",
      "Model acting\n",
      "\n",
      "episode: 140, reward: 41.25\n",
      "Model saved\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: 10.0, Total Reward: 10.0, Loss: 2.7417526245117188\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: 10.0, Total Reward: 20.0, Loss: 5.938353538513184\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: 10.0, Total Reward: 30.0, Loss: 3.7900590896606445\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: 10.0, Total Reward: 40.0, Loss: 8.66148567199707\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: 0.0, Total Reward: 40.0, Loss: 5.4597673416137695\n",
      "\n",
      "episode: 141, reward: -60.0\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: 10.0, Total Reward: 10.0, Loss: 6.526272773742676\n",
      "Optimizing model...\n",
      "Step: 1, Action: 2, Reward: 0.25, Total Reward: 10.25, Loss: 5.791854381561279\n",
      "Optimizing model...\n",
      "Step: 2, Action: 1, Reward: -5.0, Total Reward: 5.25, Loss: 6.304561614990234\n",
      "Optimizing model...\n",
      "Step: 3, Action: 2, Reward: 0.25, Total Reward: 5.5, Loss: 3.858912467956543\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: 10.0, Total Reward: 15.5, Loss: 8.571136474609375\n",
      "Optimizing model...\n",
      "Step: 5, Action: 0, Reward: 10.0, Total Reward: 25.5, Loss: 6.567142486572266\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 6, Action: 0, Reward: 10.0, Total Reward: 35.5, Loss: 7.292686462402344\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 7, Action: 0, Reward: 10.0, Total Reward: 45.5, Loss: 3.5801219940185547\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 8, Action: 0, Reward: 0.0, Total Reward: 45.5, Loss: 8.87471866607666\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 9, Action: 0, Reward: 0.0, Total Reward: 45.5, Loss: 12.872876167297363\n",
      "Model acting\n",
      "\n",
      "episode: 142, reward: -54.5\n",
      "Optimizing model...\n",
      "Step: 0, Action: 1, Reward: -5.0, Total Reward: -5.0, Loss: 2.53311824798584\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: 10.0, Total Reward: 5.0, Loss: 7.20007848739624\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: 10.0, Total Reward: 15.0, Loss: 4.110813140869141\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: 0.0, Total Reward: 15.0, Loss: 6.225564956665039\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: 10.0, Total Reward: 25.0, Loss: 4.478635787963867\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 5, Action: 0, Reward: 10.0, Total Reward: 35.0, Loss: 4.070092678070068\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 6, Action: 0, Reward: 10.0, Total Reward: 45.0, Loss: 7.996189117431641\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 7, Action: 0, Reward: 10.0, Total Reward: 55.0, Loss: 5.173827171325684\n",
      "Model acting\n",
      "\n",
      "episode: 143, reward: -45.0\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: 10.0, Total Reward: 10.0, Loss: 9.859685897827148\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: 10.0, Total Reward: 20.0, Loss: 7.0934929847717285\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: 0.0, Total Reward: 20.0, Loss: 4.056524276733398\n",
      "Optimizing model...\n",
      "Step: 3, Action: 3, Reward: 0.25, Total Reward: 20.25, Loss: 4.971222400665283\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: 10.0, Total Reward: 30.25, Loss: 5.0511980056762695\n",
      "Optimizing model...\n",
      "Step: 5, Action: 1, Reward: -5.0, Total Reward: 25.25, Loss: 2.4542758464813232\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 6, Action: 0, Reward: 10.0, Total Reward: 35.25, Loss: 4.861243724822998\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 7, Action: 0, Reward: 10.0, Total Reward: 45.25, Loss: 3.05606746673584\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 8, Action: 0, Reward: 10.0, Total Reward: 55.25, Loss: 2.166947603225708\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 9, Action: 0, Reward: 0.0, Total Reward: 55.25, Loss: 5.63871955871582\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 10, Action: 0, Reward: 10.0, Total Reward: 65.25, Loss: 2.8792238235473633\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 11, Action: 0, Reward: 10.0, Total Reward: 75.25, Loss: 5.162860870361328\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 12, Action: 0, Reward: 10.0, Total Reward: 85.25, Loss: 7.825000286102295\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 13, Action: 0, Reward: 10.0, Total Reward: 95.25, Loss: 7.72851037979126\n",
      "Model acting\n",
      "\n",
      "episode: 144, reward: -4.75\n",
      "Optimizing model...\n",
      "Step: 0, Action: 1, Reward: -5.0, Total Reward: -5.0, Loss: 10.57119083404541\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: 10.0, Total Reward: 5.0, Loss: 6.097771644592285\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: 10.0, Total Reward: 15.0, Loss: 4.555273532867432\n",
      "Optimizing model...\n",
      "Step: 3, Action: 3, Reward: 0.25, Total Reward: 15.25, Loss: 8.87032699584961\n",
      "Optimizing model...\n",
      "Step: 4, Action: 2, Reward: 0.25, Total Reward: 15.5, Loss: 5.831963539123535\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 5, Action: 0, Reward: 10.0, Total Reward: 25.5, Loss: 3.234503984451294\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 6, Action: 0, Reward: 10.0, Total Reward: 35.5, Loss: 8.310302734375\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 7, Action: 0, Reward: 0.0, Total Reward: 35.5, Loss: 5.590514183044434\n",
      "Model acting\n",
      "\n",
      "episode: 145, reward: -64.5\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: 10.0, Total Reward: 10.0, Loss: 5.109640121459961\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: 10.0, Total Reward: 20.0, Loss: 5.2189531326293945\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: 10.0, Total Reward: 30.0, Loss: 2.4755468368530273\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 4, Reward: 0.2, Total Reward: 30.2, Loss: 8.411540985107422\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: 10.0, Total Reward: 40.2, Loss: 6.680397987365723\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 5, Action: 0, Reward: 10.0, Total Reward: 50.2, Loss: 10.075607299804688\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 6, Action: 0, Reward: 10.0, Total Reward: 60.2, Loss: 7.104252815246582\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 7, Action: 3, Reward: 0.25, Total Reward: 60.45, Loss: 7.330573081970215\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 8, Action: 0, Reward: 10.0, Total Reward: 70.45, Loss: 10.148294448852539\n",
      "Optimizing model...\n",
      "Step: 9, Action: 4, Reward: 0.2, Total Reward: 70.65, Loss: 3.841346025466919\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 10, Action: 2, Reward: 0.25, Total Reward: 70.9, Loss: 5.024820327758789\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 11, Action: 3, Reward: 0.25, Total Reward: 71.15, Loss: 2.3314907550811768\n",
      "Optimizing model...\n",
      "Step: 12, Action: 4, Reward: 0.2, Total Reward: 71.35000000000001, Loss: 3.7096786499023438\n",
      "Model acting\n",
      "\n",
      "episode: 146, reward: -28.64999999999999\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 4, Reward: 0.2, Total Reward: 0.2, Loss: 5.181214809417725\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 1, Action: 4, Reward: -9.8, Total Reward: -9.600000000000001, Loss: 3.5130622386932373\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: 10.0, Total Reward: 0.3999999999999986, Loss: 5.606219291687012\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: 10.0, Total Reward: 10.399999999999999, Loss: 5.9865217208862305\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 4, Action: 3, Reward: 0.25, Total Reward: 10.649999999999999, Loss: 5.017082691192627\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 5, Action: 4, Reward: 0.2, Total Reward: 10.849999999999998, Loss: 3.717684268951416\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 6, Action: 0, Reward: 10.0, Total Reward: 20.849999999999998, Loss: 5.077618598937988\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 7, Action: 3, Reward: 0.25, Total Reward: 21.099999999999998, Loss: 6.457905292510986\n",
      "Optimizing model...\n",
      "Step: 8, Action: 2, Reward: 0.25, Total Reward: 21.349999999999998, Loss: 2.04465913772583\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 9, Action: 3, Reward: 0.25, Total Reward: 21.599999999999998, Loss: 9.179296493530273\n",
      "Model acting\n",
      "\n",
      "episode: 147, reward: -78.4\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 3, Reward: 0.25, Total Reward: 0.25, Loss: 5.159049034118652\n",
      "Optimizing model...\n",
      "Step: 1, Action: 3, Reward: 0.25, Total Reward: 0.5, Loss: 3.653099536895752\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 4, Reward: 0.2, Total Reward: 0.7, Loss: 7.933475494384766\n",
      "Optimizing model...\n",
      "Step: 3, Action: 3, Reward: 0.25, Total Reward: 0.95, Loss: 5.955349922180176\n",
      "Optimizing model...\n",
      "Step: 4, Action: 1, Reward: -5.0, Total Reward: -4.05, Loss: 8.023101806640625\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 5, Action: 0, Reward: 10.0, Total Reward: 5.95, Loss: 3.993051528930664\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 6, Action: 0, Reward: 10.0, Total Reward: 15.95, Loss: 6.897861957550049\n",
      "Optimizing model...\n",
      "Step: 7, Action: 2, Reward: 0.25, Total Reward: 16.2, Loss: 3.8645389080047607\n",
      "Optimizing model...\n",
      "Step: 8, Action: 0, Reward: 10.0, Total Reward: 26.2, Loss: 8.363763809204102\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 9, Action: 0, Reward: 10.0, Total Reward: 36.2, Loss: 4.966237545013428\n",
      "Model acting\n",
      "\n",
      "episode: 148, reward: -63.8\n",
      "Optimizing model...\n",
      "Step: 0, Action: 3, Reward: 0.25, Total Reward: 0.25, Loss: 3.973569631576538\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: 10.0, Total Reward: 10.25, Loss: 3.5058374404907227\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: 10.0, Total Reward: 20.25, Loss: 4.937935829162598\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 4, Reward: 0.2, Total Reward: 20.45, Loss: 8.445722579956055\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: 10.0, Total Reward: 30.45, Loss: 6.553101539611816\n",
      "Optimizing model...\n",
      "Step: 5, Action: 3, Reward: 0.25, Total Reward: 30.7, Loss: 6.002172470092773\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 6, Action: 4, Reward: 0.2, Total Reward: 30.9, Loss: 5.3277201652526855\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 7, Action: 0, Reward: 10.0, Total Reward: 40.9, Loss: 4.218310356140137\n",
      "Model acting\n",
      "\n",
      "episode: 149, reward: -59.1\n",
      "Optimizing model...\n",
      "Step: 0, Action: 2, Reward: 0.25, Total Reward: 0.25, Loss: 5.584064483642578\n",
      "Optimizing model...\n",
      "Step: 1, Action: 4, Reward: 0.2, Total Reward: 0.45, Loss: 8.082776069641113\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: 10.0, Total Reward: 10.45, Loss: 5.44542121887207\n",
      "Optimizing model...\n",
      "Step: 3, Action: 4, Reward: 0.2, Total Reward: 10.649999999999999, Loss: 6.641928672790527\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: 10.0, Total Reward: 20.65, Loss: 7.865900993347168\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 5, Action: 0, Reward: 10.0, Total Reward: 30.65, Loss: 6.4452924728393555\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 6, Action: 0, Reward: 10.0, Total Reward: 40.65, Loss: 11.29373550415039\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 7, Action: 0, Reward: 10.0, Total Reward: 50.65, Loss: 3.09885835647583\n",
      "Model acting\n",
      "\n",
      "episode: 150, reward: -49.35\n",
      "Model saved\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: 10.0, Total Reward: 10.0, Loss: 5.432298183441162\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: 10.0, Total Reward: 20.0, Loss: 3.8504931926727295\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: 0.0, Total Reward: 20.0, Loss: 6.852888107299805\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: 10.0, Total Reward: 30.0, Loss: 5.2393035888671875\n",
      "Optimizing model...\n",
      "Step: 4, Action: 2, Reward: 0.25, Total Reward: 30.25, Loss: 3.5449562072753906\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 5, Action: 0, Reward: 10.0, Total Reward: 40.25, Loss: 4.09934663772583\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 6, Action: 0, Reward: 10.0, Total Reward: 50.25, Loss: 8.225764274597168\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 7, Action: 0, Reward: 0.0, Total Reward: 50.25, Loss: 4.107819557189941\n",
      "Optimizing model...\n",
      "Step: 8, Action: 3, Reward: 0.25, Total Reward: 50.5, Loss: 13.666023254394531\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 9, Action: 0, Reward: 10.0, Total Reward: 60.5, Loss: 4.549417495727539\n",
      "Model acting\n",
      "\n",
      "episode: 151, reward: -39.5\n",
      "Optimizing model...\n",
      "Step: 0, Action: 4, Reward: 0.2, Total Reward: 0.2, Loss: 1.9414421319961548\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: 10.0, Total Reward: 10.2, Loss: 4.45893669128418\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: 10.0, Total Reward: 20.2, Loss: 6.716733932495117\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: 10.0, Total Reward: 30.2, Loss: 4.111349105834961\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: 0.0, Total Reward: 30.2, Loss: 5.368747711181641\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 5, Action: 0, Reward: 10.0, Total Reward: 40.2, Loss: 3.419175863265991\n",
      "Optimizing model...\n",
      "Step: 6, Action: 4, Reward: 0.2, Total Reward: 40.400000000000006, Loss: 6.402429580688477\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 7, Action: 0, Reward: 10.0, Total Reward: 50.400000000000006, Loss: 3.856299877166748\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 8, Action: 0, Reward: 10.0, Total Reward: 60.400000000000006, Loss: 7.119483947753906\n",
      "Optimizing model...\n",
      "Step: 9, Action: 4, Reward: 0.2, Total Reward: 60.60000000000001, Loss: 2.47168231010437\n",
      "Optimizing model...\n",
      "Step: 10, Action: 3, Reward: 0.25, Total Reward: 60.85000000000001, Loss: 6.2950439453125\n",
      "Optimizing model...\n",
      "Step: 11, Action: 2, Reward: 0.25, Total Reward: 61.10000000000001, Loss: 5.867132186889648\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 12, Action: 0, Reward: 10.0, Total Reward: 71.10000000000001, Loss: 4.142481803894043\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 13, Action: 0, Reward: 0.0, Total Reward: 71.10000000000001, Loss: 7.970477104187012\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 14, Action: 0, Reward: 0.0, Total Reward: 71.10000000000001, Loss: 7.2184929847717285\n",
      "\n",
      "episode: 152, reward: -28.89999999999999\n",
      "Optimizing model...\n",
      "Step: 0, Action: 4, Reward: 0.2, Total Reward: 0.2, Loss: 4.776475429534912\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: 10.0, Total Reward: 10.2, Loss: 2.4819557666778564\n",
      "Optimizing model...\n",
      "Step: 2, Action: 4, Reward: 0.2, Total Reward: 10.399999999999999, Loss: 8.223295211791992\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: 10.0, Total Reward: 20.4, Loss: 8.748046875\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: 10.0, Total Reward: 30.4, Loss: 4.457508087158203\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 5, Action: 0, Reward: 10.0, Total Reward: 40.4, Loss: 2.360351800918579\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 6, Action: 0, Reward: 0.0, Total Reward: 40.4, Loss: 4.2562642097473145\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 7, Action: 0, Reward: 0.0, Total Reward: 40.4, Loss: 4.107456207275391\n",
      "Optimizing model...\n",
      "Step: 8, Action: 2, Reward: 0.25, Total Reward: 40.65, Loss: 5.095977783203125\n",
      "Optimizing model...\n",
      "Step: 9, Action: 2, Reward: 0.25, Total Reward: 40.9, Loss: 8.85422420501709\n",
      "Optimizing model...\n",
      "Step: 10, Action: 3, Reward: 0.25, Total Reward: 41.15, Loss: 4.732161521911621\n",
      "Optimizing model...\n",
      "Step: 11, Action: 0, Reward: 10.0, Total Reward: 51.15, Loss: 8.71860408782959\n",
      "Optimizing model...\n",
      "Step: 12, Action: 0, Reward: 10.0, Total Reward: 61.15, Loss: 4.638566017150879\n",
      "Optimizing model...\n",
      "Step: 13, Action: 2, Reward: 0.25, Total Reward: 61.4, Loss: 6.7222466468811035\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 14, Action: 0, Reward: 10.0, Total Reward: 71.4, Loss: 2.194347620010376\n",
      "Optimizing model...\n",
      "Step: 15, Action: 0, Reward: 0.0, Total Reward: 71.4, Loss: 3.7391791343688965\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 16, Action: 0, Reward: 10.0, Total Reward: 81.4, Loss: 6.415813446044922\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 17, Action: 0, Reward: 10.0, Total Reward: 91.4, Loss: 8.478666305541992\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 18, Action: 0, Reward: 10.0, Total Reward: 101.4, Loss: 6.340922832489014\n",
      "Optimizing model...\n",
      "Step: 19, Action: 1, Reward: -5.0, Total Reward: 96.4, Loss: 6.327187538146973\n",
      "Optimizing model...\n",
      "Step: 20, Action: 4, Reward: 0.2, Total Reward: 96.60000000000001, Loss: 5.84780740737915\n",
      "Model acting\n",
      "\n",
      "episode: 153, reward: -3.3999999999999915\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: 10.0, Total Reward: 10.0, Loss: 6.851185321807861\n",
      "Optimizing model...\n",
      "Step: 1, Action: 2, Reward: 0.25, Total Reward: 10.25, Loss: 5.723764419555664\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: 10.0, Total Reward: 20.25, Loss: 4.09855318069458\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: 0.0, Total Reward: 20.25, Loss: 4.008350849151611\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: 10.0, Total Reward: 30.25, Loss: 4.0318803787231445\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 5, Action: 0, Reward: 0.0, Total Reward: 30.25, Loss: 4.1731085777282715\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 6, Action: 0, Reward: 0.0, Total Reward: 30.25, Loss: 5.34575080871582\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 7, Action: 0, Reward: 10.0, Total Reward: 40.25, Loss: 2.747312307357788\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 8, Action: 0, Reward: 10.0, Total Reward: 50.25, Loss: 2.4726662635803223\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 9, Action: 0, Reward: 10.0, Total Reward: 60.25, Loss: 5.072911262512207\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 10, Action: 0, Reward: 0.0, Total Reward: 60.25, Loss: 3.797179698944092\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 11, Action: 0, Reward: 0.0, Total Reward: 60.25, Loss: 4.040422439575195\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 12, Action: 0, Reward: 0.0, Total Reward: 60.25, Loss: 4.9429931640625\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 13, Action: 0, Reward: 0.0, Total Reward: 60.25, Loss: 3.6892752647399902\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 14, Action: 0, Reward: 0.0, Total Reward: 60.25, Loss: 6.326523780822754\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 15, Action: 0, Reward: 0.0, Total Reward: 60.25, Loss: 6.461509704589844\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 16, Action: 0, Reward: 10.0, Total Reward: 70.25, Loss: 3.621232748031616\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 17, Action: 0, Reward: 0.0, Total Reward: 70.25, Loss: 7.508993148803711\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 18, Action: 0, Reward: 10.0, Total Reward: 80.25, Loss: 5.047466278076172\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 19, Action: 0, Reward: 0.0, Total Reward: 80.25, Loss: 5.951951026916504\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 20, Action: 0, Reward: 0.0, Total Reward: 80.25, Loss: 4.029094696044922\n",
      "Model acting\n",
      "\n",
      "episode: 154, reward: -19.75\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: 10.0, Total Reward: 10.0, Loss: 11.02908992767334\n",
      "Optimizing model...\n",
      "Step: 1, Action: 4, Reward: 0.2, Total Reward: 10.2, Loss: 2.2484195232391357\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: 10.0, Total Reward: 20.2, Loss: 5.656806945800781\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: 10.0, Total Reward: 30.2, Loss: 7.647963523864746\n",
      "Optimizing model...\n",
      "Step: 4, Action: 1, Reward: -5.0, Total Reward: 25.2, Loss: 1.8801696300506592\n",
      "Model acting\n",
      "\n",
      "episode: 155, reward: -74.8\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: 10.0, Total Reward: 10.0, Loss: 7.109917640686035\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: 10.0, Total Reward: 20.0, Loss: 5.363886833190918\n",
      "Optimizing model...\n",
      "Step: 2, Action: 1, Reward: -5.0, Total Reward: 15.0, Loss: 7.320317268371582\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: 10.0, Total Reward: 25.0, Loss: 6.625792503356934\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: 0.0, Total Reward: 25.0, Loss: 5.414905071258545\n",
      "Optimizing model...\n",
      "Step: 5, Action: 1, Reward: -5.0, Total Reward: 20.0, Loss: 3.3803369998931885\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 6, Action: 0, Reward: 10.0, Total Reward: 30.0, Loss: 3.7000961303710938\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 7, Action: 0, Reward: 10.0, Total Reward: 40.0, Loss: 5.752346992492676\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 8, Action: 0, Reward: 10.0, Total Reward: 50.0, Loss: 6.848700046539307\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 9, Action: 0, Reward: 0.0, Total Reward: 50.0, Loss: 3.8343582153320312\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 10, Action: 0, Reward: 10.0, Total Reward: 60.0, Loss: 8.073111534118652\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 11, Action: 0, Reward: 10.0, Total Reward: 70.0, Loss: 3.3442978858947754\n",
      "Model acting\n",
      "\n",
      "episode: 156, reward: -30.0\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: 10.0, Total Reward: 10.0, Loss: 2.607903480529785\n",
      "Optimizing model...\n",
      "Step: 1, Action: 1, Reward: -5.0, Total Reward: 5.0, Loss: 8.105030059814453\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: 10.0, Total Reward: 15.0, Loss: 4.350100517272949\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: 10.0, Total Reward: 25.0, Loss: 3.52295184135437\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: 10.0, Total Reward: 35.0, Loss: 6.036285400390625\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 5, Action: 0, Reward: 10.0, Total Reward: 45.0, Loss: 4.084214687347412\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 6, Action: 0, Reward: 10.0, Total Reward: 55.0, Loss: 5.3442063331604\n",
      "Optimizing model...\n",
      "Step: 7, Action: 2, Reward: 0.25, Total Reward: 55.25, Loss: 5.194941520690918\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 8, Action: 0, Reward: 10.0, Total Reward: 65.25, Loss: 9.485076904296875\n",
      "Optimizing model...\n",
      "Step: 9, Action: 4, Reward: 0.2, Total Reward: 65.45, Loss: 2.490816116333008\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 10, Action: 0, Reward: 10.0, Total Reward: 75.45, Loss: 3.218214988708496\n",
      "Model acting\n",
      "\n",
      "episode: 157, reward: -24.549999999999997\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: 10.0, Total Reward: 10.0, Loss: 4.157331466674805\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: 10.0, Total Reward: 20.0, Loss: 5.708477020263672\n",
      "Optimizing model...\n",
      "Step: 2, Action: 2, Reward: 0.25, Total Reward: 20.25, Loss: 5.0693817138671875\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: 10.0, Total Reward: 30.25, Loss: 5.214631080627441\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: 0.0, Total Reward: 30.25, Loss: 5.591741561889648\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 5, Action: 0, Reward: 10.0, Total Reward: 40.25, Loss: 5.241611957550049\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 6, Action: 0, Reward: 0.0, Total Reward: 40.25, Loss: 8.203575134277344\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 7, Action: 0, Reward: 0.0, Total Reward: 40.25, Loss: 7.0948028564453125\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 8, Action: 0, Reward: 10.0, Total Reward: 50.25, Loss: 8.675460815429688\n",
      "Model acting\n",
      "\n",
      "episode: 158, reward: -49.75\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: 10.0, Total Reward: 10.0, Loss: 5.417424201965332\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: 10.0, Total Reward: 20.0, Loss: 6.246222496032715\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: 0.0, Total Reward: 20.0, Loss: 2.2231411933898926\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: 10.0, Total Reward: 30.0, Loss: 5.425713062286377\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: 10.0, Total Reward: 40.0, Loss: 7.307196617126465\n",
      "Optimizing model...\n",
      "Step: 5, Action: 3, Reward: 0.25, Total Reward: 40.25, Loss: 8.807353973388672\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 6, Action: 0, Reward: 10.0, Total Reward: 50.25, Loss: 4.890323638916016\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 7, Action: 0, Reward: 10.0, Total Reward: 60.25, Loss: 4.875138282775879\n",
      "Model acting\n",
      "\n",
      "episode: 159, reward: -39.75\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 280\u001b[0m\n\u001b[0;32m    277\u001b[0m     agent\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdqn.pth\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    278\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel saved\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 280\u001b[0m \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m3.25\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    281\u001b[0m keyboard\u001b[38;5;241m.\u001b[39mpress_and_release(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspace\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# run DQN agent on Crossy Road game to train and play the game\n",
    "import pyautogui\n",
    "import cv2\n",
    "import time\n",
    "import keyboard\n",
    "import torchvision\n",
    "from ultralytics import YOLO\n",
    "\n",
    "\n",
    "RES_X = 1920\n",
    "RES_Y = 1080\n",
    "\n",
    "GAME_REGION = (405, 210, 850, 480)\n",
    "restart_button = cv2.imread('restart_button.png', cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "\n",
    "def get_screen(region):\n",
    "    screen = pyautogui.screenshot(region=(region[0], region[1], region[2], region[3]))\n",
    "    \n",
    "    non_crop = screen.copy()\n",
    "\n",
    "    transforms = torchvision.transforms.Compose([\n",
    "        torchvision.transforms.RandomRotation((14, 14)),\n",
    "        torchvision.transforms.CenterCrop((320, 566)),\n",
    "        torchvision.transforms.Resize((240, 425)),\n",
    "    ])\n",
    "    \n",
    "    screen = transforms(screen)   \n",
    "    \n",
    "    screen = cv2.cvtColor(np.array(screen), cv2.COLOR_RGB2BGR)\n",
    "    \n",
    "    non_crop = cv2.cvtColor(np.array(non_crop), cv2.COLOR_RGB2BGR)\n",
    "    non_crop = cv2.resize(non_crop, (425, 240))\n",
    "    \n",
    "    return screen, non_crop\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def map_to_grid(image_size, grid_size, boxes, class_labels):\n",
    "    \"\"\"\n",
    "    Map detected bounding boxes to a grid representation.\n",
    "\n",
    "    Args:\n",
    "        image_size: Tuple (width, height) of the image.\n",
    "        grid_size: Tuple (N, M) of the grid dimensions.\n",
    "        boxes: List of bounding boxes [(x_min, y_min, x_max, y_max)].\n",
    "        class_labels: List of class labels corresponding to the boxes.\n",
    "\n",
    "    Returns:\n",
    "        grid: 2D numpy array of shape (N, M) with object class labels.\n",
    "    \"\"\"\n",
    "    width, height = image_size\n",
    "    grid_width, grid_height = grid_size\n",
    "    grid = np.zeros((grid_height, grid_width), dtype=int)\n",
    "\n",
    "    cell_width = width / grid_width\n",
    "    cell_height = height / grid_height\n",
    "\n",
    "    for (x_min, y_min, x_max, y_max), label in zip(boxes, class_labels):\n",
    "        x_start = int(x_min // cell_width)\n",
    "        y_start = int(y_min // cell_height)\n",
    "        x_end = int(np.ceil(x_max / cell_width))\n",
    "        y_end = int(np.ceil(y_max / cell_height))\n",
    "\n",
    "        for y in range(y_start, y_end):\n",
    "            for x in range(x_start, x_end):\n",
    "                grid[y, x] = label + 1\n",
    "\n",
    "    return grid\n",
    "\n",
    "\n",
    "def get_state(screen):\n",
    "    results = cv_model(screen, verbose=False)\n",
    "    \n",
    "    # save the image with bounding boxes\n",
    "    # results[0].save(\"dataset/screen_detections.png\")\n",
    "\n",
    "    image_size = (425, 240)  # Example image dimensions (width, height)\n",
    "    grid_size = (36, 32)    # Example grid dimensions (N, M)\n",
    "\n",
    "    boxes = []\n",
    "    labels = []\n",
    "    \n",
    "    boxes_ = results[0].boxes\n",
    "    for box in boxes_:\n",
    "        x_min, y_min, x_max, y_max = box.xyxy[0].tolist()\n",
    "        \n",
    "        class_id = int(box.cls[0].item())\n",
    "        \n",
    "        boxes.append((x_min, y_min, x_max, y_max))\n",
    "        labels.append(class_id)  # Assuming class_id is the label\n",
    "\n",
    "    grid = map_to_grid(image_size, grid_size, boxes, labels)\n",
    "    # print(grid)\n",
    "    return grid\n",
    "\n",
    "def is_game_over(image, score_threshold=0.5, scale=0.5):\n",
    "    grey_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    resized_template = cv2.resize(restart_button, (0, 0), fx=scale, fy=scale)\n",
    "    h, w = grey_image.shape\n",
    "\n",
    "    cropped_search_box = grey_image[int(h * 0.87):, int(w * 0.43):int(w * 0.57)]\n",
    "    \n",
    "    # cv2.imwrite('cropped_search_box.png', cropped_search_box)\n",
    "    \n",
    "    result = cv2.matchTemplate(cropped_search_box, resized_template, cv2.TM_CCOEFF_NORMED)\n",
    "    result = np.sort(result.flatten())[::-1]\n",
    "    \n",
    "    return result.max() > score_threshold\n",
    "\n",
    "def process_state(state, max_obstacles=1152, max_timbers=1152):\n",
    "    agent_pos, obstacles, timbers = state[0], state[1], state[2]\n",
    "    \n",
    "    # Agent position\n",
    "    # print(agent_pos)\n",
    "    state_vector = list(agent_pos)\n",
    "    \n",
    "    # Obstacles\n",
    "    obstacles = list(obstacles)\n",
    "    for i in range(max_obstacles):\n",
    "        if i < len(obstacles):\n",
    "            state_vector.extend(obstacles[i])  # Assuming obstacle positions are tuples\n",
    "        else:\n",
    "            state_vector.extend([0, 0])  # Padding\n",
    "   \n",
    "    # Timbers\n",
    "    timbers = list(timbers)\n",
    "    for i in range(max_timbers):\n",
    "        if i < len(timbers):\n",
    "            state_vector.extend(timbers[i])  # Assuming timber positions are tuples\n",
    "        else:\n",
    "            state_vector.extend([0, 0])  # Padding\n",
    "    \n",
    "    return state_vector\n",
    "\n",
    "def compute_reward(reward_state):\n",
    "    # Compute reward based on the change in the game screen   \n",
    "    time = reward_state['time']\n",
    "    state = reward_state['state']\n",
    "    action = reward_state['action']\n",
    "    prev_action = reward_state['prev_action']\n",
    "    next_state = reward_state['next_state']\n",
    "    total_reward = reward_state['total_reward']\n",
    "    non_crop_state = reward_state['non_crop_state']\n",
    "    reward = 0\n",
    "    \n",
    "    if is_game_over(non_crop_state):\n",
    "        reward = -100\n",
    "        return reward\n",
    "    \n",
    "    if action == 0:\n",
    "        reward += 10\n",
    "    elif action == 1:\n",
    "        reward -= 5\n",
    "    elif action == 2:\n",
    "        reward += 0.25\n",
    "    elif action == 3:\n",
    "        reward += 0.25\n",
    "    elif action == 4:\n",
    "        reward += 0.2\n",
    "\n",
    "    # if action == prev_action:\n",
    "    #     reward += 0\n",
    "    \n",
    "    hen_count = 0\n",
    "    for row in next_state:\n",
    "        for cell in row:\n",
    "            if cell == 1:\n",
    "                hen_count += 1\n",
    "                \n",
    "    reward += (hen_count * 0.25)\n",
    "    \n",
    "\n",
    "    if action == prev_action:\n",
    "        if state[0][0] // 5 == next_state[0][0] // 5 \\\n",
    "        and state[0][1] // 5 == next_state[0][1] // 5:\n",
    "            reward -= 10\n",
    "            \n",
    "    reward = round(reward, 2)\n",
    "    \n",
    "    return reward\n",
    "    \n",
    "\n",
    "# train DQN agent\n",
    "actions = [0, 1, 2, 3, 4]\n",
    "\n",
    "input_size = 4610\n",
    "agent = IQNAgent(input_size, 5, 128, 10000, 32, 0.99, 1.0, 0.1, 1000)\n",
    "cv_model = YOLO('best_cv.pt')\n",
    "episodes = 1000\n",
    "episode_length = 1000\n",
    "losses = []\n",
    "rewards = []\n",
    "\n",
    "# agent.load('dqn.pth')\n",
    "\n",
    "print(\"Model is ready to train\")\n",
    "\n",
    "keyboard.wait('q')\n",
    "\n",
    "# Loop for self-play training\n",
    "for episode in range(episodes):\n",
    "    screenshot, non_crop_state = get_screen(GAME_REGION)\n",
    "    state_raw = get_state(screenshot)\n",
    "    \n",
    "    total_reward = 0\n",
    "    total_loss = 0\n",
    "    action = 0\n",
    "\n",
    "    state = simplify_state(state_raw)\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "    \n",
    "    \n",
    "    start_time = time.time()\n",
    "    for step in range(episode_length):\n",
    "        prev_action = action\n",
    "        time.sleep(0.1)\n",
    "        \n",
    "        state_vector = process_state(state)\n",
    "        \n",
    "        action = agent.select_action(state_vector)\n",
    "        \n",
    "        if action < 4:\n",
    "            pyautogui.press(['up', 'down', 'left', 'right'][action])\n",
    "        \n",
    "        \n",
    "        next_screenshot, non_crop_state = get_screen(GAME_REGION)\n",
    "        next_state_raw = get_state(next_screenshot)\n",
    "        \n",
    "        with open('state.txt', 'w') as f:\n",
    "            for row in next_state_raw:\n",
    "                f.write(' '.join(map(str, row)) + '\\n')\n",
    "                \n",
    "        \n",
    "        # cv2.imwrite(f'state_{step}.png', state)\n",
    "        \n",
    "        next_state = simplify_state(next_state_raw)\n",
    "        next_state_vector = process_state(next_state)\n",
    "        \n",
    "        reward_state = {\n",
    "            'state': state,\n",
    "            'action': action,\n",
    "            'prev_action': prev_action,\n",
    "            'next_state': next_state,\n",
    "            'time': time.time() - start_time,\n",
    "            'total_reward': total_reward,\n",
    "            'non_crop_state': non_crop_state\n",
    "        }\n",
    "        \n",
    "        done = 0\n",
    "        reward = compute_reward(reward_state)\n",
    "\n",
    "        agent.push(state_vector, action, reward, next_state_vector, done)\n",
    "\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "        \n",
    "        if is_game_over(non_crop_state):\n",
    "            done = 1\n",
    "            break\n",
    "        \n",
    "        loss = agent.optimize_model()\n",
    "        # else:\n",
    "        #     print(\"False\")\n",
    "        \n",
    "        print(f\"Step: {step}, Action: {action}, Reward: {reward}, Total Reward: {total_reward}, Loss: {loss}\")\n",
    " \n",
    "    # tap space key to restart the game\n",
    "    keyboard.press_and_release('space')\n",
    "    losses.append(total_loss)\n",
    "    rewards.append(total_reward)\n",
    "    print('\\nepisode: {}, reward: {}'.format(episode, total_reward))\n",
    "    \n",
    "    # Save model\n",
    "    if episode % 10 == 0:\n",
    "        agent.save('dqn.pth')\n",
    "        print(\"Model saved\")\n",
    "    \n",
    "    time.sleep(3.25)\n",
    "    keyboard.press_and_release('space')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
