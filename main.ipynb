{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DQN model for Crossy Road game\n",
    "# input is the 2d matrix of the game state (90 x 90)\n",
    "# output is the action to take (0, 1, 2, 3) for (up, down, left, right)\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_size):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size * 160, hidden_size * 16)\n",
    "        self.fc2 = nn.Linear(hidden_size * 16, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, hidden_size // 4)\n",
    "        self.fc4 = nn.Linear(hidden_size // 4, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        if len(x.shape) > 2:\n",
    "            batch_size = x.shape[0]\n",
    "            x = x.view(batch_size, -1)\n",
    "        else:\n",
    "            x = torch.flatten(x)\n",
    "        \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        \n",
    "        return x\n",
    "        \n",
    "    \n",
    "    def act(self, state, epsilon):\n",
    "        # print(epsilon)\n",
    "        if random.random() < 0.975:\n",
    "            state = torch.tensor(state, dtype=torch.float32)\n",
    "            \n",
    "            q_value = self.forward(state)\n",
    "            action = torch.argmax(input=q_value).item()\n",
    "            \n",
    "        else:\n",
    "            action = random.choice([0, 1, 2, 3, 4])\n",
    "        return action\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replay buffer for DQN\n",
    "# stores the transitions (state, action, reward, next_state, done)\n",
    "# and samples a batch of transitions for training\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.buffer = []\n",
    "        self.position = 0\n",
    "        \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        if len(self.buffer) < self.capacity:\n",
    "            self.buffer.append(None)\n",
    "        self.buffer[self.position] = (state, action, reward, next_state, done)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "        \n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        state, action, reward, next_state, done = map(np.stack, zip(*batch))\n",
    "        return state, action, reward, next_state, done\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "    \n",
    "    \n",
    "# DQN agent for Crossy Road game\n",
    "# uses DQN model and replay buffer for training\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self, input_size, output_size, hidden_size, replay_buffer_capacity, batch_size, gamma, epsilon_start, epsilon_end, epsilon_decay):\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.replay_buffer_capacity = replay_buffer_capacity\n",
    "        self.batch_size = batch_size\n",
    "        self.gamma = gamma\n",
    "        self.epsilon_start = epsilon_start\n",
    "        self.epsilon_end = epsilon_end\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        \n",
    "        self.model = DQN(input_size, output_size, hidden_size)\n",
    "        self.replay_buffer = ReplayBuffer(replay_buffer_capacity)\n",
    "        self.optimizer = optim.Adam(self.model.parameters())\n",
    "        \n",
    "        self.steps_done = 0\n",
    "        \n",
    "    def select_action(self, state):\n",
    "        epsilon = self.epsilon_end + (self.epsilon_start - self.epsilon_end) * np.exp(-1. * self.steps_done / self.epsilon_decay)\n",
    "        self.steps_done += 1\n",
    "        return self.model.act(state, epsilon)\n",
    "    \n",
    "    def optimize_model(self):\n",
    "        if len(self.replay_buffer) < self.batch_size:\n",
    "            return\n",
    "        \n",
    "        states, action, reward, next_states, done = self.replay_buffer.sample(self.batch_size)\n",
    "        \n",
    "        states = torch.FloatTensor(np.float32(states))\n",
    "        action = torch.LongTensor(action)\n",
    "        reward = torch.FloatTensor(reward)\n",
    "        next_states = torch.FloatTensor(np.float32(next_states))\n",
    "        done = torch.FloatTensor(done)\n",
    "\n",
    "        q_values = self.model(states)\n",
    "        next_q_values = self.model(next_states)\n",
    "        \n",
    "        q_value = q_values.gather(1, action.unsqueeze(1)).squeeze(1)\n",
    "        \n",
    "        \n",
    "        next_q_value = next_q_values.max(1)[0]  \n",
    "        expected_q_value = reward + self.gamma *  next_q_value\n",
    "        \n",
    "        # print(q_value, expected_q_value)\n",
    "        \n",
    "        loss = F.mse_loss(q_value, expected_q_value)\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        return loss.item()\n",
    "    \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        self.replay_buffer.push(state, action, reward, next_state, done)\n",
    "        \n",
    "    def save(self, path):\n",
    "        torch.save(self.model.state_dict(), path)\n",
    "        \n",
    "    def load(self, path):\n",
    "        self.model.load_state_dict(torch.load(path))\n",
    "        self.model.eval()\n",
    "        \n",
    "    def reset(self):\n",
    "        self.steps_done = 0\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import random\n",
    "\n",
    "class QLearningAgent:\n",
    "    def __init__(self, actions, learning_rate, gamma, epsilon):\n",
    "        self.actions = actions\n",
    "        self.lr = learning_rate\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.q_table = collections.defaultdict(float)\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        if random.random() < self.epsilon:\n",
    "            # Explore: random action\n",
    "            return random.choice(self.actions)\n",
    "        else:\n",
    "            # Exploit: choose the best action from Q-table\n",
    "            q_values = [self.q_table[(state, a)] for a in self.actions]\n",
    "            max_q = max(q_values)\n",
    "            # Handle multiple actions with the same max Q-value\n",
    "            best_actions = [a for a, q in zip(self.actions, q_values) if q == max_q]\n",
    "            return random.choice(best_actions)\n",
    "\n",
    "    def learn(self, state, action, reward, next_state, done):\n",
    "        current_q = self.q_table[(state, action)]\n",
    "        max_next_q = max([self.q_table[(next_state, a)] for a in self.actions]) if not done else 0\n",
    "        target_q = reward + self.gamma * max_next_q\n",
    "        # Update Q-value using the learning rate\n",
    "        self.q_table[(state, action)] += self.lr * (target_q - current_q)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simplify_state(state):\n",
    "    # Extract agent's position\n",
    "    agent_x, agent_y = get_agent_position(state)\n",
    "    # Extract positions of nearby obstacles\n",
    "    obstacles = get_nearby_obstacles(state, agent_x, agent_y)\n",
    "    # Extract positions of nearby timbers\n",
    "    timbers = get_nearby_timbers(state, agent_x, agent_y)\n",
    "    \n",
    "    # Discretize positions\n",
    "    # agent_pos = (agent_x // 10, agent_y // 10)\n",
    "    # obstacle_features = []\n",
    "    # for obs in obstacles:\n",
    "    #     obs_x, obs_y = obs\n",
    "    #     rel_x = (obs_x - agent_x) // 10\n",
    "    #     rel_y = (obs_y - agent_y) // 10\n",
    "    #     obstacle_features.append((rel_x, rel_y))\n",
    "    # Construct simplified state\n",
    "    simplified_state = ((agent_x, agent_y), tuple(set(obstacles)), tuple(set(timbers)))\n",
    "    return simplified_state\n",
    "\n",
    "def get_agent_position(state):\n",
    "    for i, row in enumerate(state):\n",
    "        for j, cell in enumerate(row):\n",
    "            if cell == 1:\n",
    "                return j, i\n",
    "    return state.shape[1] // 2, state.shape[0] - 1\n",
    "    \n",
    "            \n",
    "def get_nearby_obstacles(state, agent_x, agent_y):\n",
    "    obstacles = []\n",
    "    for i, row in enumerate(state):\n",
    "        for j, cell in enumerate(row):\n",
    "            if cell == 2:\n",
    "                obstacles.append((j, i))\n",
    "    return obstacles\n",
    "\n",
    "def get_nearby_timbers(state, agent_x, agent_y):\n",
    "    timbers = []\n",
    "    for i, row in enumerate(state):\n",
    "        for j, cell in enumerate(row):\n",
    "            if cell == 3:\n",
    "                timbers.append((j, i))\n",
    "    return timbers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model is ready to train\n",
      "Step: 0, Action: 2, Reward: -0.25, Total Reward: -0.25, Hen Position: (77, 110)\n",
      "Step: 1, Action: 1, Reward: -2, Total Reward: -2.25, Hen Position: (73, 127)\n",
      "Step: 2, Action: 0, Reward: -7, Total Reward: -9.25, Hen Position: (77, 122)\n",
      "Step: 3, Action: 1, Reward: -2, Total Reward: -11.25, Hen Position: (79, 133)\n",
      "Step: 4, Action: 2, Reward: -0.25, Total Reward: -11.5, Hen Position: (65, 147)\n",
      "Step: 5, Action: 1, Reward: -2, Total Reward: -13.5, Hen Position: (90, 159)\n",
      "Step: 6, Action: 4, Reward: -0.5, Total Reward: -14.0, Hen Position: (90, 159)\n",
      "Step: 7, Action: 3, Reward: -0.25, Total Reward: -14.25, Hen Position: (90, 159)\n",
      "Step: 8, Action: 2, Reward: -0.25, Total Reward: -14.5, Hen Position: (90, 159)\n",
      "Step: 9, Action: 0, Reward: -7, Total Reward: -21.5, Hen Position: (90, 159)\n",
      "Step: 10, Action: 1, Reward: -2, Total Reward: -23.5, Hen Position: (90, 159)\n",
      "Step: 11, Action: 4, Reward: -0.5, Total Reward: -24.0, Hen Position: (90, 159)\n",
      "Step: 12, Action: 2, Reward: -0.25, Total Reward: -24.25, Hen Position: (90, 100)\n",
      "Step: 13, Action: 3, Reward: -0.25, Total Reward: -24.5, Hen Position: (92, 94)\n",
      "Step: 14, Action: 2, Reward: -0.25, Total Reward: -24.75, Hen Position: (90, 159)\n",
      "Step: 15, Action: 1, Reward: -2, Total Reward: -26.75, Hen Position: (90, 159)\n",
      "Step: 16, Action: 0, Reward: -7, Total Reward: -33.75, Hen Position: (90, 159)\n",
      "Step: 17, Action: 3, Reward: -0.25, Total Reward: -34.0, Hen Position: (90, 159)\n",
      "\n",
      "episode: 0, loss: 0, reward: -134.0\n",
      "Step: 0, Action: 1, Reward: -2, Total Reward: -2, Hen Position: (92, 109)\n",
      "Step: 1, Action: 2, Reward: -0.25, Total Reward: -2.25, Hen Position: (75, 127)\n",
      "Step: 2, Action: 4, Reward: -0.5, Total Reward: -2.75, Hen Position: (70, 135)\n",
      "Step: 3, Action: 3, Reward: -0.25, Total Reward: -3.0, Hen Position: (84, 140)\n",
      "Step: 4, Action: 4, Reward: -0.5, Total Reward: -3.5, Hen Position: (94, 147)\n",
      "Step: 5, Action: 3, Reward: -0.25, Total Reward: -3.75, Hen Position: (103, 151)\n",
      "Step: 6, Action: 2, Reward: -0.25, Total Reward: -4.0, Hen Position: (90, 159)\n",
      "Step: 7, Action: 4, Reward: -0.5, Total Reward: -4.5, Hen Position: (90, 159)\n",
      "Step: 8, Action: 2, Reward: -0.25, Total Reward: -4.75, Hen Position: (90, 159)\n",
      "Step: 9, Action: 3, Reward: -0.25, Total Reward: -5.0, Hen Position: (90, 159)\n",
      "Step: 10, Action: 0, Reward: -7, Total Reward: -12.0, Hen Position: (90, 159)\n",
      "Step: 11, Action: 2, Reward: -0.25, Total Reward: -12.25, Hen Position: (90, 159)\n",
      "Step: 12, Action: 4, Reward: -0.5, Total Reward: -12.75, Hen Position: (90, 159)\n",
      "Step: 13, Action: 0, Reward: -7, Total Reward: -19.75, Hen Position: (90, 159)\n",
      "Step: 14, Action: 2, Reward: -0.25, Total Reward: -20.0, Hen Position: (90, 159)\n",
      "Step: 15, Action: 4, Reward: -0.5, Total Reward: -20.5, Hen Position: (90, 159)\n",
      "Step: 16, Action: 2, Reward: -0.25, Total Reward: -20.75, Hen Position: (90, 159)\n",
      "Step: 17, Action: 3, Reward: -0.25, Total Reward: -21.0, Hen Position: (90, 159)\n",
      "Step: 18, Action: 3, Reward: -0.5, Total Reward: -21.5, Hen Position: (90, 159)\n",
      "Step: 19, Action: 0, Reward: -7, Total Reward: -28.5, Hen Position: (90, 159)\n",
      "Step: 20, Action: 3, Reward: -0.25, Total Reward: -28.75, Hen Position: (90, 159)\n",
      "Step: 21, Action: 1, Reward: -2, Total Reward: -30.75, Hen Position: (90, 159)\n",
      "Step: 22, Action: 2, Reward: -0.25, Total Reward: -31.0, Hen Position: (90, 159)\n",
      "\n",
      "episode: 1, loss: 0, reward: -131.0\n",
      "Step: 0, Action: 3, Reward: -0.25, Total Reward: -0.25, Hen Position: (104, 95)\n",
      "Step: 1, Action: 4, Reward: -0.5, Total Reward: -0.75, Hen Position: (109, 102)\n",
      "Step: 2, Action: 0, Reward: 3, Total Reward: 2.25, Hen Position: (108, 86)\n",
      "Step: 3, Action: 0, Reward: 2.75, Total Reward: 5.0, Hen Position: (107, 70)\n",
      "Step: 4, Action: 0, Reward: 2.75, Total Reward: 7.75, Hen Position: (90, 159)\n",
      "Step: 5, Action: 0, Reward: -7.25, Total Reward: 0.5, Hen Position: (90, 159)\n",
      "\n",
      "episode: 2, loss: 0, reward: -99.5\n",
      "Step: 0, Action: 4, Reward: -0.5, Total Reward: -0.5, Hen Position: (95, 93)\n",
      "Step: 1, Action: 3, Reward: -0.25, Total Reward: -0.75, Hen Position: (102, 103)\n",
      "Step: 2, Action: 0, Reward: 3, Total Reward: 2.25, Hen Position: (109, 92)\n",
      "Step: 3, Action: 0, Reward: 2.75, Total Reward: 5.0, Hen Position: (108, 76)\n",
      "Step: 4, Action: 0, Reward: 2.75, Total Reward: 7.75, Hen Position: (107, 90)\n",
      "Step: 5, Action: 0, Reward: -7.25, Total Reward: 0.5, Hen Position: (103, 99)\n",
      "Step: 6, Action: 1, Reward: -2, Total Reward: -1.5, Hen Position: (96, 113)\n",
      "Step: 7, Action: 4, Reward: -0.5, Total Reward: -2.0, Hen Position: (94, 131)\n",
      "Step: 8, Action: 1, Reward: -2, Total Reward: -4.0, Hen Position: (90, 159)\n",
      "Step: 9, Action: 0, Reward: 3, Total Reward: -1.0, Hen Position: (89, 152)\n",
      "Step: 10, Action: 0, Reward: 2.75, Total Reward: 1.75, Hen Position: (90, 134)\n",
      "Step: 11, Action: 0, Reward: 2.75, Total Reward: 4.5, Hen Position: (91, 144)\n",
      "Step: 12, Action: 0, Reward: -7.25, Total Reward: -2.75, Hen Position: (90, 143)\n",
      "Step: 13, Action: 4, Reward: -0.5, Total Reward: -3.25, Hen Position: (89, 148)\n",
      "Step: 14, Action: 1, Reward: -2, Total Reward: -5.25, Hen Position: (90, 159)\n",
      "Step: 15, Action: 0, Reward: -7, Total Reward: -12.25, Hen Position: (90, 159)\n",
      "Step: 16, Action: 1, Reward: -2, Total Reward: -14.25, Hen Position: (90, 159)\n",
      "Step: 17, Action: 0, Reward: -7, Total Reward: -21.25, Hen Position: (90, 159)\n",
      "Step: 18, Action: 4, Reward: -0.5, Total Reward: -21.75, Hen Position: (90, 159)\n",
      "Step: 19, Action: 0, Reward: -7, Total Reward: -28.75, Hen Position: (90, 159)\n",
      "Step: 20, Action: 2, Reward: -0.25, Total Reward: -29.0, Hen Position: (90, 159)\n",
      "Step: 21, Action: 1, Reward: -2, Total Reward: -31.0, Hen Position: (90, 159)\n",
      "Step: 22, Action: 4, Reward: -0.5, Total Reward: -31.5, Hen Position: (90, 94)\n",
      "Step: 23, Action: 3, Reward: -0.25, Total Reward: -31.75, Hen Position: (89, 94)\n",
      "Step: 24, Action: 4, Reward: -0.5, Total Reward: -32.25, Hen Position: (90, 159)\n",
      "Step: 25, Action: 1, Reward: -2, Total Reward: -34.25, Hen Position: (90, 159)\n",
      "Step: 26, Action: 2, Reward: -0.25, Total Reward: -34.5, Hen Position: (90, 159)\n",
      "\n",
      "episode: 3, loss: 0, reward: -134.5\n",
      "Step: 0, Action: 2, Reward: -0.25, Total Reward: -0.25, Hen Position: (79, 96)\n",
      "Step: 1, Action: 3, Reward: -0.25, Total Reward: -0.5, Hen Position: (87, 104)\n",
      "Step: 2, Action: 0, Reward: 3, Total Reward: 2.5, Hen Position: (97, 93)\n",
      "Step: 3, Action: 0, Reward: 2.75, Total Reward: 5.25, Hen Position: (99, 74)\n",
      "Step: 4, Action: 0, Reward: 2.75, Total Reward: 8.0, Hen Position: (101, 63)\n",
      "Step: 5, Action: 0, Reward: 2.75, Total Reward: 10.75, Hen Position: (101, 52)\n",
      "Step: 6, Action: 0, Reward: 2.75, Total Reward: 13.5, Hen Position: (102, 44)\n",
      "Step: 7, Action: 0, Reward: -7.25, Total Reward: 6.25, Hen Position: (102, 45)\n",
      "Step: 8, Action: 2, Reward: -0.25, Total Reward: 6.0, Hen Position: (102, 52)\n",
      "Step: 9, Action: 4, Reward: -0.5, Total Reward: 5.5, Hen Position: (95, 65)\n",
      "Step: 10, Action: 2, Reward: -0.25, Total Reward: 5.25, Hen Position: (85, 80)\n",
      "Step: 11, Action: 0, Reward: 3, Total Reward: 8.25, Hen Position: (76, 74)\n",
      "Step: 12, Action: 0, Reward: 2.75, Total Reward: 11.0, Hen Position: (79, 61)\n",
      "Step: 13, Action: 0, Reward: 2.75, Total Reward: 13.75, Hen Position: (86, 60)\n",
      "Step: 14, Action: 0, Reward: 2.75, Total Reward: 16.5, Hen Position: (90, 159)\n",
      "Step: 15, Action: 0, Reward: -7.25, Total Reward: 9.25, Hen Position: (90, 159)\n",
      "Step: 16, Action: 2, Reward: -0.25, Total Reward: 9.0, Hen Position: (90, 159)\n",
      "Step: 17, Action: 3, Reward: -0.25, Total Reward: 8.75, Hen Position: (90, 159)\n",
      "\n",
      "episode: 4, loss: 0, reward: -91.25\n",
      "Step: 0, Action: 3, Reward: -0.25, Total Reward: -0.25, Hen Position: (104, 94)\n",
      "Step: 1, Action: 1, Reward: -2, Total Reward: -2.25, Hen Position: (107, 117)\n",
      "Step: 2, Action: 4, Reward: -0.5, Total Reward: -2.75, Hen Position: (103, 133)\n",
      "Step: 3, Action: 1, Reward: -2, Total Reward: -4.75, Hen Position: (90, 159)\n",
      "Step: 4, Action: 2, Reward: -0.25, Total Reward: -5.0, Hen Position: (90, 159)\n",
      "Step: 5, Action: 2, Reward: -0.5, Total Reward: -5.5, Hen Position: (90, 159)\n",
      "Step: 6, Action: 3, Reward: -0.25, Total Reward: -5.75, Hen Position: (90, 159)\n",
      "Step: 7, Action: 1, Reward: -2, Total Reward: -7.75, Hen Position: (85, 150)\n",
      "Step: 8, Action: 0, Reward: 3, Total Reward: -4.75, Hen Position: (89, 94)\n",
      "Step: 9, Action: 0, Reward: -7.25, Total Reward: -12.0, Hen Position: (89, 94)\n",
      "Step: 10, Action: 2, Reward: -0.25, Total Reward: -12.25, Hen Position: (90, 159)\n",
      "Step: 11, Action: 3, Reward: -0.25, Total Reward: -12.5, Hen Position: (90, 159)\n",
      "Step: 12, Action: 1, Reward: -2, Total Reward: -14.5, Hen Position: (90, 159)\n",
      "\n",
      "episode: 5, loss: 0, reward: -114.5\n",
      "Step: 0, Action: 4, Reward: -0.5, Total Reward: -0.5, Hen Position: (95, 92)\n",
      "Step: 1, Action: 2, Reward: -0.25, Total Reward: -0.75, Hen Position: (80, 99)\n",
      "Step: 2, Action: 4, Reward: -0.5, Total Reward: -1.25, Hen Position: (74, 108)\n",
      "Step: 3, Action: 0, Reward: 3, Total Reward: 1.75, Hen Position: (80, 93)\n",
      "Step: 4, Action: 0, Reward: 2.75, Total Reward: 4.5, Hen Position: (99, 79)\n",
      "Step: 5, Action: 0, Reward: 2.75, Total Reward: 7.25, Hen Position: (97, 95)\n",
      "Step: 6, Action: 0, Reward: 2.75, Total Reward: 10.0, Hen Position: (104, 120)\n",
      "\n",
      "episode: 6, loss: 0, reward: -90.0\n",
      "Step: 0, Action: 1, Reward: -2, Total Reward: -2, Hen Position: (91, 112)\n",
      "Step: 1, Action: 2, Reward: -0.25, Total Reward: -2.25, Hen Position: (75, 128)\n",
      "Step: 2, Action: 4, Reward: -0.5, Total Reward: -2.75, Hen Position: (69, 136)\n",
      "Step: 3, Action: 1, Reward: -2, Total Reward: -4.75, Hen Position: (90, 159)\n",
      "Step: 4, Action: 0, Reward: 3, Total Reward: -1.75, Hen Position: (76, 152)\n",
      "Step: 5, Action: 0, Reward: 2.75, Total Reward: 1.0, Hen Position: (80, 134)\n",
      "Step: 6, Action: 0, Reward: 2.75, Total Reward: 3.75, Hen Position: (84, 113)\n",
      "Step: 7, Action: 0, Reward: 2.75, Total Reward: 6.5, Hen Position: (89, 96)\n",
      "Step: 8, Action: 0, Reward: 2.75, Total Reward: 9.25, Hen Position: (92, 77)\n",
      "Step: 9, Action: 0, Reward: 2.75, Total Reward: 12.0, Hen Position: (95, 69)\n",
      "Step: 10, Action: 0, Reward: 2.75, Total Reward: 14.75, Hen Position: (107, 86)\n",
      "Step: 11, Action: 0, Reward: 2.75, Total Reward: 17.5, Hen Position: (90, 159)\n",
      "\n",
      "episode: 7, loss: 0, reward: -82.5\n",
      "Step: 0, Action: 0, Reward: 2.75, Total Reward: 2.75, Hen Position: (95, 80)\n",
      "Step: 1, Action: 0, Reward: 2.75, Total Reward: 5.5, Hen Position: (98, 66)\n",
      "Step: 2, Action: 0, Reward: -7.25, Total Reward: -1.75, Hen Position: (99, 66)\n",
      "Step: 3, Action: 1, Reward: -2, Total Reward: -3.75, Hen Position: (96, 81)\n",
      "Step: 4, Action: 0, Reward: 3, Total Reward: -0.75, Hen Position: (95, 95)\n",
      "Step: 5, Action: 0, Reward: 2.75, Total Reward: 2.0, Hen Position: (90, 159)\n",
      "Step: 6, Action: 0, Reward: 2.75, Total Reward: 4.75, Hen Position: (102, 120)\n",
      "\n",
      "episode: 8, loss: 0, reward: -95.25\n",
      "Step: 0, Action: 3, Reward: -0.25, Total Reward: -0.25, Hen Position: (104, 92)\n",
      "Step: 1, Action: 1, Reward: -2, Total Reward: -2.25, Hen Position: (106, 116)\n",
      "Step: 2, Action: 2, Reward: -0.25, Total Reward: -2.5, Hen Position: (85, 133)\n",
      "Step: 3, Action: 0, Reward: 3, Total Reward: 0.5, Hen Position: (82, 123)\n",
      "Step: 4, Action: 0, Reward: 2.75, Total Reward: 3.25, Hen Position: (87, 104)\n",
      "Step: 5, Action: 0, Reward: 2.75, Total Reward: 6.0, Hen Position: (91, 88)\n",
      "Step: 6, Action: 0, Reward: 2.75, Total Reward: 8.75, Hen Position: (97, 72)\n",
      "Step: 7, Action: 0, Reward: 2.75, Total Reward: 11.5, Hen Position: (96, 62)\n",
      "Step: 8, Action: 0, Reward: 2.75, Total Reward: 14.25, Hen Position: (98, 54)\n",
      "Step: 9, Action: 0, Reward: 2.75, Total Reward: 17.0, Hen Position: (108, 44)\n",
      "Step: 10, Action: 0, Reward: 2.75, Total Reward: 19.75, Hen Position: (90, 159)\n",
      "Step: 11, Action: 0, Reward: -7.25, Total Reward: 12.5, Hen Position: (90, 159)\n",
      "Step: 12, Action: 2, Reward: -0.25, Total Reward: 12.25, Hen Position: (90, 159)\n",
      "Step: 13, Action: 0, Reward: -7, Total Reward: 5.25, Hen Position: (90, 159)\n",
      "\n",
      "episode: 9, loss: 0, reward: -94.75\n",
      "Step: 0, Action: 3, Reward: -0.25, Total Reward: -0.25, Hen Position: (104, 92)\n",
      "Step: 1, Action: 1, Reward: -2, Total Reward: -2.25, Hen Position: (106, 116)\n",
      "Step: 2, Action: 3, Reward: -0.25, Total Reward: -2.5, Hen Position: (109, 133)\n",
      "Step: 3, Action: 4, Reward: -0.5, Total Reward: -3.0, Hen Position: (113, 141)\n",
      "Step: 4, Action: 2, Reward: -0.25, Total Reward: -3.25, Hen Position: (92, 144)\n",
      "Step: 5, Action: 4, Reward: -0.5, Total Reward: -3.75, Hen Position: (84, 152)\n",
      "Step: 6, Action: 3, Reward: -0.25, Total Reward: -4.0, Hen Position: (90, 159)\n",
      "Step: 7, Action: 4, Reward: -0.5, Total Reward: -4.5, Hen Position: (90, 159)\n",
      "Step: 8, Action: 3, Reward: -0.25, Total Reward: -4.75, Hen Position: (90, 159)\n",
      "Step: 9, Action: 0, Reward: -7, Total Reward: -11.75, Hen Position: (90, 159)\n",
      "Step: 10, Action: 2, Reward: -0.25, Total Reward: -12.0, Hen Position: (90, 159)\n",
      "Step: 11, Action: 0, Reward: 3, Total Reward: -9.0, Hen Position: (82, 150)\n",
      "Step: 12, Action: 0, Reward: 2.75, Total Reward: -6.25, Hen Position: (84, 131)\n",
      "Step: 13, Action: 0, Reward: 2.75, Total Reward: -3.5, Hen Position: (88, 113)\n",
      "Step: 14, Action: 0, Reward: 2.75, Total Reward: -0.75, Hen Position: (91, 93)\n",
      "Step: 15, Action: 0, Reward: 2.75, Total Reward: 2.0, Hen Position: (95, 77)\n",
      "Step: 16, Action: 0, Reward: 2.75, Total Reward: 4.75, Hen Position: (97, 63)\n",
      "Step: 17, Action: 0, Reward: 2.75, Total Reward: 7.5, Hen Position: (98, 52)\n",
      "Step: 18, Action: 1, Reward: -2, Total Reward: 5.5, Hen Position: (96, 78)\n",
      "Step: 19, Action: 4, Reward: -0.5, Total Reward: 5.0, Hen Position: (94, 101)\n",
      "Step: 20, Action: 1, Reward: -2, Total Reward: 3.0, Hen Position: (89, 126)\n",
      "Step: 21, Action: 1, Reward: -2.25, Total Reward: 0.75, Hen Position: (90, 159)\n",
      "Step: 22, Action: 2, Reward: -0.25, Total Reward: 0.5, Hen Position: (90, 159)\n",
      "Step: 23, Action: 3, Reward: -0.25, Total Reward: 0.25, Hen Position: (90, 159)\n",
      "Step: 24, Action: 4, Reward: -0.5, Total Reward: -0.25, Hen Position: (90, 159)\n",
      "Step: 25, Action: 2, Reward: -0.25, Total Reward: -0.5, Hen Position: (90, 159)\n",
      "Step: 26, Action: 1, Reward: -2, Total Reward: -2.5, Hen Position: (89, 91)\n",
      "Step: 27, Action: 0, Reward: 3, Total Reward: 0.5, Hen Position: (90, 93)\n",
      "Step: 28, Action: 0, Reward: 2.75, Total Reward: 3.25, Hen Position: (90, 159)\n",
      "Step: 29, Action: 0, Reward: -7.25, Total Reward: -4.0, Hen Position: (90, 159)\n",
      "Step: 30, Action: 4, Reward: -0.5, Total Reward: -4.5, Hen Position: (90, 159)\n",
      "\n",
      "episode: 10, loss: 0, reward: -104.5\n",
      "Step: 0, Action: 1, Reward: -2, Total Reward: -2, Hen Position: (91, 110)\n",
      "Step: 1, Action: 2, Reward: -0.25, Total Reward: -2.25, Hen Position: (75, 127)\n",
      "Step: 2, Action: 3, Reward: -0.25, Total Reward: -2.5, Hen Position: (82, 133)\n",
      "Step: 3, Action: 1, Reward: -2, Total Reward: -4.5, Hen Position: (90, 159)\n",
      "Step: 4, Action: 2, Reward: -0.25, Total Reward: -4.75, Hen Position: (90, 159)\n",
      "Step: 5, Action: 4, Reward: -0.5, Total Reward: -5.25, Hen Position: (90, 159)\n",
      "Step: 6, Action: 3, Reward: -0.25, Total Reward: -5.5, Hen Position: (90, 159)\n",
      "Step: 7, Action: 4, Reward: -0.5, Total Reward: -6.0, Hen Position: (90, 159)\n",
      "Step: 8, Action: 1, Reward: -2, Total Reward: -8.0, Hen Position: (90, 159)\n",
      "Step: 9, Action: 2, Reward: -0.25, Total Reward: -8.25, Hen Position: (89, 93)\n",
      "Step: 10, Action: 0, Reward: -7, Total Reward: -15.25, Hen Position: (89, 94)\n",
      "Step: 11, Action: 1, Reward: -2, Total Reward: -17.25, Hen Position: (90, 159)\n",
      "Step: 12, Action: 4, Reward: -0.5, Total Reward: -17.75, Hen Position: (90, 159)\n",
      "Step: 13, Action: 2, Reward: -0.25, Total Reward: -18.0, Hen Position: (90, 159)\n",
      "Step: 14, Action: 3, Reward: -0.25, Total Reward: -18.25, Hen Position: (90, 159)\n",
      "\n",
      "episode: 11, loss: 0, reward: -118.25\n",
      "Step: 0, Action: 1, Reward: -2, Total Reward: -2, Hen Position: (91, 110)\n",
      "Step: 1, Action: 2, Reward: -0.25, Total Reward: -2.25, Hen Position: (75, 127)\n",
      "Step: 2, Action: 3, Reward: -0.25, Total Reward: -2.5, Hen Position: (82, 133)\n",
      "Step: 3, Action: 4, Reward: -0.5, Total Reward: -3.0, Hen Position: (90, 141)\n",
      "Step: 4, Action: 0, Reward: 3, Total Reward: 0.0, Hen Position: (91, 125)\n",
      "Step: 5, Action: 0, Reward: 2.75, Total Reward: 2.75, Hen Position: (94, 105)\n",
      "Step: 6, Action: 4, Reward: -0.5, Total Reward: 2.25, Hen Position: (95, 104)\n",
      "Step: 7, Action: 1, Reward: -2, Total Reward: 0.25, Hen Position: (91, 124)\n",
      "Step: 8, Action: 0, Reward: -7, Total Reward: -6.75, Hen Position: (90, 123)\n",
      "Step: 9, Action: 2, Reward: -0.25, Total Reward: -7.0, Hen Position: (77, 122)\n",
      "Step: 10, Action: 4, Reward: -0.5, Total Reward: -7.5, Hen Position: (71, 130)\n",
      "Step: 11, Action: 0, Reward: 3, Total Reward: -4.5, Hen Position: (77, 115)\n",
      "Step: 12, Action: 0, Reward: 2.75, Total Reward: -1.75, Hen Position: (83, 96)\n",
      "Step: 13, Action: 0, Reward: 2.75, Total Reward: 1.0, Hen Position: (89, 79)\n",
      "Step: 14, Action: 0, Reward: 2.75, Total Reward: 3.75, Hen Position: (90, 159)\n",
      "Step: 15, Action: 0, Reward: -7.25, Total Reward: -3.5, Hen Position: (90, 159)\n",
      "\n",
      "episode: 12, loss: 0, reward: -103.5\n",
      "Step: 0, Action: 3, Reward: -0.25, Total Reward: -0.25, Hen Position: (104, 95)\n",
      "Step: 1, Action: 4, Reward: -0.5, Total Reward: -0.75, Hen Position: (109, 102)\n",
      "Step: 2, Action: 0, Reward: 3, Total Reward: 2.25, Hen Position: (108, 86)\n",
      "Step: 3, Action: 0, Reward: -7.25, Total Reward: -5.0, Hen Position: (107, 80)\n",
      "Step: 4, Action: 4, Reward: -0.5, Total Reward: -5.5, Hen Position: (107, 75)\n",
      "Step: 5, Action: 3, Reward: -0.25, Total Reward: -5.75, Hen Position: (113, 84)\n",
      "Step: 6, Action: 1, Reward: -2, Total Reward: -7.75, Hen Position: (113, 106)\n",
      "Step: 7, Action: 1, Reward: -2.25, Total Reward: -10.0, Hen Position: (103, 144)\n",
      "Step: 8, Action: 0, Reward: 3, Total Reward: -7.0, Hen Position: (99, 142)\n",
      "Step: 9, Action: 0, Reward: 2.75, Total Reward: -4.25, Hen Position: (98, 126)\n",
      "Step: 10, Action: 0, Reward: 2.75, Total Reward: -1.5, Hen Position: (98, 108)\n",
      "Step: 11, Action: 0, Reward: 2.75, Total Reward: 1.25, Hen Position: (98, 115)\n",
      "Step: 12, Action: 0, Reward: 2.75, Total Reward: 4.0, Hen Position: (96, 123)\n",
      "Step: 13, Action: 0, Reward: 2.75, Total Reward: 6.75, Hen Position: (94, 131)\n",
      "Step: 14, Action: 0, Reward: -7.25, Total Reward: -0.5, Hen Position: (92, 139)\n",
      "Step: 15, Action: 3, Reward: -0.25, Total Reward: -0.75, Hen Position: (107, 143)\n",
      "Step: 16, Action: 1, Reward: -2, Total Reward: -2.75, Hen Position: (90, 159)\n",
      "Step: 17, Action: 0, Reward: -7, Total Reward: -9.75, Hen Position: (90, 159)\n",
      "Step: 18, Action: 0, Reward: -7.25, Total Reward: -17.0, Hen Position: (90, 159)\n",
      "\n",
      "episode: 13, loss: 0, reward: -117.0\n",
      "Step: 0, Action: 3, Reward: -0.25, Total Reward: -0.25, Hen Position: (103, 98)\n",
      "Step: 1, Action: 2, Reward: -0.25, Total Reward: -0.5, Hen Position: (94, 106)\n",
      "Step: 2, Action: 4, Reward: -0.5, Total Reward: -1.0, Hen Position: (86, 113)\n",
      "Step: 3, Action: 0, Reward: 3, Total Reward: 2.0, Hen Position: (89, 97)\n",
      "Step: 4, Action: 0, Reward: 2.75, Total Reward: 4.75, Hen Position: (93, 79)\n",
      "Step: 5, Action: 0, Reward: 2.75, Total Reward: 7.5, Hen Position: (96, 66)\n",
      "Step: 6, Action: 0, Reward: 2.75, Total Reward: 10.25, Hen Position: (98, 56)\n",
      "Step: 7, Action: 0, Reward: -7.25, Total Reward: 3.0, Hen Position: (99, 53)\n",
      "Step: 8, Action: 4, Reward: -0.5, Total Reward: 2.5, Hen Position: (100, 63)\n",
      "Step: 9, Action: 1, Reward: -2, Total Reward: 0.5, Hen Position: (95, 84)\n",
      "Step: 10, Action: 1, Reward: -2.25, Total Reward: -1.75, Hen Position: (90, 124)\n",
      "Step: 11, Action: 3, Reward: -0.25, Total Reward: -2.0, Hen Position: (90, 159)\n",
      "Step: 12, Action: 2, Reward: -0.25, Total Reward: -2.25, Hen Position: (90, 159)\n",
      "Step: 13, Action: 1, Reward: -2, Total Reward: -4.25, Hen Position: (102, 126)\n",
      "Step: 14, Action: 4, Reward: -0.5, Total Reward: -4.75, Hen Position: (90, 159)\n",
      "\n",
      "episode: 14, loss: 0, reward: -104.75\n",
      "Step: 0, Action: 1, Reward: -2, Total Reward: -2, Hen Position: (91, 115)\n",
      "Step: 1, Action: 3, Reward: -0.25, Total Reward: -2.25, Hen Position: (98, 131)\n",
      "Step: 2, Action: 4, Reward: -0.5, Total Reward: -2.75, Hen Position: (103, 139)\n",
      "Step: 3, Action: 1, Reward: -2, Total Reward: -4.75, Hen Position: (90, 159)\n",
      "Step: 4, Action: 0, Reward: -7, Total Reward: -11.75, Hen Position: (90, 159)\n",
      "Step: 5, Action: 1, Reward: -2, Total Reward: -13.75, Hen Position: (90, 159)\n",
      "Step: 6, Action: 4, Reward: -0.5, Total Reward: -14.25, Hen Position: (90, 159)\n",
      "Step: 7, Action: 0, Reward: -7, Total Reward: -21.25, Hen Position: (90, 159)\n",
      "Step: 8, Action: 3, Reward: -0.25, Total Reward: -21.5, Hen Position: (90, 159)\n",
      "Step: 9, Action: 2, Reward: -0.25, Total Reward: -21.75, Hen Position: (90, 159)\n",
      "Step: 10, Action: 3, Reward: -0.25, Total Reward: -22.0, Hen Position: (90, 159)\n",
      "Step: 11, Action: 0, Reward: -7, Total Reward: -29.0, Hen Position: (90, 159)\n",
      "Step: 12, Action: 1, Reward: -2, Total Reward: -31.0, Hen Position: (90, 159)\n",
      "Step: 13, Action: 2, Reward: -0.25, Total Reward: -31.25, Hen Position: (90, 159)\n",
      "Step: 14, Action: 0, Reward: 3, Total Reward: -28.25, Hen Position: (88, 95)\n",
      "Step: 15, Action: 0, Reward: -7.25, Total Reward: -35.5, Hen Position: (88, 94)\n",
      "Step: 16, Action: 2, Reward: -0.25, Total Reward: -35.75, Hen Position: (90, 159)\n",
      "Step: 17, Action: 0, Reward: -7, Total Reward: -42.75, Hen Position: (90, 159)\n",
      "Step: 18, Action: 4, Reward: -0.5, Total Reward: -43.25, Hen Position: (90, 159)\n",
      "Step: 19, Action: 0, Reward: -7, Total Reward: -50.25, Hen Position: (90, 159)\n",
      "\n",
      "episode: 15, loss: 0, reward: -150.25\n",
      "Step: 0, Action: 4, Reward: -0.5, Total Reward: -0.5, Hen Position: (95, 94)\n",
      "Step: 1, Action: 2, Reward: -0.25, Total Reward: -0.75, Hen Position: (79, 100)\n",
      "Step: 2, Action: 3, Reward: -0.25, Total Reward: -1.0, Hen Position: (85, 107)\n",
      "Step: 3, Action: 0, Reward: 3, Total Reward: 2.0, Hen Position: (97, 95)\n",
      "Step: 4, Action: 0, Reward: 2.75, Total Reward: 4.75, Hen Position: (99, 79)\n",
      "Step: 5, Action: 0, Reward: 2.75, Total Reward: 7.5, Hen Position: (100, 65)\n",
      "Step: 6, Action: 0, Reward: 2.75, Total Reward: 10.25, Hen Position: (101, 53)\n",
      "Step: 7, Action: 0, Reward: 2.75, Total Reward: 13.0, Hen Position: (102, 46)\n",
      "Step: 8, Action: 0, Reward: -7.25, Total Reward: 5.75, Hen Position: (102, 41)\n",
      "Step: 9, Action: 4, Reward: -0.5, Total Reward: 5.25, Hen Position: (103, 48)\n",
      "Step: 10, Action: 0, Reward: -7, Total Reward: -1.75, Hen Position: (102, 45)\n",
      "Step: 11, Action: 3, Reward: -0.25, Total Reward: -2.0, Hen Position: (110, 58)\n",
      "Step: 12, Action: 0, Reward: -7, Total Reward: -9.0, Hen Position: (116, 56)\n",
      "Step: 13, Action: 3, Reward: -0.25, Total Reward: -9.25, Hen Position: (129, 66)\n",
      "Step: 14, Action: 4, Reward: -0.5, Total Reward: -9.75, Hen Position: (124, 77)\n",
      "Step: 15, Action: 0, Reward: 3, Total Reward: -6.75, Hen Position: (116, 88)\n",
      "Step: 16, Action: 0, Reward: 2.75, Total Reward: -4.0, Hen Position: (110, 100)\n",
      "Step: 17, Action: 0, Reward: 2.75, Total Reward: -1.25, Hen Position: (104, 111)\n",
      "Step: 18, Action: 0, Reward: 2.75, Total Reward: 1.5, Hen Position: (100, 120)\n",
      "Step: 19, Action: 0, Reward: 2.75, Total Reward: 4.25, Hen Position: (97, 137)\n",
      "Step: 20, Action: 0, Reward: 2.75, Total Reward: 7.0, Hen Position: (94, 145)\n",
      "Step: 21, Action: 0, Reward: -7.25, Total Reward: -0.25, Hen Position: (92, 146)\n",
      "Step: 22, Action: 3, Reward: -0.25, Total Reward: -0.5, Hen Position: (90, 159)\n",
      "Step: 23, Action: 0, Reward: -7, Total Reward: -7.5, Hen Position: (90, 159)\n",
      "Step: 24, Action: 4, Reward: -0.5, Total Reward: -8.0, Hen Position: (90, 159)\n",
      "Step: 25, Action: 0, Reward: -7, Total Reward: -15.0, Hen Position: (90, 159)\n",
      "Step: 26, Action: 4, Reward: -0.5, Total Reward: -15.5, Hen Position: (90, 159)\n",
      "Step: 27, Action: 1, Reward: -2, Total Reward: -17.5, Hen Position: (89, 149)\n",
      "Step: 28, Action: 3, Reward: -0.25, Total Reward: -17.75, Hen Position: (91, 99)\n",
      "Step: 29, Action: 4, Reward: -0.5, Total Reward: -18.25, Hen Position: (91, 101)\n",
      "Step: 30, Action: 0, Reward: 3, Total Reward: -15.25, Hen Position: (90, 159)\n",
      "Step: 31, Action: 0, Reward: -7.25, Total Reward: -22.5, Hen Position: (90, 159)\n",
      "Step: 32, Action: 4, Reward: -0.5, Total Reward: -23.0, Hen Position: (90, 159)\n",
      "\n",
      "episode: 16, loss: 0, reward: -123.0\n",
      "Step: 0, Action: 2, Reward: -0.25, Total Reward: -0.25, Hen Position: (80, 96)\n",
      "Step: 1, Action: 1, Reward: -2, Total Reward: -2.25, Hen Position: (74, 119)\n",
      "Step: 2, Action: 0, Reward: -7, Total Reward: -9.25, Hen Position: (78, 117)\n",
      "Step: 3, Action: 2, Reward: -0.25, Total Reward: -9.5, Hen Position: (67, 115)\n",
      "Step: 4, Action: 1, Reward: -2, Total Reward: -11.5, Hen Position: (65, 138)\n",
      "Step: 5, Action: 0, Reward: 3, Total Reward: -8.5, Hen Position: (70, 137)\n",
      "Step: 6, Action: 0, Reward: 2.75, Total Reward: -5.75, Hen Position: (76, 115)\n",
      "Step: 7, Action: 0, Reward: 2.75, Total Reward: -3.0, Hen Position: (83, 98)\n",
      "Step: 8, Action: 0, Reward: 2.75, Total Reward: -0.25, Hen Position: (88, 80)\n",
      "Step: 9, Action: 1, Reward: -2, Total Reward: -2.25, Hen Position: (87, 99)\n",
      "Step: 10, Action: 2, Reward: -0.25, Total Reward: -2.5, Hen Position: (73, 120)\n",
      "Step: 11, Action: 4, Reward: -0.5, Total Reward: -3.0, Hen Position: (68, 128)\n",
      "Step: 12, Action: 4, Reward: -0.75, Total Reward: -3.75, Hen Position: (69, 131)\n",
      "Step: 13, Action: 1, Reward: -2, Total Reward: -5.75, Hen Position: (71, 150)\n",
      "Step: 14, Action: 4, Reward: -0.5, Total Reward: -6.25, Hen Position: (90, 159)\n",
      "Step: 15, Action: 0, Reward: -7, Total Reward: -13.25, Hen Position: (90, 159)\n",
      "Step: 16, Action: 1, Reward: -2, Total Reward: -15.25, Hen Position: (90, 159)\n",
      "Step: 17, Action: 0, Reward: -7, Total Reward: -22.25, Hen Position: (90, 159)\n",
      "\n",
      "episode: 17, loss: 0, reward: -122.25\n",
      "Step: 0, Action: 3, Reward: -0.25, Total Reward: -0.25, Hen Position: (104, 92)\n",
      "Step: 1, Action: 0, Reward: 3, Total Reward: 2.75, Hen Position: (108, 84)\n",
      "Step: 2, Action: 0, Reward: 2.75, Total Reward: 5.5, Hen Position: (107, 71)\n",
      "Step: 3, Action: 0, Reward: 2.75, Total Reward: 8.25, Hen Position: (107, 58)\n",
      "Step: 4, Action: 3, Reward: -0.25, Total Reward: 8.0, Hen Position: (115, 66)\n",
      "Step: 5, Action: 2, Reward: -0.25, Total Reward: 7.75, Hen Position: (104, 80)\n",
      "Step: 6, Action: 1, Reward: -2, Total Reward: 5.75, Hen Position: (92, 106)\n",
      "Step: 7, Action: 3, Reward: -0.25, Total Reward: 5.5, Hen Position: (100, 128)\n",
      "Step: 8, Action: 4, Reward: -0.5, Total Reward: 5.0, Hen Position: (104, 138)\n",
      "Step: 9, Action: 0, Reward: 3, Total Reward: 8.0, Hen Position: (103, 123)\n",
      "Step: 10, Action: 0, Reward: 2.75, Total Reward: 10.75, Hen Position: (102, 106)\n",
      "Step: 11, Action: 0, Reward: 2.75, Total Reward: 13.5, Hen Position: (101, 112)\n",
      "Step: 12, Action: 0, Reward: 2.75, Total Reward: 16.25, Hen Position: (98, 121)\n",
      "Step: 13, Action: 0, Reward: -7.25, Total Reward: 9.0, Hen Position: (96, 129)\n",
      "Step: 14, Action: 2, Reward: -0.25, Total Reward: 8.75, Hen Position: (79, 130)\n",
      "Step: 15, Action: 4, Reward: -0.5, Total Reward: 8.25, Hen Position: (73, 139)\n",
      "Step: 16, Action: 0, Reward: 3, Total Reward: 11.25, Hen Position: (78, 121)\n",
      "Step: 17, Action: 2, Reward: -0.25, Total Reward: 11.0, Hen Position: (68, 121)\n",
      "Step: 18, Action: 4, Reward: -0.5, Total Reward: 10.5, Hen Position: (90, 159)\n",
      "Step: 19, Action: 3, Reward: -0.25, Total Reward: 10.25, Hen Position: (90, 159)\n",
      "Step: 20, Action: 3, Reward: -0.5, Total Reward: 9.75, Hen Position: (87, 143)\n",
      "Step: 21, Action: 4, Reward: -0.5, Total Reward: 9.25, Hen Position: (88, 148)\n",
      "Step: 22, Action: 0, Reward: 3, Total Reward: 12.25, Hen Position: (88, 133)\n",
      "Step: 23, Action: 0, Reward: 2.75, Total Reward: 15.0, Hen Position: (91, 118)\n",
      "Step: 24, Action: 0, Reward: 2.75, Total Reward: 17.75, Hen Position: (93, 101)\n",
      "Step: 25, Action: 0, Reward: 2.75, Total Reward: 20.5, Hen Position: (96, 83)\n",
      "Step: 26, Action: 0, Reward: 2.75, Total Reward: 23.25, Hen Position: (98, 69)\n",
      "Step: 27, Action: 0, Reward: 2.75, Total Reward: 26.0, Hen Position: (99, 57)\n",
      "Step: 28, Action: 0, Reward: 2.75, Total Reward: 28.75, Hen Position: (100, 56)\n",
      "Step: 29, Action: 0, Reward: 2.75, Total Reward: 31.5, Hen Position: (104, 80)\n",
      "Step: 30, Action: 0, Reward: 2.75, Total Reward: 34.25, Hen Position: (103, 121)\n",
      "\n",
      "episode: 18, loss: 0, reward: -65.75\n",
      "Step: 0, Action: 3, Reward: -0.25, Total Reward: -0.25, Hen Position: (104, 92)\n",
      "Step: 1, Action: 1, Reward: -2, Total Reward: -2.25, Hen Position: (107, 114)\n",
      "Step: 2, Action: 2, Reward: -0.25, Total Reward: -2.5, Hen Position: (87, 130)\n",
      "Step: 3, Action: 4, Reward: -0.5, Total Reward: -3.0, Hen Position: (79, 137)\n",
      "Step: 4, Action: 1, Reward: -2, Total Reward: -5.0, Hen Position: (90, 159)\n",
      "Step: 5, Action: 3, Reward: -0.25, Total Reward: -5.25, Hen Position: (90, 159)\n",
      "Step: 6, Action: 4, Reward: -0.5, Total Reward: -5.75, Hen Position: (90, 159)\n",
      "Step: 7, Action: 0, Reward: -7, Total Reward: -12.75, Hen Position: (90, 159)\n",
      "Step: 8, Action: 4, Reward: -0.5, Total Reward: -13.25, Hen Position: (90, 159)\n",
      "Step: 9, Action: 3, Reward: -0.25, Total Reward: -13.5, Hen Position: (90, 159)\n",
      "Step: 10, Action: 2, Reward: -0.25, Total Reward: -13.75, Hen Position: (90, 159)\n",
      "Step: 11, Action: 3, Reward: -0.25, Total Reward: -14.0, Hen Position: (90, 159)\n",
      "Step: 12, Action: 2, Reward: -0.25, Total Reward: -14.25, Hen Position: (90, 159)\n",
      "Step: 13, Action: 3, Reward: -0.25, Total Reward: -14.5, Hen Position: (90, 159)\n",
      "Step: 14, Action: 4, Reward: -0.5, Total Reward: -15.0, Hen Position: (90, 159)\n",
      "Step: 15, Action: 3, Reward: -0.25, Total Reward: -15.25, Hen Position: (90, 159)\n",
      "Step: 16, Action: 0, Reward: -7, Total Reward: -22.25, Hen Position: (90, 159)\n",
      "Step: 17, Action: 4, Reward: -0.5, Total Reward: -22.75, Hen Position: (90, 159)\n",
      "Step: 18, Action: 1, Reward: -2, Total Reward: -24.75, Hen Position: (90, 159)\n",
      "Step: 19, Action: 2, Reward: -0.25, Total Reward: -25.0, Hen Position: (90, 159)\n",
      "Step: 20, Action: 0, Reward: -7, Total Reward: -32.0, Hen Position: (90, 159)\n",
      "Step: 21, Action: 3, Reward: -0.25, Total Reward: -32.25, Hen Position: (90, 159)\n",
      "Step: 22, Action: 4, Reward: -0.5, Total Reward: -32.75, Hen Position: (90, 159)\n",
      "\n",
      "episode: 19, loss: 0, reward: -132.75\n",
      "Step: 0, Action: 3, Reward: -0.25, Total Reward: -0.25, Hen Position: (103, 98)\n",
      "Step: 1, Action: 4, Reward: -0.5, Total Reward: -0.75, Hen Position: (108, 106)\n",
      "Step: 2, Action: 0, Reward: 3, Total Reward: 2.25, Hen Position: (107, 90)\n",
      "Step: 3, Action: 0, Reward: 2.75, Total Reward: 5.0, Hen Position: (106, 73)\n",
      "Step: 4, Action: 0, Reward: 2.75, Total Reward: 7.75, Hen Position: (90, 159)\n",
      "Step: 5, Action: 0, Reward: -7.25, Total Reward: 0.5, Hen Position: (90, 159)\n",
      "\n",
      "episode: 20, loss: 0, reward: -99.5\n",
      "Step: 0, Action: 0, Reward: 2.75, Total Reward: 2.75, Hen Position: (90, 159)\n",
      "Step: 1, Action: 0, Reward: 2.75, Total Reward: 5.5, Hen Position: (98, 63)\n",
      "Step: 2, Action: 0, Reward: 2.75, Total Reward: 8.25, Hen Position: (99, 54)\n",
      "Step: 3, Action: 0, Reward: 2.75, Total Reward: 11.0, Hen Position: (100, 51)\n",
      "Step: 4, Action: 0, Reward: 2.75, Total Reward: 13.75, Hen Position: (101, 45)\n",
      "Step: 5, Action: 0, Reward: -7.25, Total Reward: 6.5, Hen Position: (101, 40)\n",
      "Step: 6, Action: 0, Reward: 2.75, Total Reward: 9.25, Hen Position: (103, 84)\n",
      "Step: 7, Action: 0, Reward: 2.75, Total Reward: 12.0, Hen Position: (90, 159)\n",
      "\n",
      "episode: 21, loss: 0, reward: -88.0\n",
      "Step: 0, Action: 3, Reward: -0.25, Total Reward: -0.25, Hen Position: (104, 92)\n",
      "Step: 1, Action: 2, Reward: -0.25, Total Reward: -0.5, Hen Position: (95, 99)\n",
      "Step: 2, Action: 1, Reward: -2, Total Reward: -2.5, Hen Position: (86, 122)\n",
      "Step: 3, Action: 0, Reward: -7, Total Reward: -9.5, Hen Position: (87, 122)\n",
      "Step: 4, Action: 3, Reward: -0.25, Total Reward: -9.75, Hen Position: (98, 121)\n",
      "Step: 5, Action: 4, Reward: -0.5, Total Reward: -10.25, Hen Position: (103, 130)\n",
      "Step: 6, Action: 3, Reward: -0.25, Total Reward: -10.5, Hen Position: (109, 134)\n",
      "Step: 7, Action: 2, Reward: -0.25, Total Reward: -10.75, Hen Position: (96, 141)\n",
      "Step: 8, Action: 3, Reward: -0.25, Total Reward: -11.0, Hen Position: (100, 149)\n",
      "Step: 9, Action: 0, Reward: -7, Total Reward: -18.0, Hen Position: (103, 140)\n",
      "Step: 10, Action: 4, Reward: -0.5, Total Reward: -18.5, Hen Position: (102, 137)\n",
      "Step: 11, Action: 2, Reward: -0.25, Total Reward: -18.75, Hen Position: (83, 143)\n",
      "Step: 12, Action: 0, Reward: 3, Total Reward: -15.75, Hen Position: (115, 130)\n",
      "Step: 13, Action: 0, Reward: 2.75, Total Reward: -13.0, Hen Position: (90, 159)\n",
      "\n",
      "episode: 22, loss: 0, reward: -113.0\n",
      "Step: 0, Action: 2, Reward: -0.25, Total Reward: -0.25, Hen Position: (80, 92)\n",
      "Step: 1, Action: 0, Reward: 3, Total Reward: 2.75, Hen Position: (80, 81)\n",
      "Step: 2, Action: 0, Reward: 2.75, Total Reward: 5.5, Hen Position: (86, 67)\n",
      "Step: 3, Action: 0, Reward: 2.75, Total Reward: 8.25, Hen Position: (90, 66)\n",
      "Step: 4, Action: 0, Reward: 2.75, Total Reward: 11.0, Hen Position: (90, 159)\n",
      "\n",
      "episode: 23, loss: 0, reward: -89.0\n",
      "Step: 0, Action: 3, Reward: -0.25, Total Reward: -0.25, Hen Position: (104, 94)\n",
      "Step: 1, Action: 0, Reward: 3, Total Reward: 2.75, Hen Position: (111, 83)\n",
      "Step: 2, Action: 0, Reward: 2.75, Total Reward: 5.5, Hen Position: (108, 92)\n",
      "Step: 3, Action: 0, Reward: -7.25, Total Reward: -1.75, Hen Position: (104, 93)\n",
      "Step: 4, Action: 2, Reward: -0.25, Total Reward: -2.0, Hen Position: (86, 101)\n",
      "Step: 5, Action: 0, Reward: 3, Total Reward: 1.0, Hen Position: (84, 90)\n",
      "Step: 6, Action: 0, Reward: 2.75, Total Reward: 3.75, Hen Position: (89, 73)\n",
      "Step: 7, Action: 0, Reward: 2.75, Total Reward: 6.5, Hen Position: (92, 61)\n",
      "Step: 8, Action: 0, Reward: 2.75, Total Reward: 9.25, Hen Position: (96, 51)\n",
      "Step: 9, Action: 0, Reward: 2.75, Total Reward: 12.0, Hen Position: (102, 44)\n",
      "Step: 10, Action: 0, Reward: 2.75, Total Reward: 14.75, Hen Position: (99, 37)\n",
      "Step: 11, Action: 0, Reward: 2.75, Total Reward: 17.5, Hen Position: (100, 33)\n",
      "Step: 12, Action: 0, Reward: -7.25, Total Reward: 10.25, Hen Position: (104, 30)\n",
      "Step: 13, Action: 2, Reward: -0.25, Total Reward: 10.0, Hen Position: (102, 44)\n",
      "Step: 14, Action: 3, Reward: -0.25, Total Reward: 9.75, Hen Position: (103, 64)\n",
      "Step: 15, Action: 2, Reward: -0.25, Total Reward: 9.5, Hen Position: (101, 79)\n",
      "Step: 16, Action: 4, Reward: -0.5, Total Reward: 9.0, Hen Position: (83, 97)\n",
      "Step: 17, Action: 1, Reward: -2, Total Reward: 7.0, Hen Position: (90, 159)\n",
      "Step: 18, Action: 2, Reward: -0.25, Total Reward: 6.75, Hen Position: (90, 159)\n",
      "Step: 19, Action: 0, Reward: 3, Total Reward: 9.75, Hen Position: (103, 129)\n",
      "\n",
      "episode: 24, loss: 0, reward: -90.25\n",
      "Step: 0, Action: 1, Reward: -2, Total Reward: -2, Hen Position: (91, 110)\n",
      "Step: 1, Action: 4, Reward: -0.5, Total Reward: -2.5, Hen Position: (90, 127)\n",
      "Step: 2, Action: 0, Reward: 3, Total Reward: 0.5, Hen Position: (90, 112)\n",
      "Step: 3, Action: 0, Reward: 2.75, Total Reward: 3.25, Hen Position: (93, 93)\n",
      "Step: 4, Action: 0, Reward: 2.75, Total Reward: 6.0, Hen Position: (96, 76)\n",
      "Step: 5, Action: 0, Reward: 2.75, Total Reward: 8.75, Hen Position: (106, 62)\n",
      "Step: 6, Action: 0, Reward: 2.75, Total Reward: 11.5, Hen Position: (103, 89)\n",
      "Step: 7, Action: 0, Reward: 2.75, Total Reward: 14.25, Hen Position: (90, 159)\n",
      "\n",
      "episode: 25, loss: 0, reward: -85.75\n",
      "Step: 0, Action: 2, Reward: -0.25, Total Reward: -0.25, Hen Position: (82, 92)\n",
      "Step: 1, Action: 1, Reward: -2, Total Reward: -2.25, Hen Position: (75, 113)\n",
      "Step: 2, Action: 3, Reward: -0.25, Total Reward: -2.5, Hen Position: (86, 129)\n",
      "Step: 3, Action: 2, Reward: -0.25, Total Reward: -2.75, Hen Position: (80, 135)\n",
      "Step: 4, Action: 1, Reward: -2, Total Reward: -4.75, Hen Position: (90, 159)\n",
      "Step: 5, Action: 2, Reward: -0.25, Total Reward: -5.0, Hen Position: (90, 159)\n",
      "Step: 6, Action: 4, Reward: -0.5, Total Reward: -5.5, Hen Position: (90, 159)\n",
      "Step: 7, Action: 0, Reward: -7, Total Reward: -12.5, Hen Position: (90, 159)\n",
      "Step: 8, Action: 3, Reward: -0.25, Total Reward: -12.75, Hen Position: (90, 159)\n",
      "Step: 9, Action: 0, Reward: -7, Total Reward: -19.75, Hen Position: (91, 151)\n",
      "Step: 10, Action: 2, Reward: -0.25, Total Reward: -20.0, Hen Position: (90, 159)\n",
      "Step: 11, Action: 0, Reward: 3, Total Reward: -17.0, Hen Position: (74, 139)\n",
      "Step: 12, Action: 0, Reward: 2.75, Total Reward: -14.25, Hen Position: (76, 122)\n",
      "Step: 13, Action: 0, Reward: 2.75, Total Reward: -11.5, Hen Position: (68, 101)\n",
      "Step: 14, Action: 0, Reward: 2.75, Total Reward: -8.75, Hen Position: (90, 159)\n",
      "Step: 15, Action: 0, Reward: -7.25, Total Reward: -16.0, Hen Position: (90, 159)\n",
      "\n",
      "episode: 26, loss: 0, reward: -116.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[68], line 237\u001b[0m\n\u001b[0;32m    233\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mepisode: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, loss: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, reward: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(episode, total_loss, total_reward))\n\u001b[0;32m    234\u001b[0m \u001b[38;5;66;03m# agent.save('dqn.pth')\u001b[39;00m\n\u001b[0;32m    235\u001b[0m \u001b[38;5;66;03m# agent.reset()\u001b[39;00m\n\u001b[1;32m--> 237\u001b[0m \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m3.25\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    238\u001b[0m keyboard\u001b[38;5;241m.\u001b[39mpress_and_release(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspace\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# run DQN agent on Crossy Road game to train and play the game\n",
    "import pyautogui\n",
    "import cv2\n",
    "import time\n",
    "import keyboard\n",
    "from pygame import ver\n",
    "from torch import res\n",
    "import torchvision\n",
    "from ultralytics import YOLO\n",
    "\n",
    "\n",
    "RES_X = 1920\n",
    "RES_Y = 1080\n",
    "\n",
    "GAME_REGION = (405, 210, 850, 480)\n",
    "restart_button = cv2.imread('restart_button.png', cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "\n",
    "def get_screen(region):\n",
    "    screen = pyautogui.screenshot(region=(region[0], region[1], region[2], region[3]))\n",
    "    \n",
    "    non_crop = screen.copy()\n",
    "\n",
    "    transforms = torchvision.transforms.Compose([\n",
    "        torchvision.transforms.RandomRotation((14, 14)),\n",
    "        torchvision.transforms.CenterCrop((320, 566)),\n",
    "        torchvision.transforms.Resize((240, 425)),\n",
    "    ])\n",
    "    \n",
    "    screen = transforms(screen)   \n",
    "    \n",
    "    screen = cv2.cvtColor(np.array(screen), cv2.COLOR_RGB2BGR)\n",
    "    \n",
    "    non_crop = cv2.cvtColor(np.array(non_crop), cv2.COLOR_RGB2BGR)\n",
    "    non_crop = cv2.resize(non_crop, (425, 240))\n",
    "    \n",
    "    return screen, non_crop\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def map_to_grid(image_size, grid_size, boxes, class_labels):\n",
    "    \"\"\"\n",
    "    Map detected bounding boxes to a grid representation.\n",
    "\n",
    "    Args:\n",
    "        image_size: Tuple (width, height) of the image.\n",
    "        grid_size: Tuple (N, M) of the grid dimensions.\n",
    "        boxes: List of bounding boxes [(x_min, y_min, x_max, y_max)].\n",
    "        class_labels: List of class labels corresponding to the boxes.\n",
    "\n",
    "    Returns:\n",
    "        grid: 2D numpy array of shape (N, M) with object class labels.\n",
    "    \"\"\"\n",
    "    width, height = image_size\n",
    "    grid_width, grid_height = grid_size\n",
    "    grid = np.zeros((grid_height, grid_width), dtype=int)\n",
    "\n",
    "    cell_width = width / grid_width\n",
    "    cell_height = height / grid_height\n",
    "\n",
    "    for (x_min, y_min, x_max, y_max), label in zip(boxes, class_labels):\n",
    "        x_start = int(x_min // cell_width)\n",
    "        y_start = int(y_min // cell_height)\n",
    "        x_end = int(np.ceil(x_max / cell_width))\n",
    "        y_end = int(np.ceil(y_max / cell_height))\n",
    "\n",
    "        for y in range(y_start, y_end):\n",
    "            for x in range(x_start, x_end):\n",
    "                grid[y, x] = label + 1\n",
    "\n",
    "    return grid\n",
    "\n",
    "\n",
    "def get_state(screen):\n",
    "    results = cv_model(screen, verbose=False)\n",
    "\n",
    "    image_size = (425, 240)  # Example image dimensions (width, height)\n",
    "    grid_size = (180, 160)    # Example grid dimensions (N, M)\n",
    "\n",
    "    boxes = []\n",
    "    labels = []\n",
    "    \n",
    "    boxes_ = results[0].boxes\n",
    "    for box in boxes_:\n",
    "        x_min, y_min, x_max, y_max = box.xyxy[0].tolist()\n",
    "        \n",
    "        class_id = int(box.cls[0].item())\n",
    "        \n",
    "        boxes.append((x_min, y_min, x_max, y_max))\n",
    "        labels.append(class_id)  # Assuming class_id is the label\n",
    "\n",
    "    grid = map_to_grid(image_size, grid_size, boxes, labels)\n",
    "    # print(grid)\n",
    "    return grid\n",
    "\n",
    "def is_game_over(image, score_threshold=0.5, scale=0.5):\n",
    "    grey_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    resized_template = cv2.resize(restart_button, (0, 0), fx=scale, fy=scale)\n",
    "    h, w = grey_image.shape\n",
    "\n",
    "    cropped_search_box = grey_image[int(h * 0.87):, int(w * 0.43):int(w * 0.57)]\n",
    "    \n",
    "    # cv2.imwrite('cropped_search_box.png', cropped_search_box)\n",
    "    \n",
    "    result = cv2.matchTemplate(cropped_search_box, resized_template, cv2.TM_CCOEFF_NORMED)\n",
    "    result = np.sort(result.flatten())[::-1]\n",
    "        \n",
    "    return result.max() > score_threshold\n",
    "\n",
    "def compute_reward(reward_state):\n",
    "    # Compute reward based on the change in the game screen   \n",
    "    time = reward_state['time']\n",
    "    state = reward_state['state']\n",
    "    action = reward_state['action']\n",
    "    prev_action = reward_state['prev_action']\n",
    "    next_state = reward_state['next_state']\n",
    "    total_reward = reward_state['total_reward']\n",
    "    non_crop_state = reward_state['non_crop_state']\n",
    "    reward = 0\n",
    "    \n",
    "    if is_game_over(non_crop_state):\n",
    "        reward = -100\n",
    "        return reward\n",
    "    \n",
    "    if action == 0:\n",
    "        reward += 3\n",
    "    elif action == 1:\n",
    "        reward -= 2\n",
    "    elif action == 2:\n",
    "        reward -= 0.25\n",
    "    elif action == 3:\n",
    "        reward -= 0.25\n",
    "    elif action == 4:\n",
    "        reward -= 0.5\n",
    "\n",
    "    if action == prev_action:\n",
    "        reward -= 0.25\n",
    "    \n",
    "    # print(state)\n",
    "    \n",
    "    hen_position_x, hen_position_y = state[0]\n",
    "    \n",
    "    if action == 0:\n",
    "        if state[0][0] // 10 == next_state[0][0] // 10 \\\n",
    "        and state[0][1] // 10 == next_state[0][1] // 10:\n",
    "            reward -= 10\n",
    "            \n",
    "    \n",
    "    return reward\n",
    "    \n",
    "\n",
    "# train DQN agent\n",
    "# agent = DQNAgent(180, 5, 128, 1000, 32, 0.99, 1.0, 0.1, 10000)\n",
    "actions = [0, 1, 2, 3, 4]\n",
    "agent = QLearningAgent(actions=actions, learning_rate=0.1, gamma=0.99, epsilon=0.1)\n",
    "cv_model = YOLO('best_cv.pt')\n",
    "episodes = 1000\n",
    "episode_length = 1000\n",
    "losses = []\n",
    "rewards = []\n",
    "\n",
    "# agent.load('dqn.pth')\n",
    "\n",
    "print(\"Model is ready to train\")\n",
    "\n",
    "keyboard.wait('q')\n",
    "\n",
    "for episode in range(episodes):\n",
    "    screenshot, non_crop_state = get_screen(GAME_REGION)\n",
    "    state = get_state(screenshot)\n",
    "    total_reward = 0\n",
    "    total_loss = 0\n",
    "    action = 0\n",
    "\n",
    "    state = simplify_state(state)\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "    \n",
    "    \n",
    "    start_time = time.time()\n",
    "    for step in range(episode_length):\n",
    "        prev_action = action\n",
    "        time.sleep(0.1)\n",
    "        \n",
    "        action = agent.choose_action(state)\n",
    "        \n",
    "        if action < 4:\n",
    "            pyautogui.press(['up', 'down', 'left', 'right'][action])\n",
    "        \n",
    "        \n",
    "        next_screenshot, non_crop_state = get_screen(GAME_REGION)\n",
    "        next_state_raw = get_state(next_screenshot)\n",
    "        # Save the state as a screenshot\n",
    "        \n",
    "        # cv2.imwrite(f'state_{step}.png', state)\n",
    "        \n",
    "        next_state = simplify_state(next_state_raw)\n",
    "        \n",
    "        reward_state = {\n",
    "            'state': state,\n",
    "            'action': action,\n",
    "            'prev_action': prev_action,\n",
    "            'next_state': next_state,\n",
    "            'time': time.time() - start_time,\n",
    "            'total_reward': total_reward,\n",
    "            'non_crop_state': non_crop_state\n",
    "        }\n",
    "        \n",
    "        done = 0\n",
    "        reward = compute_reward(reward_state)\n",
    "        \n",
    "        \n",
    "        \n",
    "        # agent.push(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "        \n",
    "        if is_game_over(non_crop_state):\n",
    "            done = 1\n",
    "            agent.learn(state, action, reward, next_state, done)\n",
    "            break\n",
    "        \n",
    "        agent.learn(state, action, reward, next_state, done)\n",
    "        # else:\n",
    "        #     print(\"False\")\n",
    "        \n",
    "        print(f\"Step: {step}, Action: {action}, Reward: {reward}, Total Reward: {total_reward}, Hen Position: {state[0]}\")\n",
    "\n",
    "    # tap space key to restart the game\n",
    "    keyboard.press_and_release('space')\n",
    "    losses.append(total_loss)\n",
    "    rewards.append(total_reward)\n",
    "    print('\\nepisode: {}, loss: {}, reward: {}'.format(episode, total_loss, total_reward))\n",
    "    # agent.save('dqn.pth')\n",
    "    # agent.reset()\n",
    "    \n",
    "    time.sleep(3.25)\n",
    "    keyboard.press_and_release('space')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
