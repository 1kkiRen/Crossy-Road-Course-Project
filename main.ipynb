{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = torch.device(\"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple\n",
    "\n",
    "\n",
    "class RecurrentIQN(nn.Module):\n",
    "    def __init__(self, input_size: int, output_size: int, hidden_size: int, n_quantiles=32):\n",
    "        \"\"\" Initialize the Recurrent IQN model\n",
    "\n",
    "        Args:\n",
    "            input_size (int): The size of the input matrix\n",
    "            output_size (int): The size of the actions\n",
    "            hidden_size (int): The size of the hidden layer\n",
    "            n_quantiles (int, optional): The number of quantiles. Defaults to 32.\n",
    "        \"\"\"\n",
    "        \n",
    "        super(RecurrentIQN, self).__init__()\n",
    "        self.n_quantiles = n_quantiles\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
    "        self.quantile_embed = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x: torch.FloatTensor, quantiles: torch.Tensor, hidden: tuple[torch.Tensor, torch.Tensor]\n",
    "):\n",
    "        \"\"\" Forward pass of the Recurrent IQN model\n",
    "\n",
    "        Args:\n",
    "            x (torch.FloatTensor): The input tensor\n",
    "            quantiles (torch.Tensor): The quantiles\n",
    "            hidden (tuple[torch.Tensor, torch.Tensor]): The hidden state\n",
    "\n",
    "        Returns:\n",
    "            torch.FloatTensor: The output tensor\n",
    "            tuple[torch.Tensor, torch.Tensor]: The hidden state\n",
    "        \"\"\"\n",
    "        \n",
    "        lstm_out, hidden = self.lstm(x, hidden)\n",
    "        \n",
    "        quantiles = quantiles.unsqueeze(-1)  \n",
    "        pi = torch.acos(torch.zeros(1)).item() * 2\n",
    "        quantile_feats = torch.cos(pi * quantiles * torch.arange(1, self.hidden_size + 1).to(x.device))\n",
    "        quantile_feats = F.relu(self.quantile_embed(quantile_feats)) \n",
    "\n",
    "        lstm_out = lstm_out[:, -1, :].unsqueeze(1) \n",
    "        x = lstm_out * quantile_feats \n",
    "\n",
    "        x = self.fc(x) \n",
    "        return x, hidden\n",
    "\n",
    "    def act(self, state: List, hidden: tuple[torch.Tensor, torch.Tensor], epsilon: float):\n",
    "        \"\"\" Acting function of the Recurrent IQN model\n",
    "\n",
    "        Args:\n",
    "            state (List): The state of the environment\n",
    "            hidden (tuple[torch.Tensor, torch.Tensor]): The hidden state of the model\n",
    "            epsilon (float): The epsilon value\n",
    "\n",
    "        Returns:\n",
    "            int: The action to take\n",
    "            Tuple: The hidden state of the model\n",
    "        \"\"\"\n",
    "        \n",
    "        if random.random() > epsilon:\n",
    "            print(\"Model acting\")\n",
    "            with torch.no_grad():\n",
    "                state = torch.FloatTensor(state).unsqueeze(0).unsqueeze(0).to(next(self.parameters()).device)\n",
    "                quantiles = torch.rand(1, self.n_quantiles).to(state.device)\n",
    "                q_values, hidden = self.forward(state, quantiles, hidden)\n",
    "                q_values = q_values.mean(dim=1)\n",
    "                action = q_values.argmax(dim=1).item()\n",
    "        else:\n",
    "            action = random.randrange(self.fc.out_features)\n",
    "        return action, hidden\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity: int):\n",
    "        \"\"\" Initialize the ReplayBuffer\n",
    "\n",
    "        Args:\n",
    "            capacity (int): The capacity of the buffer\n",
    "        \"\"\"\n",
    "        \n",
    "        self.capacity = capacity\n",
    "        self.buffer = []\n",
    "        self.position = 0\n",
    "        \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        if len(self.buffer) < self.capacity:\n",
    "            self.buffer.append(None)\n",
    "        self.buffer[self.position] = (state, action, reward, next_state, done)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "        \n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        state, action, reward, next_state, done = map(np.stack, zip(*batch))\n",
    "        return state, action, reward, next_state, done\n",
    "    \n",
    "    def pop(self, count=1):\n",
    "        for _ in range(count):\n",
    "            self.buffer.pop(0)\n",
    "            self.position -= 1\n",
    "            if self.position < 0:\n",
    "                self.position = 0\n",
    "     \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "    \n",
    "\n",
    "class IQNAgent:\n",
    "    def __init__(self, input_size, output_size, hidden_size, replay_buffer_capacity, batch_size, gamma, epsilon_start, epsilon_end, epsilon_decay, n_quantiles=32):\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.replay_buffer_capacity = replay_buffer_capacity\n",
    "        self.batch_size = batch_size\n",
    "        self.gamma = gamma\n",
    "        self.epsilon_start = epsilon_start\n",
    "        self.epsilon_end = epsilon_end\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.n_quantiles = n_quantiles\n",
    "\n",
    "        self.model = RecurrentIQN(input_size, output_size, hidden_size, n_quantiles).to(device)\n",
    "        self.model_target = RecurrentIQN(input_size, output_size, hidden_size, n_quantiles).to(device)\n",
    "        self.model_target.load_state_dict(self.model.state_dict())\n",
    "\n",
    "        self.hidden = (torch.zeros(1, 1, hidden_size).to(device),\n",
    "                       torch.zeros(1, 1, hidden_size).to(device))\n",
    "\n",
    "        self.replay_buffer = ReplayBuffer(replay_buffer_capacity)\n",
    "        self.optimizer = optim.Adam(self.model.parameters())\n",
    "\n",
    "        self.steps_done = 0\n",
    "\n",
    "        self.update_counter = 0\n",
    "        self.target_update_freq = 1000\n",
    "\n",
    "    def select_action(self, state):\n",
    "        epsilon = self.epsilon_end + (self.epsilon_start - self.epsilon_end) * np.exp(-1. * self.steps_done / self.epsilon_decay)\n",
    "        self.steps_done += 1\n",
    "        action, self.hidden = self.model.act(state, self.hidden, epsilon)\n",
    "        return action\n",
    "\n",
    "    def optimize_model(self):\n",
    "        if len(self.replay_buffer) < self.batch_size:\n",
    "            return\n",
    "        \n",
    "        self.model.train()\n",
    "        print(\"Optimizing model...\")\n",
    "        states, actions, rewards, next_states, dones = self.replay_buffer.sample(self.batch_size)\n",
    "\n",
    "        states = torch.FloatTensor(states).unsqueeze(1).to(device)\n",
    "        actions = torch.LongTensor(actions).to(device)\n",
    "        rewards = torch.FloatTensor(rewards).to(device)\n",
    "        next_states = torch.FloatTensor(next_states).unsqueeze(1).to(device)\n",
    "        dones = torch.FloatTensor(dones).to(device)\n",
    "        \n",
    "        quantiles = torch.rand(self.batch_size, self.n_quantiles).to(device)\n",
    "\n",
    "        hidden = (torch.zeros(1, self.batch_size, self.hidden_size).to(device),\n",
    "                  torch.zeros(1, self.batch_size, self.hidden_size).to(device))\n",
    "\n",
    "        current_q, _ = self.model(states, quantiles, hidden)\n",
    "        current_q = current_q.gather(2, actions.unsqueeze(-1).unsqueeze(-1).expand(-1, self.n_quantiles, -1)).squeeze(-1)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            next_hidden = (torch.zeros(1, self.batch_size, self.hidden_size).to(device),\n",
    "                           torch.zeros(1, self.batch_size, self.hidden_size).to(device))\n",
    "            next_quantiles = torch.rand(self.batch_size, self.n_quantiles).to(device)\n",
    "            next_q, _ = self.model_target(next_states, next_quantiles, next_hidden)\n",
    "            next_q = next_q.max(2)[0]\n",
    "            target_q = rewards.unsqueeze(1) + self.gamma * next_q * (1 - dones.unsqueeze(1))\n",
    "\n",
    "        td_errors = target_q.unsqueeze(1) - current_q\n",
    "        huber_loss = F.smooth_l1_loss(current_q, target_q.unsqueeze(1), reduction='none')\n",
    "        quantile_loss = (torch.abs(quantiles.unsqueeze(-1) - (td_errors.detach() < 0).float()) * huber_loss).mean()\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        quantile_loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        self.update_counter += 1\n",
    "        if self.update_counter % self.target_update_freq == 0:\n",
    "            self.update_target_network()\n",
    "\n",
    "        return quantile_loss.item()\n",
    "\n",
    "    def update_target_network(self):\n",
    "        self.model_target.load_state_dict(self.model.state_dict())\n",
    "\n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        self.replay_buffer.push(state, action, reward, next_state, done)\n",
    "\n",
    "    def save(self, path):\n",
    "        torch.save(self.model.state_dict(), path)\n",
    "\n",
    "    def load(self, path):\n",
    "        self.model.load_state_dict(torch.load(path))\n",
    "        self.model.eval()\n",
    "\n",
    "    def reset(self):\n",
    "        self.steps_done = 0\n",
    "    \n",
    "    def pop(self, count=1):\n",
    "        self.replay_buffer.pop(count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simplify_state(state):\n",
    "    agent_x, agent_y = get_agent_position(state)\n",
    "    obstacles = get_nearby_obstacles(state, agent_x, agent_y)\n",
    "    timbers = get_nearby_timbers(state, agent_x, agent_y)\n",
    "    \n",
    "    simplified_state = ((agent_x, agent_y), tuple(set(obstacles)), tuple(set(timbers)))\n",
    "    \n",
    "    return simplified_state\n",
    "\n",
    "def get_agent_position(state):\n",
    "    for i, row in enumerate(state):\n",
    "        for j, cell in enumerate(row):\n",
    "            if cell == 1:\n",
    "                return j, i\n",
    "    return state.shape[1] // 2, state.shape[0] - 1\n",
    "    \n",
    "            \n",
    "def get_nearby_obstacles(state, agent_x, agent_y):\n",
    "    obstacles = []\n",
    "    for i, row in enumerate(state):\n",
    "        for j, cell in enumerate(row):\n",
    "            if cell == 2:\n",
    "                obstacles.append((j, i))\n",
    "    return obstacles\n",
    "\n",
    "def get_nearby_timbers(state, agent_x, agent_y):\n",
    "    timbers = []\n",
    "    for i, row in enumerate(state):\n",
    "        for j, cell in enumerate(row):\n",
    "            if cell == 3:\n",
    "                timbers.append((j, i))\n",
    "    return timbers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model is ready to train\n",
      "Step: 0, Action: 0, Reward: 2.0, Total Reward: 2.0, Loss: None\n",
      "Step: 1, Action: 2, Reward: 0.25, Total Reward: 2.25, Loss: None\n",
      "Step: 2, Action: 2, Reward: 0.25, Total Reward: 2.5, Loss: None\n",
      "Step: 3, Action: 2, Reward: 0.25, Total Reward: 2.75, Loss: None\n",
      "Step: 4, Action: 0, Reward: 2.0, Total Reward: 4.75, Loss: None\n",
      "Step: 5, Action: 0, Reward: -8.0, Total Reward: -3.25, Loss: None\n",
      "Step: 6, Action: 3, Reward: 0.25, Total Reward: -3.0, Loss: None\n",
      "Step: 7, Action: 3, Reward: 0.25, Total Reward: -2.75, Loss: None\n",
      "Step: 8, Action: 3, Reward: -9.75, Total Reward: -12.5, Loss: None\n",
      "Step: 9, Action: 1, Reward: 0.0, Total Reward: -12.5, Loss: None\n",
      "Step: 10, Action: 3, Reward: 0.25, Total Reward: -12.25, Loss: None\n",
      "Step: 11, Action: 4, Reward: 0.2, Total Reward: -12.05, Loss: None\n",
      "Step: 12, Action: 1, Reward: 0.0, Total Reward: -12.05, Loss: None\n",
      "Step: 13, Action: 1, Reward: -10.0, Total Reward: -22.05, Loss: None\n",
      "Step: 14, Action: 2, Reward: 0.25, Total Reward: -21.8, Loss: None\n",
      "Step: 15, Action: 0, Reward: 2.1, Total Reward: -19.7, Loss: None\n",
      "Step: 16, Action: 4, Reward: 0.3, Total Reward: -19.4, Loss: None\n",
      "Step: 17, Action: 4, Reward: -9.8, Total Reward: -29.2, Loss: None\n",
      "Step: 18, Action: 1, Reward: 0.0, Total Reward: -29.2, Loss: None\n",
      "Step: 19, Action: 1, Reward: 0.0, Total Reward: -29.2, Loss: None\n",
      "Step: 20, Action: 0, Reward: 2.0, Total Reward: -27.2, Loss: None\n",
      "Step: 21, Action: 3, Reward: 0.25, Total Reward: -26.95, Loss: None\n",
      "Step: 22, Action: 2, Reward: 0.25, Total Reward: -26.7, Loss: None\n",
      "Step: 23, Action: 1, Reward: 0.0, Total Reward: -26.7, Loss: None\n",
      "Step: 24, Action: 1, Reward: -10.0, Total Reward: -36.7, Loss: None\n",
      "Step: 25, Action: 1, Reward: -10.0, Total Reward: -46.7, Loss: None\n",
      "Step: 26, Action: 4, Reward: 0.2, Total Reward: -46.5, Loss: None\n",
      "Step: 27, Action: 2, Reward: 0.25, Total Reward: -46.25, Loss: None\n",
      "\n",
      "episode: 0, reward: -146.25\n",
      "\n",
      "Model saved\n",
      "Step: 0, Action: 0, Reward: 2.0, Total Reward: 2.0, Loss: None\n",
      "Step: 1, Action: 3, Reward: 0.25, Total Reward: 2.25, Loss: None\n",
      "Step: 2, Action: 1, Reward: 0.0, Total Reward: 2.25, Loss: None\n",
      "Step: 3, Action: 2, Reward: 0.25, Total Reward: 2.5, Loss: None\n",
      "Step: 4, Action: 3, Reward: 0.25, Total Reward: 2.75, Loss: None\n",
      "Step: 5, Action: 0, Reward: 2.0, Total Reward: 4.75, Loss: None\n",
      "Optimizing model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ikkiren\\AppData\\Local\\Temp\\ipykernel_14376\\1696763271.py:105: UserWarning: Using a target size (torch.Size([32, 1, 32])) that is different to the input size (torch.Size([32, 32])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  huber_loss = F.smooth_l1_loss(current_q, target_q.unsqueeze(1), reduction='none')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 6, Action: 4, Reward: 0.2, Total Reward: 4.95, Loss: 2.746652364730835\n",
      "Optimizing model...\n",
      "Step: 7, Action: 0, Reward: 2.0, Total Reward: 6.95, Loss: 2.687303304672241\n",
      "Optimizing model...\n",
      "Step: 8, Action: 2, Reward: 0.25, Total Reward: 7.2, Loss: 2.721449851989746\n",
      "Optimizing model...\n",
      "Step: 9, Action: 3, Reward: 0.25, Total Reward: 7.45, Loss: 2.374948501586914\n",
      "\n",
      "episode: 1, reward: -92.55\n",
      "\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: 2.0, Total Reward: 2.0, Loss: 4.115664005279541\n",
      "Optimizing model...\n",
      "Step: 1, Action: 1, Reward: 0.0, Total Reward: 2.0, Loss: 4.0185089111328125\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: 2.0, Total Reward: 4.0, Loss: 3.8132288455963135\n",
      "Optimizing model...\n",
      "Step: 3, Action: 2, Reward: 0.25, Total Reward: 4.25, Loss: 3.7864651679992676\n",
      "Optimizing model...\n",
      "Step: 4, Action: 1, Reward: 0.0, Total Reward: 4.25, Loss: 4.2097978591918945\n",
      "Optimizing model...\n",
      "Step: 5, Action: 4, Reward: 0.2, Total Reward: 4.45, Loss: 2.673335552215576\n",
      "Optimizing model...\n",
      "Step: 6, Action: 4, Reward: 0.3, Total Reward: 4.75, Loss: 2.6553902626037598\n",
      "Optimizing model...\n",
      "Step: 7, Action: 3, Reward: 0.25, Total Reward: 5.0, Loss: 2.450993061065674\n",
      "Optimizing model...\n",
      "Step: 8, Action: 0, Reward: 2.1, Total Reward: 7.1, Loss: 2.215276002883911\n",
      "Optimizing model...\n",
      "Step: 9, Action: 1, Reward: 0.0, Total Reward: 7.1, Loss: 3.7015442848205566\n",
      "Optimizing model...\n",
      "Step: 10, Action: 3, Reward: 0.25, Total Reward: 7.35, Loss: 4.065572738647461\n",
      "Optimizing model...\n",
      "Step: 11, Action: 4, Reward: 0.2, Total Reward: 7.55, Loss: 3.5299296379089355\n",
      "Optimizing model...\n",
      "Step: 12, Action: 0, Reward: 2.0, Total Reward: 9.55, Loss: 3.9741783142089844\n",
      "Optimizing model...\n",
      "Step: 13, Action: 4, Reward: 0.2, Total Reward: 9.75, Loss: 4.231616497039795\n",
      "Optimizing model...\n",
      "Step: 14, Action: 2, Reward: 0.25, Total Reward: 10.0, Loss: 3.8024373054504395\n",
      "Optimizing model...\n",
      "Step: 15, Action: 4, Reward: 0.2, Total Reward: 10.2, Loss: 2.24581241607666\n",
      "Optimizing model...\n",
      "Step: 16, Action: 4, Reward: 0.3, Total Reward: 10.5, Loss: 0.5600159168243408\n",
      "Optimizing model...\n",
      "Step: 17, Action: 4, Reward: -9.8, Total Reward: 0.6999999999999993, Loss: 3.78538179397583\n",
      "Optimizing model...\n",
      "Step: 18, Action: 2, Reward: 0.25, Total Reward: 0.9499999999999993, Loss: 2.747511863708496\n",
      "Optimizing model...\n",
      "Step: 19, Action: 3, Reward: 0.25, Total Reward: 1.1999999999999993, Loss: 0.43616625666618347\n",
      "Optimizing model...\n",
      "Step: 20, Action: 4, Reward: 0.2, Total Reward: 1.3999999999999992, Loss: 0.6846853494644165\n",
      "Optimizing model...\n",
      "Step: 21, Action: 4, Reward: -9.8, Total Reward: -8.400000000000002, Loss: 2.596723794937134\n",
      "Optimizing model...\n",
      "Step: 22, Action: 4, Reward: 0.2, Total Reward: -8.200000000000003, Loss: 3.631260395050049\n",
      "Optimizing model...\n",
      "Step: 23, Action: 2, Reward: 0.25, Total Reward: -7.950000000000003, Loss: 1.9715569019317627\n",
      "Optimizing model...\n",
      "Step: 24, Action: 3, Reward: 0.25, Total Reward: -7.700000000000003, Loss: 2.0555977821350098\n",
      "Optimizing model...\n",
      "Step: 25, Action: 3, Reward: 0.25, Total Reward: -7.450000000000003, Loss: 0.7191343307495117\n",
      "Optimizing model...\n",
      "Step: 26, Action: 2, Reward: 0.25, Total Reward: -7.200000000000003, Loss: 2.206733226776123\n",
      "Optimizing model...\n",
      "Step: 27, Action: 3, Reward: 0.25, Total Reward: -6.950000000000003, Loss: 2.2575111389160156\n",
      "Optimizing model...\n",
      "Step: 28, Action: 1, Reward: 0.0, Total Reward: -6.950000000000003, Loss: 0.5719422698020935\n",
      "Optimizing model...\n",
      "Step: 29, Action: 4, Reward: 0.2, Total Reward: -6.750000000000003, Loss: 3.437255382537842\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 30, Action: 2, Reward: 0.25, Total Reward: -6.500000000000003, Loss: 3.862760066986084\n",
      "Optimizing model...\n",
      "Step: 31, Action: 3, Reward: 0.25, Total Reward: -6.250000000000003, Loss: 2.0114221572875977\n",
      "Optimizing model...\n",
      "Step: 32, Action: 1, Reward: 0.0, Total Reward: -6.250000000000003, Loss: 2.4690604209899902\n",
      "Optimizing model...\n",
      "Step: 33, Action: 2, Reward: 0.25, Total Reward: -6.000000000000003, Loss: 3.65787935256958\n",
      "Optimizing model...\n",
      "Step: 34, Action: 3, Reward: 0.25, Total Reward: -5.750000000000003, Loss: 0.49269476532936096\n",
      "Optimizing model...\n",
      "Step: 35, Action: 0, Reward: 2.0, Total Reward: -3.7500000000000027, Loss: 3.7559738159179688\n",
      "Optimizing model...\n",
      "Step: 36, Action: 0, Reward: 2.0, Total Reward: -1.7500000000000027, Loss: 2.64562726020813\n",
      "\n",
      "episode: 2, reward: -101.75\n",
      "\n",
      "Optimizing model...\n",
      "Step: 0, Action: 2, Reward: 0.25, Total Reward: 0.25, Loss: 3.4339656829833984\n",
      "Optimizing model...\n",
      "Step: 1, Action: 2, Reward: -9.75, Total Reward: -9.5, Loss: 0.8088604211807251\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: 2.0, Total Reward: -7.5, Loss: 1.9576820135116577\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 4, Reward: 0.3, Total Reward: -7.2, Loss: 2.1522860527038574\n",
      "Optimizing model...\n",
      "Step: 4, Action: 1, Reward: 0.0, Total Reward: -7.2, Loss: 3.6191511154174805\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 5, Action: 3, Reward: 0.25, Total Reward: -6.95, Loss: 3.7340831756591797\n",
      "Optimizing model...\n",
      "Step: 6, Action: 2, Reward: 0.25, Total Reward: -6.7, Loss: 2.531838893890381\n",
      "Optimizing model...\n",
      "Step: 7, Action: 4, Reward: 0.2, Total Reward: -6.5, Loss: 5.205943584442139\n",
      "Optimizing model...\n",
      "Step: 8, Action: 2, Reward: 0.25, Total Reward: -6.25, Loss: 2.2803971767425537\n",
      "Optimizing model...\n",
      "Step: 9, Action: 0, Reward: 2.0, Total Reward: -4.25, Loss: 1.9582942724227905\n",
      "Optimizing model...\n",
      "Step: 10, Action: 0, Reward: 2.0, Total Reward: -2.25, Loss: 3.1219277381896973\n",
      "Optimizing model...\n",
      "Step: 11, Action: 4, Reward: 0.2, Total Reward: -2.05, Loss: 0.7087196707725525\n",
      "Optimizing model...\n",
      "Step: 12, Action: 0, Reward: 2.0, Total Reward: -0.04999999999999982, Loss: 5.137425899505615\n",
      "Optimizing model...\n",
      "Step: 13, Action: 0, Reward: 2.0, Total Reward: 1.9500000000000002, Loss: 0.4943581223487854\n",
      "Optimizing model...\n",
      "Step: 14, Action: 1, Reward: 0.0, Total Reward: 1.9500000000000002, Loss: 0.6244453191757202\n",
      "Optimizing model...\n",
      "Step: 15, Action: 1, Reward: -10.0, Total Reward: -8.05, Loss: 2.37123441696167\n",
      "Optimizing model...\n",
      "Step: 16, Action: 1, Reward: 0.0, Total Reward: -8.05, Loss: 3.532541513442993\n",
      "Optimizing model...\n",
      "Step: 17, Action: 1, Reward: 0.0, Total Reward: -8.05, Loss: 2.1346983909606934\n",
      "Optimizing model...\n",
      "Step: 18, Action: 4, Reward: 0.2, Total Reward: -7.8500000000000005, Loss: 0.7190492749214172\n",
      "Optimizing model...\n",
      "Step: 19, Action: 4, Reward: -9.8, Total Reward: -17.650000000000002, Loss: 3.3319945335388184\n",
      "Optimizing model...\n",
      "Step: 20, Action: 2, Reward: 0.25, Total Reward: -17.400000000000002, Loss: 2.141339063644409\n",
      "Optimizing model...\n",
      "Step: 21, Action: 3, Reward: 0.25, Total Reward: -17.150000000000002, Loss: 1.9624443054199219\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 22, Action: 3, Reward: 0.25, Total Reward: -16.900000000000002, Loss: 2.0156145095825195\n",
      "Optimizing model...\n",
      "Step: 23, Action: 4, Reward: 0.2, Total Reward: -16.700000000000003, Loss: 2.1600799560546875\n",
      "Optimizing model...\n",
      "Step: 24, Action: 2, Reward: 0.25, Total Reward: -16.450000000000003, Loss: 2.0886855125427246\n",
      "\n",
      "episode: 3, reward: -116.45\n",
      "\n",
      "Optimizing model...\n",
      "Step: 0, Action: 4, Reward: 0.2, Total Reward: 0.2, Loss: 3.8166236877441406\n",
      "Optimizing model...\n",
      "Step: 1, Action: 4, Reward: 0.2, Total Reward: 0.4, Loss: 5.086162567138672\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: 2.0, Total Reward: 2.4, Loss: 3.6883459091186523\n",
      "Optimizing model...\n",
      "Step: 3, Action: 2, Reward: 0.25, Total Reward: 2.65, Loss: 2.4097490310668945\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: 2.0, Total Reward: 4.65, Loss: 2.6388888359069824\n",
      "Optimizing model...\n",
      "Step: 5, Action: 0, Reward: 2.0, Total Reward: 6.65, Loss: 3.3137998580932617\n",
      "Optimizing model...\n",
      "Step: 6, Action: 3, Reward: 0.25, Total Reward: 6.9, Loss: 0.39751768112182617\n",
      "Optimizing model...\n",
      "Step: 7, Action: 1, Reward: 0.0, Total Reward: 6.9, Loss: 1.9409873485565186\n",
      "\n",
      "episode: 4, reward: -93.1\n",
      "\n",
      "Optimizing model...\n",
      "Step: 0, Action: 3, Reward: 0.25, Total Reward: 0.25, Loss: 2.0903031826019287\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: 2.0, Total Reward: 2.25, Loss: 2.1296498775482178\n",
      "Optimizing model...\n",
      "Step: 2, Action: 3, Reward: 0.25, Total Reward: 2.5, Loss: 3.457324504852295\n",
      "Optimizing model...\n",
      "Step: 3, Action: 1, Reward: 0.0, Total Reward: 2.5, Loss: 3.5067920684814453\n",
      "Optimizing model...\n",
      "Step: 4, Action: 2, Reward: 0.25, Total Reward: 2.75, Loss: 3.9011175632476807\n",
      "Optimizing model...\n",
      "Step: 5, Action: 2, Reward: 0.25, Total Reward: 3.0, Loss: 2.266162872314453\n",
      "Optimizing model...\n",
      "Step: 6, Action: 0, Reward: 2.1, Total Reward: 5.1, Loss: 0.2817528247833252\n",
      "Optimizing model...\n",
      "Step: 7, Action: 4, Reward: 0.2, Total Reward: 5.3, Loss: 2.231511116027832\n",
      "Optimizing model...\n",
      "Step: 8, Action: 0, Reward: 2.0, Total Reward: 7.3, Loss: 2.2780399322509766\n",
      "Optimizing model...\n",
      "Step: 9, Action: 2, Reward: 0.25, Total Reward: 7.55, Loss: 1.894221544265747\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 10, Action: 2, Reward: 0.25, Total Reward: 7.8, Loss: 3.4679222106933594\n",
      "Optimizing model...\n",
      "Step: 11, Action: 1, Reward: 0.0, Total Reward: 7.8, Loss: 0.35574406385421753\n",
      "Optimizing model...\n",
      "Step: 12, Action: 4, Reward: 0.2, Total Reward: 8.0, Loss: 0.7248694896697998\n",
      "Optimizing model...\n",
      "Step: 13, Action: 4, Reward: -9.8, Total Reward: -1.8000000000000007, Loss: 2.1774425506591797\n",
      "Optimizing model...\n",
      "Step: 14, Action: 0, Reward: 2.0, Total Reward: 0.1999999999999993, Loss: 3.524887800216675\n",
      "Optimizing model...\n",
      "Step: 15, Action: 2, Reward: 0.35, Total Reward: 0.5499999999999993, Loss: 0.7948392629623413\n",
      "Optimizing model...\n",
      "Step: 16, Action: 0, Reward: 2.0, Total Reward: 2.5499999999999994, Loss: 3.4341421127319336\n",
      "Optimizing model...\n",
      "Step: 17, Action: 2, Reward: 0.25, Total Reward: 2.7999999999999994, Loss: 4.713062286376953\n",
      "Optimizing model...\n",
      "Step: 18, Action: 0, Reward: 2.0, Total Reward: 4.799999999999999, Loss: 3.175191879272461\n",
      "Optimizing model...\n",
      "Step: 19, Action: 4, Reward: 0.2, Total Reward: 4.999999999999999, Loss: 1.8829758167266846\n",
      "Optimizing model...\n",
      "Step: 20, Action: 2, Reward: 0.25, Total Reward: 5.249999999999999, Loss: 5.224114418029785\n",
      "Optimizing model...\n",
      "Step: 21, Action: 0, Reward: 2.0, Total Reward: 7.249999999999999, Loss: 3.328007221221924\n",
      "Optimizing model...\n",
      "Step: 22, Action: 3, Reward: 0.35, Total Reward: 7.599999999999999, Loss: 3.23881196975708\n",
      "Optimizing model...\n",
      "Step: 23, Action: 3, Reward: -9.75, Total Reward: -2.1500000000000012, Loss: 1.974521279335022\n",
      "Optimizing model...\n",
      "Step: 24, Action: 0, Reward: 2.0, Total Reward: -0.15000000000000124, Loss: 3.910371780395508\n",
      "Optimizing model...\n",
      "Step: 25, Action: 2, Reward: 0.25, Total Reward: 0.09999999999999876, Loss: 1.948468804359436\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 26, Action: 0, Reward: 2.0, Total Reward: 2.0999999999999988, Loss: 5.751108646392822\n",
      "Optimizing model...\n",
      "Step: 27, Action: 2, Reward: 0.25, Total Reward: 2.3499999999999988, Loss: 3.3446450233459473\n",
      "Optimizing model...\n",
      "Step: 28, Action: 0, Reward: 2.0, Total Reward: 4.349999999999999, Loss: 2.274716377258301\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 29, Action: 2, Reward: 0.25, Total Reward: 4.599999999999999, Loss: 2.1784558296203613\n",
      "Optimizing model...\n",
      "Step: 30, Action: 1, Reward: 0.0, Total Reward: 4.599999999999999, Loss: 1.7836718559265137\n",
      "Optimizing model...\n",
      "Step: 31, Action: 3, Reward: 0.25, Total Reward: 4.849999999999999, Loss: 1.7550365924835205\n",
      "Optimizing model...\n",
      "Step: 32, Action: 2, Reward: 0.35, Total Reward: 5.199999999999998, Loss: 0.4297906160354614\n",
      "Optimizing model...\n",
      "Step: 33, Action: 0, Reward: 2.0, Total Reward: 7.199999999999998, Loss: 3.7576005458831787\n",
      "Optimizing model...\n",
      "Step: 34, Action: 1, Reward: 0.0, Total Reward: 7.199999999999998, Loss: 1.7244030237197876\n",
      "Optimizing model...\n",
      "Step: 35, Action: 3, Reward: 0.25, Total Reward: 7.449999999999998, Loss: 0.2736457288265228\n",
      "Optimizing model...\n",
      "Step: 36, Action: 2, Reward: 0.25, Total Reward: 7.699999999999998, Loss: 0.464931845664978\n",
      "Optimizing model...\n",
      "Step: 37, Action: 4, Reward: 0.2, Total Reward: 7.899999999999999, Loss: 1.9970667362213135\n",
      "Optimizing model...\n",
      "Step: 38, Action: 4, Reward: 0.2, Total Reward: 8.099999999999998, Loss: 1.7096304893493652\n",
      "Optimizing model...\n",
      "Step: 39, Action: 3, Reward: 0.25, Total Reward: 8.349999999999998, Loss: 1.9784212112426758\n",
      "Optimizing model...\n",
      "Step: 40, Action: 3, Reward: -9.75, Total Reward: -1.4000000000000021, Loss: 0.5829957723617554\n",
      "\n",
      "episode: 5, reward: -101.4\n",
      "\n",
      "Optimizing model...\n",
      "Step: 0, Action: 2, Reward: 0.25, Total Reward: 0.25, Loss: 0.5493021011352539\n",
      "Optimizing model...\n",
      "Step: 1, Action: 3, Reward: 0.25, Total Reward: 0.5, Loss: 4.194822311401367\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: 2.0, Total Reward: 2.5, Loss: 1.974948763847351\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: 2.0, Total Reward: 4.5, Loss: 2.1192805767059326\n",
      "Optimizing model...\n",
      "Step: 4, Action: 4, Reward: 0.2, Total Reward: 4.7, Loss: 5.076964378356934\n",
      "Optimizing model...\n",
      "Step: 5, Action: 2, Reward: 0.25, Total Reward: 4.95, Loss: 0.402047723531723\n",
      "Optimizing model...\n",
      "Step: 6, Action: 1, Reward: 0.0, Total Reward: 4.95, Loss: 2.231544017791748\n",
      "Optimizing model...\n",
      "Step: 7, Action: 0, Reward: 2.0, Total Reward: 6.95, Loss: 4.700922966003418\n",
      "Optimizing model...\n",
      "Step: 8, Action: 3, Reward: 0.25, Total Reward: 7.2, Loss: 1.9727394580841064\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 9, Action: 2, Reward: 0.25, Total Reward: 7.45, Loss: 4.0550665855407715\n",
      "Optimizing model...\n",
      "Step: 10, Action: 1, Reward: 0.0, Total Reward: 7.45, Loss: 2.0371594429016113\n",
      "Optimizing model...\n",
      "Step: 11, Action: 3, Reward: 0.25, Total Reward: 7.7, Loss: 2.3974032402038574\n",
      "Optimizing model...\n",
      "Step: 12, Action: 0, Reward: 2.0, Total Reward: 9.7, Loss: 0.7794111967086792\n",
      "Optimizing model...\n",
      "Step: 13, Action: 2, Reward: 0.25, Total Reward: 9.95, Loss: 6.504591941833496\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 14, Action: 0, Reward: 2.0, Total Reward: 11.95, Loss: 5.258697986602783\n",
      "Optimizing model...\n",
      "Step: 15, Action: 1, Reward: 0.0, Total Reward: 11.95, Loss: 0.45728302001953125\n",
      "Optimizing model...\n",
      "Step: 16, Action: 0, Reward: 2.0, Total Reward: 13.95, Loss: 2.184995412826538\n",
      "Optimizing model...\n",
      "Step: 17, Action: 0, Reward: 2.0, Total Reward: 15.95, Loss: 0.7723339200019836\n",
      "Optimizing model...\n",
      "Step: 18, Action: 3, Reward: 0.25, Total Reward: 16.2, Loss: 1.8378446102142334\n",
      "Optimizing model...\n",
      "Step: 19, Action: 4, Reward: 0.2, Total Reward: 16.4, Loss: 3.4154531955718994\n",
      "Optimizing model...\n",
      "Step: 20, Action: 4, Reward: -9.8, Total Reward: 6.599999999999998, Loss: 1.9492785930633545\n",
      "Optimizing model...\n",
      "Step: 21, Action: 2, Reward: 0.25, Total Reward: 6.849999999999998, Loss: 3.1044301986694336\n",
      "Optimizing model...\n",
      "Step: 22, Action: 0, Reward: 2.0, Total Reward: 8.849999999999998, Loss: 4.027022838592529\n",
      "Optimizing model...\n",
      "Step: 23, Action: 0, Reward: 2.0, Total Reward: 10.849999999999998, Loss: 3.821676254272461\n",
      "Optimizing model...\n",
      "Step: 24, Action: 3, Reward: 0.25, Total Reward: 11.099999999999998, Loss: 1.9648067951202393\n",
      "Optimizing model...\n",
      "Step: 25, Action: 2, Reward: 0.25, Total Reward: 11.349999999999998, Loss: 3.6177945137023926\n",
      "Optimizing model...\n",
      "Step: 26, Action: 4, Reward: 0.2, Total Reward: 11.549999999999997, Loss: 3.4621071815490723\n",
      "Optimizing model...\n",
      "Step: 27, Action: 1, Reward: 0.0, Total Reward: 11.549999999999997, Loss: 0.35744160413742065\n",
      "Optimizing model...\n",
      "Step: 28, Action: 1, Reward: 0.0, Total Reward: 11.549999999999997, Loss: 1.967658281326294\n",
      "Optimizing model...\n",
      "Step: 29, Action: 4, Reward: 0.2, Total Reward: 11.749999999999996, Loss: 1.9358696937561035\n",
      "Optimizing model...\n",
      "Step: 30, Action: 0, Reward: 2.0, Total Reward: 13.749999999999996, Loss: 3.3187928199768066\n",
      "Optimizing model...\n",
      "Step: 31, Action: 1, Reward: 0.0, Total Reward: 13.749999999999996, Loss: 1.8542429208755493\n",
      "Optimizing model...\n",
      "Step: 32, Action: 4, Reward: 0.2, Total Reward: 13.949999999999996, Loss: 2.4099833965301514\n",
      "Optimizing model...\n",
      "Step: 33, Action: 0, Reward: 2.0, Total Reward: 15.949999999999996, Loss: 1.9217374324798584\n",
      "Optimizing model...\n",
      "Step: 34, Action: 2, Reward: 0.25, Total Reward: 16.199999999999996, Loss: 3.4807064533233643\n",
      "Optimizing model...\n",
      "Step: 35, Action: 0, Reward: 2.0, Total Reward: 18.199999999999996, Loss: 5.029118061065674\n",
      "Optimizing model...\n",
      "Step: 36, Action: 1, Reward: 0.0, Total Reward: 18.199999999999996, Loss: 2.1428470611572266\n",
      "Optimizing model...\n",
      "Step: 37, Action: 1, Reward: 0.0, Total Reward: 18.199999999999996, Loss: 3.495669364929199\n",
      "Optimizing model...\n",
      "Step: 38, Action: 2, Reward: 0.35, Total Reward: 18.549999999999997, Loss: 3.7101755142211914\n",
      "Optimizing model...\n",
      "Step: 39, Action: 4, Reward: 0.2, Total Reward: 18.749999999999996, Loss: 1.7327594757080078\n",
      "Optimizing model...\n",
      "Step: 40, Action: 3, Reward: 0.25, Total Reward: 18.999999999999996, Loss: 5.493903160095215\n",
      "Optimizing model...\n",
      "Step: 41, Action: 2, Reward: 0.35, Total Reward: 19.349999999999998, Loss: 0.4690932035446167\n",
      "Optimizing model...\n",
      "Step: 42, Action: 3, Reward: 0.25, Total Reward: 19.599999999999998, Loss: 0.32150787115097046\n",
      "Optimizing model...\n",
      "Step: 43, Action: 0, Reward: 2.0, Total Reward: 21.599999999999998, Loss: 2.537226676940918\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 44, Action: 0, Reward: 2.0, Total Reward: 23.599999999999998, Loss: 0.5099499225616455\n",
      "Optimizing model...\n",
      "Step: 45, Action: 0, Reward: 2.0, Total Reward: 25.599999999999998, Loss: 0.5921270847320557\n",
      "\n",
      "episode: 6, reward: -74.4\n",
      "\n",
      "Optimizing model...\n",
      "Step: 0, Action: 2, Reward: 0.35, Total Reward: 0.35, Loss: 0.24362829327583313\n",
      "Optimizing model...\n",
      "Step: 1, Action: 4, Reward: 0.2, Total Reward: 0.55, Loss: 3.364528179168701\n",
      "Optimizing model...\n",
      "Step: 2, Action: 2, Reward: 0.25, Total Reward: 0.8, Loss: 3.705390453338623\n",
      "Optimizing model...\n",
      "Step: 3, Action: 1, Reward: 0.0, Total Reward: 0.8, Loss: 0.6194819211959839\n",
      "Optimizing model...\n",
      "Step: 4, Action: 2, Reward: 0.25, Total Reward: 1.05, Loss: 0.6865006685256958\n",
      "Optimizing model...\n",
      "Step: 5, Action: 2, Reward: -9.75, Total Reward: -8.7, Loss: 4.648846626281738\n",
      "Optimizing model...\n",
      "Step: 6, Action: 1, Reward: 0.0, Total Reward: -8.7, Loss: 3.595421314239502\n",
      "Optimizing model...\n",
      "Step: 7, Action: 4, Reward: 0.2, Total Reward: -8.5, Loss: 0.6171196699142456\n",
      "Optimizing model...\n",
      "Step: 8, Action: 2, Reward: 0.25, Total Reward: -8.25, Loss: 2.0094916820526123\n",
      "Optimizing model...\n",
      "Step: 9, Action: 2, Reward: 0.25, Total Reward: -8.0, Loss: 0.7631045579910278\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 10, Action: 0, Reward: 2.0, Total Reward: -6.0, Loss: 1.8178136348724365\n",
      "Optimizing model...\n",
      "Step: 11, Action: 3, Reward: 0.25, Total Reward: -5.75, Loss: 1.591644048690796\n",
      "Optimizing model...\n",
      "Step: 12, Action: 3, Reward: 0.25, Total Reward: -5.5, Loss: 0.36369335651397705\n",
      "Optimizing model...\n",
      "Step: 13, Action: 1, Reward: 0.1, Total Reward: -5.4, Loss: 2.138028144836426\n",
      "Optimizing model...\n",
      "Step: 14, Action: 4, Reward: 0.2, Total Reward: -5.2, Loss: 3.4877071380615234\n",
      "Optimizing model...\n",
      "Step: 15, Action: 0, Reward: 2.0, Total Reward: -3.2, Loss: 1.9877095222473145\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 16, Action: 0, Reward: 2.0, Total Reward: -1.2000000000000002, Loss: 0.41360095143318176\n",
      "Optimizing model...\n",
      "Step: 17, Action: 2, Reward: 0.25, Total Reward: -0.9500000000000002, Loss: 0.3043777644634247\n",
      "Optimizing model...\n",
      "Step: 18, Action: 0, Reward: 2.0, Total Reward: 1.0499999999999998, Loss: 3.6430115699768066\n",
      "Optimizing model...\n",
      "Step: 19, Action: 0, Reward: -8.0, Total Reward: -6.95, Loss: 3.311008930206299\n",
      "Optimizing model...\n",
      "Step: 20, Action: 3, Reward: 0.25, Total Reward: -6.7, Loss: 3.7466306686401367\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 21, Action: 0, Reward: 2.0, Total Reward: -4.7, Loss: 4.796721935272217\n",
      "Optimizing model...\n",
      "Step: 22, Action: 4, Reward: 0.2, Total Reward: -4.5, Loss: 1.5819640159606934\n",
      "Optimizing model...\n",
      "Step: 23, Action: 2, Reward: 0.25, Total Reward: -4.25, Loss: 3.3053369522094727\n",
      "\n",
      "episode: 7, reward: -104.25\n",
      "\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: 2.0, Total Reward: 2.0, Loss: 0.2963532507419586\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: 2.0, Total Reward: 4.0, Loss: 2.2139105796813965\n",
      "Optimizing model...\n",
      "Step: 2, Action: 1, Reward: 0.0, Total Reward: 4.0, Loss: 2.096320152282715\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: 2.0, Total Reward: 6.0, Loss: 3.379655599594116\n",
      "Optimizing model...\n",
      "Step: 4, Action: 2, Reward: 0.25, Total Reward: 6.25, Loss: 3.8910205364227295\n",
      "Optimizing model...\n",
      "Step: 5, Action: 2, Reward: 0.35, Total Reward: 6.6, Loss: 0.504808783531189\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 6, Action: 0, Reward: 2.1, Total Reward: 8.7, Loss: 3.7479724884033203\n",
      "Optimizing model...\n",
      "Step: 7, Action: 2, Reward: 0.25, Total Reward: 8.95, Loss: 1.8250718116760254\n",
      "\n",
      "episode: 8, reward: -91.05\n",
      "\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: 2.0, Total Reward: 2.0, Loss: 1.96537184715271\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: -8.0, Total Reward: -6.0, Loss: 1.9947564601898193\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: 2.0, Total Reward: -4.0, Loss: 2.517653703689575\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: -7.9, Total Reward: -11.9, Loss: 5.416782379150391\n",
      "Optimizing model...\n",
      "Step: 4, Action: 2, Reward: 0.25, Total Reward: -11.65, Loss: 3.585134267807007\n",
      "Optimizing model...\n",
      "Step: 5, Action: 3, Reward: 0.25, Total Reward: -11.4, Loss: 1.9777185916900635\n",
      "Optimizing model...\n",
      "Step: 6, Action: 2, Reward: 0.25, Total Reward: -11.15, Loss: 2.303997755050659\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 7, Action: 0, Reward: 2.1, Total Reward: -9.05, Loss: 3.8208231925964355\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 8, Action: 0, Reward: 2.0, Total Reward: -7.050000000000001, Loss: 3.1749844551086426\n",
      "\n",
      "episode: 9, reward: -107.05\n",
      "\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: -7.9, Total Reward: -7.9, Loss: 0.21193230152130127\n",
      "Optimizing model...\n",
      "Step: 1, Action: 2, Reward: 0.25, Total Reward: -7.65, Loss: 3.5661582946777344\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 3, Reward: 0.25, Total Reward: -7.4, Loss: 1.8435546159744263\n",
      "Optimizing model...\n",
      "Step: 3, Action: 1, Reward: 0.0, Total Reward: -7.4, Loss: 0.5565227270126343\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: 2.0, Total Reward: -5.4, Loss: 1.9632387161254883\n",
      "Optimizing model...\n",
      "Step: 5, Action: 3, Reward: 0.25, Total Reward: -5.15, Loss: 1.7954189777374268\n",
      "Optimizing model...\n",
      "Step: 6, Action: 2, Reward: 0.25, Total Reward: -4.9, Loss: 3.329847812652588\n",
      "Optimizing model...\n",
      "Step: 7, Action: 2, Reward: 0.25, Total Reward: -4.65, Loss: 2.050187826156616\n",
      "Optimizing model...\n",
      "Step: 8, Action: 1, Reward: 0.0, Total Reward: -4.65, Loss: 5.621212959289551\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 9, Action: 0, Reward: 2.0, Total Reward: -2.6500000000000004, Loss: 3.296574115753174\n",
      "Optimizing model...\n",
      "Step: 10, Action: 1, Reward: 0.0, Total Reward: -2.6500000000000004, Loss: 3.9569969177246094\n",
      "Optimizing model...\n",
      "Step: 11, Action: 1, Reward: -10.0, Total Reward: -12.65, Loss: 2.0700552463531494\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 12, Action: 0, Reward: 2.0, Total Reward: -10.65, Loss: 5.45961332321167\n",
      "Optimizing model...\n",
      "Step: 13, Action: 3, Reward: 0.35, Total Reward: -10.3, Loss: 0.35156792402267456\n",
      "Optimizing model...\n",
      "Step: 14, Action: 1, Reward: 0.0, Total Reward: -10.3, Loss: 3.75770902633667\n",
      "Optimizing model...\n",
      "Step: 15, Action: 0, Reward: 2.0, Total Reward: -8.3, Loss: 0.22905774414539337\n",
      "Optimizing model...\n",
      "Step: 16, Action: 1, Reward: 0.0, Total Reward: -8.3, Loss: 1.5758330821990967\n",
      "Optimizing model...\n",
      "Step: 17, Action: 3, Reward: 0.25, Total Reward: -8.05, Loss: 1.9825117588043213\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 18, Action: 0, Reward: 2.0, Total Reward: -6.050000000000001, Loss: 5.547513961791992\n",
      "Optimizing model...\n",
      "Step: 19, Action: 1, Reward: 0.0, Total Reward: -6.050000000000001, Loss: 1.8232840299606323\n",
      "Optimizing model...\n",
      "Step: 20, Action: 0, Reward: 2.0, Total Reward: -4.050000000000001, Loss: 1.8618667125701904\n",
      "Optimizing model...\n",
      "Step: 21, Action: 2, Reward: 0.25, Total Reward: -3.8000000000000007, Loss: 3.301764965057373\n",
      "Optimizing model...\n",
      "Step: 22, Action: 1, Reward: 0.0, Total Reward: -3.8000000000000007, Loss: 2.0858750343322754\n",
      "Optimizing model...\n",
      "Step: 23, Action: 4, Reward: 0.2, Total Reward: -3.6000000000000005, Loss: 3.6094141006469727\n",
      "Optimizing model...\n",
      "Step: 24, Action: 4, Reward: -9.8, Total Reward: -13.400000000000002, Loss: 0.8505882024765015\n",
      "Optimizing model...\n",
      "Step: 25, Action: 2, Reward: 0.25, Total Reward: -13.150000000000002, Loss: 0.5473603010177612\n",
      "Model acting\n",
      "\n",
      "episode: 10, reward: -113.15\n",
      "\n",
      "Model saved\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: 2.0, Total Reward: 2.0, Loss: 2.2942843437194824\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: -8.0, Total Reward: -6.0, Loss: 0.5837284922599792\n",
      "Optimizing model...\n",
      "Step: 2, Action: 2, Reward: 0.25, Total Reward: -5.75, Loss: 1.6672018766403198\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: 2.0, Total Reward: -3.75, Loss: 5.106175422668457\n",
      "Optimizing model...\n",
      "Step: 4, Action: 3, Reward: 0.35, Total Reward: -3.4, Loss: 2.1133928298950195\n",
      "Optimizing model...\n",
      "Step: 5, Action: 4, Reward: 0.2, Total Reward: -3.1999999999999997, Loss: 4.120357990264893\n",
      "Optimizing model...\n",
      "Step: 6, Action: 0, Reward: 2.0, Total Reward: -1.1999999999999997, Loss: 2.1826140880584717\n",
      "Optimizing model...\n",
      "Step: 7, Action: 1, Reward: 0.0, Total Reward: -1.1999999999999997, Loss: 3.930835485458374\n",
      "Optimizing model...\n",
      "Step: 8, Action: 0, Reward: 2.0, Total Reward: 0.8000000000000003, Loss: 0.5534358620643616\n",
      "Optimizing model...\n",
      "Step: 9, Action: 2, Reward: 0.25, Total Reward: 1.0500000000000003, Loss: 0.6765151023864746\n",
      "\n",
      "episode: 11, reward: -98.95\n",
      "\n",
      "Optimizing model...\n",
      "Step: 0, Action: 1, Reward: 0.0, Total Reward: 0.0, Loss: 1.910346508026123\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: 2.0, Total Reward: 2.0, Loss: 1.9524563550949097\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: -8.0, Total Reward: -6.0, Loss: 2.246448278427124\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: 2.0, Total Reward: -4.0, Loss: 3.4646193981170654\n",
      "Optimizing model...\n",
      "Step: 4, Action: 2, Reward: 0.25, Total Reward: -3.75, Loss: 2.613551139831543\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 5, Action: 0, Reward: 2.1, Total Reward: -1.65, Loss: 3.508301019668579\n",
      "Optimizing model...\n",
      "Step: 6, Action: 2, Reward: 0.25, Total Reward: -1.4, Loss: 3.9115123748779297\n",
      "Optimizing model...\n",
      "Step: 7, Action: 3, Reward: 0.25, Total Reward: -1.15, Loss: 4.647197723388672\n",
      "Optimizing model...\n",
      "Step: 8, Action: 1, Reward: 0.0, Total Reward: -1.15, Loss: 0.5638397336006165\n",
      "Optimizing model...\n",
      "Step: 9, Action: 2, Reward: 0.35, Total Reward: -0.7999999999999999, Loss: 0.49687328934669495\n",
      "\n",
      "episode: 12, reward: -100.8\n",
      "\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: 2.0, Total Reward: 2.0, Loss: 5.399633884429932\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: 2.0, Total Reward: 4.0, Loss: 0.3173275589942932\n",
      "Optimizing model...\n",
      "Step: 2, Action: 2, Reward: 0.25, Total Reward: 4.25, Loss: 3.7118473052978516\n",
      "Optimizing model...\n",
      "Step: 3, Action: 2, Reward: 0.25, Total Reward: 4.5, Loss: 3.7051749229431152\n",
      "\n",
      "episode: 13, reward: -95.5\n",
      "\n",
      "Optimizing model...\n",
      "Step: 0, Action: 3, Reward: 0.25, Total Reward: 0.25, Loss: 0.8450879454612732\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: 2.0, Total Reward: 2.25, Loss: 3.739170551300049\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: 2.0, Total Reward: 4.25, Loss: 2.6893467903137207\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: 2.0, Total Reward: 6.25, Loss: 3.6097841262817383\n",
      "Optimizing model...\n",
      "Step: 4, Action: 1, Reward: 0.0, Total Reward: 6.25, Loss: 0.35714972019195557\n",
      "Optimizing model...\n",
      "Step: 5, Action: 2, Reward: 0.25, Total Reward: 6.5, Loss: 0.6796842217445374\n",
      "\n",
      "episode: 14, reward: -93.5\n",
      "\n",
      "Optimizing model...\n",
      "Step: 0, Action: 1, Reward: 0.0, Total Reward: 0.0, Loss: 3.5265450477600098\n",
      "Optimizing model...\n",
      "Step: 1, Action: 1, Reward: -10.0, Total Reward: -10.0, Loss: 2.164853572845459\n",
      "Optimizing model...\n",
      "Step: 2, Action: 4, Reward: 0.2, Total Reward: -9.8, Loss: 3.6601905822753906\n",
      "Optimizing model...\n",
      "Step: 3, Action: 3, Reward: 0.25, Total Reward: -9.55, Loss: 1.0162209272384644\n",
      "Optimizing model...\n",
      "Step: 4, Action: 2, Reward: 0.25, Total Reward: -9.3, Loss: 2.356929302215576\n",
      "Optimizing model...\n",
      "Step: 5, Action: 1, Reward: 0.0, Total Reward: -9.3, Loss: 3.078181743621826\n",
      "Optimizing model...\n",
      "Step: 6, Action: 1, Reward: 0.0, Total Reward: -9.3, Loss: 2.168881893157959\n",
      "Optimizing model...\n",
      "Step: 7, Action: 3, Reward: 0.25, Total Reward: -9.05, Loss: 2.316399097442627\n",
      "Optimizing model...\n",
      "Step: 8, Action: 4, Reward: 0.2, Total Reward: -8.850000000000001, Loss: 5.411663055419922\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 9, Action: 0, Reward: 2.0, Total Reward: -6.850000000000001, Loss: 3.718965530395508\n",
      "Optimizing model...\n",
      "Step: 10, Action: 0, Reward: 2.0, Total Reward: -4.850000000000001, Loss: 0.9371519088745117\n",
      "Optimizing model...\n",
      "Step: 11, Action: 0, Reward: 2.0, Total Reward: -2.8500000000000014, Loss: 0.47078531980514526\n",
      "Optimizing model...\n",
      "Step: 12, Action: 0, Reward: -8.0, Total Reward: -10.850000000000001, Loss: 1.890613317489624\n",
      "Optimizing model...\n",
      "Step: 13, Action: 1, Reward: 0.0, Total Reward: -10.850000000000001, Loss: 1.8874573707580566\n",
      "Optimizing model...\n",
      "Step: 14, Action: 2, Reward: 0.25, Total Reward: -10.600000000000001, Loss: 2.0554709434509277\n",
      "\n",
      "episode: 15, reward: -110.6\n",
      "\n",
      "Optimizing model...\n",
      "Step: 0, Action: 3, Reward: 0.35, Total Reward: 0.35, Loss: 2.1130459308624268\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: 2.0, Total Reward: 2.35, Loss: 0.4837576448917389\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: 2.0, Total Reward: 4.35, Loss: 5.228806018829346\n",
      "Optimizing model...\n",
      "Step: 3, Action: 2, Reward: 0.25, Total Reward: 4.6, Loss: 1.7440531253814697\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: 2.1, Total Reward: 6.699999999999999, Loss: 4.988584518432617\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 5, Action: 0, Reward: -8.0, Total Reward: -1.3000000000000007, Loss: 5.519580364227295\n",
      "Optimizing model...\n",
      "Step: 6, Action: 0, Reward: 2.0, Total Reward: 0.6999999999999993, Loss: 3.446444272994995\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 7, Action: 0, Reward: 2.0, Total Reward: 2.6999999999999993, Loss: 2.3419837951660156\n",
      "\n",
      "episode: 16, reward: -97.3\n",
      "\n",
      "Optimizing model...\n",
      "Step: 0, Action: 4, Reward: 0.2, Total Reward: 0.2, Loss: 4.8508477210998535\n",
      "Optimizing model...\n",
      "Step: 1, Action: 4, Reward: 0.2, Total Reward: 0.4, Loss: 3.179387331008911\n",
      "Optimizing model...\n",
      "Step: 2, Action: 2, Reward: 0.25, Total Reward: 0.65, Loss: 2.5469048023223877\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: 2.0, Total Reward: 2.65, Loss: 5.198238849639893\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: 2.0, Total Reward: 4.65, Loss: 3.8285138607025146\n",
      "Optimizing model...\n",
      "Step: 5, Action: 4, Reward: 0.2, Total Reward: 4.8500000000000005, Loss: 3.2932634353637695\n",
      "Optimizing model...\n",
      "Step: 6, Action: 1, Reward: 0.0, Total Reward: 4.8500000000000005, Loss: 0.6963359117507935\n",
      "\n",
      "episode: 17, reward: -95.15\n",
      "\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: 2.0, Total Reward: 2.0, Loss: 4.154495716094971\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: -8.0, Total Reward: -6.0, Loss: 1.9488587379455566\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: 2.0, Total Reward: -4.0, Loss: 2.227344036102295\n",
      "Optimizing model...\n",
      "Step: 3, Action: 1, Reward: 0.0, Total Reward: -4.0, Loss: 0.486339271068573\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: 2.1, Total Reward: -1.9, Loss: 1.8978965282440186\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 5, Action: 0, Reward: 2.0, Total Reward: 0.10000000000000009, Loss: 3.391390800476074\n",
      "Optimizing model...\n",
      "Step: 6, Action: 1, Reward: 0.0, Total Reward: 0.10000000000000009, Loss: 4.570510387420654\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 7, Action: 0, Reward: 2.0, Total Reward: 2.1, Loss: 4.562829971313477\n",
      "Optimizing model...\n",
      "Step: 8, Action: 2, Reward: 0.25, Total Reward: 2.35, Loss: 3.852259397506714\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 9, Action: 0, Reward: 2.0, Total Reward: 4.35, Loss: 5.6783342361450195\n",
      "Optimizing model...\n",
      "Step: 10, Action: 2, Reward: 0.25, Total Reward: 4.6, Loss: 3.350503921508789\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 11, Action: 0, Reward: 2.0, Total Reward: 6.6, Loss: 3.9706177711486816\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 12, Action: 0, Reward: 2.0, Total Reward: 8.6, Loss: 2.3769469261169434\n",
      "Optimizing model...\n",
      "Step: 13, Action: 3, Reward: 0.25, Total Reward: 8.85, Loss: 5.324933052062988\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 14, Action: 0, Reward: 2.0, Total Reward: 10.85, Loss: 4.016427993774414\n",
      "Optimizing model...\n",
      "Step: 15, Action: 2, Reward: 0.25, Total Reward: 11.1, Loss: 4.7878522872924805\n",
      "Optimizing model...\n",
      "Step: 16, Action: 1, Reward: 0.0, Total Reward: 11.1, Loss: 3.6662940979003906\n",
      "Optimizing model...\n",
      "Step: 17, Action: 0, Reward: 2.0, Total Reward: 13.1, Loss: 1.8982791900634766\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 18, Action: 0, Reward: -8.0, Total Reward: 5.1, Loss: 0.5739668607711792\n",
      "Optimizing model...\n",
      "Step: 19, Action: 4, Reward: 0.2, Total Reward: 5.3, Loss: 3.766414165496826\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 20, Action: 0, Reward: 2.0, Total Reward: 7.3, Loss: 3.4797520637512207\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 21, Action: 0, Reward: 2.0, Total Reward: 9.3, Loss: 2.4711403846740723\n",
      "Optimizing model...\n",
      "Step: 22, Action: 2, Reward: 0.35, Total Reward: 9.65, Loss: 2.5600883960723877\n",
      "Optimizing model...\n",
      "Step: 23, Action: 0, Reward: 2.0, Total Reward: 11.65, Loss: 3.3849048614501953\n",
      "Optimizing model...\n",
      "Step: 24, Action: 0, Reward: 2.0, Total Reward: 13.65, Loss: 3.4847607612609863\n",
      "Optimizing model...\n",
      "Step: 25, Action: 0, Reward: 2.0, Total Reward: 15.65, Loss: 0.8170514702796936\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 26, Action: 0, Reward: -8.0, Total Reward: 7.65, Loss: 5.273078441619873\n",
      "Optimizing model...\n",
      "Step: 27, Action: 0, Reward: 2.1, Total Reward: 9.75, Loss: 2.08560848236084\n",
      "Optimizing model...\n",
      "Step: 28, Action: 2, Reward: 0.25, Total Reward: 10.0, Loss: 3.8366756439208984\n",
      "Optimizing model...\n",
      "Step: 29, Action: 1, Reward: 0.0, Total Reward: 10.0, Loss: 0.672551155090332\n",
      "Optimizing model...\n",
      "Step: 30, Action: 2, Reward: 0.25, Total Reward: 10.25, Loss: 4.899724960327148\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 31, Action: 0, Reward: 2.0, Total Reward: 12.25, Loss: 3.8007664680480957\n",
      "Optimizing model...\n",
      "Step: 32, Action: 0, Reward: 2.0, Total Reward: 14.25, Loss: 0.3768153190612793\n",
      "Optimizing model...\n",
      "Step: 33, Action: 0, Reward: 2.0, Total Reward: 16.25, Loss: 0.3417702913284302\n",
      "Optimizing model...\n",
      "Step: 34, Action: 3, Reward: 0.25, Total Reward: 16.5, Loss: 6.4545392990112305\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 35, Action: 0, Reward: 2.0, Total Reward: 18.5, Loss: 0.7410331964492798\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 36, Action: 0, Reward: 2.0, Total Reward: 20.5, Loss: 2.2289347648620605\n",
      "Optimizing model...\n",
      "Step: 37, Action: 3, Reward: 0.25, Total Reward: 20.75, Loss: 3.475921154022217\n",
      "Optimizing model...\n",
      "Step: 38, Action: 3, Reward: 0.25, Total Reward: 21.0, Loss: 4.525232315063477\n",
      "\n",
      "episode: 18, reward: -79.0\n",
      "\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: -8.0, Total Reward: -8.0, Loss: 2.214205265045166\n",
      "Optimizing model...\n",
      "Step: 1, Action: 3, Reward: 0.25, Total Reward: -7.75, Loss: 2.1087043285369873\n",
      "Optimizing model...\n",
      "Step: 2, Action: 4, Reward: 0.2, Total Reward: -7.55, Loss: 2.287680149078369\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: 2.1, Total Reward: -5.449999999999999, Loss: 0.6422679424285889\n",
      "Optimizing model...\n",
      "Step: 4, Action: 4, Reward: 0.2, Total Reward: -5.249999999999999, Loss: 2.312056064605713\n",
      "Optimizing model...\n",
      "Step: 5, Action: 4, Reward: -9.8, Total Reward: -15.05, Loss: 3.4310812950134277\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 6, Action: 0, Reward: 2.0, Total Reward: -13.05, Loss: 3.028421401977539\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 7, Action: 0, Reward: 2.0, Total Reward: -11.05, Loss: 3.879737138748169\n",
      "Optimizing model...\n",
      "Step: 8, Action: 4, Reward: 0.2, Total Reward: -10.850000000000001, Loss: 3.746727705001831\n",
      "Optimizing model...\n",
      "Step: 9, Action: 3, Reward: 0.25, Total Reward: -10.600000000000001, Loss: 1.7495896816253662\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 10, Action: 0, Reward: 2.0, Total Reward: -8.600000000000001, Loss: 8.477235794067383\n",
      "\n",
      "episode: 19, reward: -108.6\n",
      "\n",
      "Optimizing model...\n",
      "Step: 0, Action: 3, Reward: 0.25, Total Reward: 0.25, Loss: 3.0629780292510986\n",
      "Optimizing model...\n",
      "Step: 1, Action: 2, Reward: 0.25, Total Reward: 0.5, Loss: 5.062334060668945\n",
      "Optimizing model...\n",
      "Step: 2, Action: 3, Reward: 0.35, Total Reward: 0.85, Loss: 1.6268763542175293\n",
      "Optimizing model...\n",
      "Step: 3, Action: 3, Reward: 0.25, Total Reward: 1.1, Loss: 0.29281729459762573\n",
      "Optimizing model...\n",
      "Step: 4, Action: 4, Reward: 0.2, Total Reward: 1.3, Loss: 6.70656681060791\n",
      "Optimizing model...\n",
      "Step: 5, Action: 4, Reward: -9.8, Total Reward: -8.5, Loss: 4.693402290344238\n",
      "Optimizing model...\n",
      "Step: 6, Action: 0, Reward: 2.0, Total Reward: -6.5, Loss: 1.1561360359191895\n",
      "Optimizing model...\n",
      "Step: 7, Action: 4, Reward: 0.3, Total Reward: -6.2, Loss: 2.006885290145874\n",
      "Optimizing model...\n",
      "Step: 8, Action: 2, Reward: 0.35, Total Reward: -5.8500000000000005, Loss: 1.9905762672424316\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 9, Action: 0, Reward: 2.0, Total Reward: -3.8500000000000005, Loss: 3.8645009994506836\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 10, Action: 0, Reward: 2.0, Total Reward: -1.8500000000000005, Loss: 0.5064804553985596\n",
      "Optimizing model...\n",
      "Step: 11, Action: 4, Reward: 0.2, Total Reward: -1.6500000000000006, Loss: 5.442877769470215\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 12, Action: 0, Reward: 2.0, Total Reward: 0.3499999999999994, Loss: 2.1350817680358887\n",
      "Optimizing model...\n",
      "Step: 13, Action: 2, Reward: 0.25, Total Reward: 0.5999999999999994, Loss: 2.1517844200134277\n",
      "Optimizing model...\n",
      "Step: 14, Action: 3, Reward: 0.25, Total Reward: 0.8499999999999994, Loss: 2.4198994636535645\n",
      "Optimizing model...\n",
      "Step: 15, Action: 2, Reward: 0.25, Total Reward: 1.0999999999999994, Loss: 3.990257978439331\n",
      "Optimizing model...\n",
      "Step: 16, Action: 2, Reward: 0.25, Total Reward: 1.3499999999999994, Loss: 2.510324716567993\n",
      "Optimizing model...\n",
      "Step: 17, Action: 1, Reward: 0.0, Total Reward: 1.3499999999999994, Loss: 5.358129501342773\n",
      "Optimizing model...\n",
      "Step: 18, Action: 0, Reward: 2.0, Total Reward: 3.3499999999999996, Loss: 2.2270872592926025\n",
      "Optimizing model...\n",
      "Step: 19, Action: 3, Reward: 0.25, Total Reward: 3.5999999999999996, Loss: 1.8951905965805054\n",
      "Optimizing model...\n",
      "Step: 20, Action: 1, Reward: 0.0, Total Reward: 3.5999999999999996, Loss: 2.0011849403381348\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 21, Action: 0, Reward: 2.0, Total Reward: 5.6, Loss: 1.8800938129425049\n",
      "Optimizing model...\n",
      "Step: 22, Action: 0, Reward: -8.0, Total Reward: -2.4000000000000004, Loss: 0.722271203994751\n",
      "Optimizing model...\n",
      "Step: 23, Action: 0, Reward: 2.0, Total Reward: -0.40000000000000036, Loss: 5.246417045593262\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 24, Action: 0, Reward: 2.0, Total Reward: 1.5999999999999996, Loss: 2.1067466735839844\n",
      "Optimizing model...\n",
      "Step: 25, Action: 3, Reward: 0.25, Total Reward: 1.8499999999999996, Loss: 5.074563980102539\n",
      "Optimizing model...\n",
      "Step: 26, Action: 4, Reward: 0.2, Total Reward: 2.05, Loss: 2.1627957820892334\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 27, Action: 0, Reward: 2.0, Total Reward: 4.05, Loss: 0.5149818658828735\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 28, Action: 0, Reward: -8.0, Total Reward: -3.95, Loss: 4.025935173034668\n",
      "Model acting\n",
      "\n",
      "episode: 20, reward: -103.95\n",
      "\n",
      "Model saved\n",
      "Optimizing model...\n",
      "Step: 0, Action: 3, Reward: 0.25, Total Reward: 0.25, Loss: 2.008941650390625\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: 2.0, Total Reward: 2.25, Loss: 2.3420422077178955\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: 2.1, Total Reward: 4.35, Loss: 1.9416409730911255\n",
      "Optimizing model...\n",
      "Step: 3, Action: 3, Reward: 0.25, Total Reward: 4.6, Loss: 3.8842363357543945\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: 2.0, Total Reward: 6.6, Loss: 6.407135486602783\n",
      "Optimizing model...\n",
      "Step: 5, Action: 1, Reward: 0.0, Total Reward: 6.6, Loss: 4.990045547485352\n",
      "Optimizing model...\n",
      "Step: 6, Action: 4, Reward: 0.2, Total Reward: 6.8, Loss: 4.949256420135498\n",
      "Optimizing model...\n",
      "Step: 7, Action: 2, Reward: 0.25, Total Reward: 7.05, Loss: 1.753228783607483\n",
      "Optimizing model...\n",
      "Step: 8, Action: 4, Reward: 0.2, Total Reward: 7.25, Loss: 5.144186019897461\n",
      "Optimizing model...\n",
      "Step: 9, Action: 2, Reward: 0.35, Total Reward: 7.6, Loss: 1.797034740447998\n",
      "Optimizing model...\n",
      "Step: 10, Action: 1, Reward: 0.0, Total Reward: 7.6, Loss: 1.9942866563796997\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 11, Action: 0, Reward: 2.0, Total Reward: 9.6, Loss: 2.048222064971924\n",
      "Optimizing model...\n",
      "Step: 12, Action: 1, Reward: 0.0, Total Reward: 9.6, Loss: 4.279249668121338\n",
      "Optimizing model...\n",
      "Step: 13, Action: 4, Reward: 0.2, Total Reward: 9.799999999999999, Loss: 5.179279327392578\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 14, Action: 0, Reward: 2.0, Total Reward: 11.799999999999999, Loss: 4.872683048248291\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 15, Action: 0, Reward: 2.0, Total Reward: 13.799999999999999, Loss: 2.841832160949707\n",
      "Optimizing model...\n",
      "Step: 16, Action: 2, Reward: 0.25, Total Reward: 14.049999999999999, Loss: 0.5258475542068481\n",
      "Optimizing model...\n",
      "Step: 17, Action: 1, Reward: 0.0, Total Reward: 14.049999999999999, Loss: 2.428157329559326\n",
      "Optimizing model...\n",
      "Step: 18, Action: 4, Reward: 0.2, Total Reward: 14.249999999999998, Loss: 0.5635503530502319\n",
      "Optimizing model...\n",
      "Step: 19, Action: 1, Reward: 0.0, Total Reward: 14.249999999999998, Loss: 0.7797881960868835\n",
      "Optimizing model...\n",
      "Step: 20, Action: 1, Reward: -10.0, Total Reward: 4.249999999999998, Loss: 2.381344795227051\n",
      "Optimizing model...\n",
      "Step: 21, Action: 1, Reward: -10.0, Total Reward: -5.750000000000002, Loss: 3.7825212478637695\n",
      "Optimizing model...\n",
      "Step: 22, Action: 0, Reward: 2.0, Total Reward: -3.7500000000000018, Loss: 1.7019052505493164\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 23, Action: 0, Reward: 2.0, Total Reward: -1.7500000000000018, Loss: 0.7076533436775208\n",
      "Optimizing model...\n",
      "Step: 24, Action: 0, Reward: 2.0, Total Reward: 0.24999999999999822, Loss: 5.091089248657227\n",
      "Optimizing model...\n",
      "Step: 25, Action: 1, Reward: 0.0, Total Reward: 0.24999999999999822, Loss: 2.4375410079956055\n",
      "\n",
      "episode: 21, reward: -99.75\n",
      "\n",
      "Optimizing model...\n",
      "Step: 0, Action: 3, Reward: 0.25, Total Reward: 0.25, Loss: 3.8951187133789062\n",
      "Optimizing model...\n",
      "Step: 1, Action: 1, Reward: 0.0, Total Reward: 0.25, Loss: 8.385931968688965\n",
      "Optimizing model...\n",
      "Step: 2, Action: 2, Reward: 0.25, Total Reward: 0.5, Loss: 3.6879942417144775\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: 2.1, Total Reward: 2.6, Loss: 1.6075066328048706\n",
      "Optimizing model...\n",
      "Step: 4, Action: 3, Reward: 0.25, Total Reward: 2.85, Loss: 3.949047565460205\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 5, Action: 0, Reward: 2.0, Total Reward: 4.85, Loss: 2.0237467288970947\n",
      "Optimizing model...\n",
      "Step: 6, Action: 0, Reward: -8.0, Total Reward: -3.1500000000000004, Loss: 2.2992756366729736\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 7, Action: 1, Reward: 0.0, Total Reward: -3.1500000000000004, Loss: 2.243762969970703\n",
      "Optimizing model...\n",
      "Step: 8, Action: 2, Reward: 0.25, Total Reward: -2.9000000000000004, Loss: 4.7798871994018555\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 9, Action: 2, Reward: 0.25, Total Reward: -2.6500000000000004, Loss: 2.0945816040039062\n",
      "Optimizing model...\n",
      "Step: 10, Action: 4, Reward: 0.2, Total Reward: -2.45, Loss: 7.427789688110352\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 11, Action: 1, Reward: 0.0, Total Reward: -2.45, Loss: 2.0791234970092773\n",
      "Optimizing model...\n",
      "Step: 12, Action: 4, Reward: 0.2, Total Reward: -2.25, Loss: 2.0909786224365234\n",
      "Optimizing model...\n",
      "Step: 13, Action: 3, Reward: 0.25, Total Reward: -2.0, Loss: 1.008177638053894\n",
      "Optimizing model...\n",
      "Step: 14, Action: 4, Reward: 0.2, Total Reward: -1.8, Loss: 3.46488094329834\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 15, Action: 0, Reward: 2.0, Total Reward: 0.19999999999999996, Loss: 3.8042244911193848\n",
      "Optimizing model...\n",
      "Step: 16, Action: 0, Reward: 2.0, Total Reward: 2.2, Loss: 3.699532985687256\n",
      "Optimizing model...\n",
      "Step: 17, Action: 1, Reward: 0.0, Total Reward: 2.2, Loss: 3.9626288414001465\n",
      "\n",
      "episode: 22, reward: -97.8\n",
      "\n",
      "Optimizing model...\n",
      "Step: 0, Action: 1, Reward: 0.0, Total Reward: 0.0, Loss: 3.649057149887085\n",
      "Optimizing model...\n",
      "Step: 1, Action: 3, Reward: 0.25, Total Reward: 0.25, Loss: 3.4414358139038086\n",
      "Optimizing model...\n",
      "Step: 2, Action: 3, Reward: 0.25, Total Reward: 0.5, Loss: 1.7034800052642822\n",
      "Optimizing model...\n",
      "Step: 3, Action: 4, Reward: 0.2, Total Reward: 0.7, Loss: 4.5403337478637695\n",
      "Optimizing model...\n",
      "Step: 4, Action: 4, Reward: -9.8, Total Reward: -9.100000000000001, Loss: 1.6418876647949219\n",
      "Optimizing model...\n",
      "Step: 5, Action: 3, Reward: 0.25, Total Reward: -8.850000000000001, Loss: 0.5647798776626587\n",
      "Optimizing model...\n",
      "Step: 6, Action: 0, Reward: 2.0, Total Reward: -6.850000000000001, Loss: 3.8366761207580566\n",
      "Optimizing model...\n",
      "Step: 7, Action: 0, Reward: -8.0, Total Reward: -14.850000000000001, Loss: 2.5199880599975586\n",
      "Optimizing model...\n",
      "Step: 8, Action: 3, Reward: 0.25, Total Reward: -14.600000000000001, Loss: 3.694650650024414\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 9, Action: 0, Reward: 2.0, Total Reward: -12.600000000000001, Loss: 3.3613715171813965\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 10, Action: 0, Reward: -8.0, Total Reward: -20.6, Loss: 1.7601208686828613\n",
      "Optimizing model...\n",
      "Step: 11, Action: 3, Reward: 0.25, Total Reward: -20.35, Loss: 3.749858856201172\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 12, Action: 0, Reward: 2.0, Total Reward: -18.35, Loss: 0.30307459831237793\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 13, Action: 0, Reward: 2.0, Total Reward: -16.35, Loss: 4.0427422523498535\n",
      "Optimizing model...\n",
      "Step: 14, Action: 1, Reward: 0.1, Total Reward: -16.25, Loss: 2.512235164642334\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 15, Action: 0, Reward: 2.0, Total Reward: -14.25, Loss: 3.486548662185669\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 16, Action: 0, Reward: 2.0, Total Reward: -12.25, Loss: 3.482703685760498\n",
      "Optimizing model...\n",
      "Step: 17, Action: 3, Reward: 0.25, Total Reward: -12.0, Loss: 3.2815675735473633\n",
      "Optimizing model...\n",
      "Step: 18, Action: 0, Reward: 2.0, Total Reward: -10.0, Loss: 0.6612566709518433\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 19, Action: 0, Reward: -8.0, Total Reward: -18.0, Loss: 3.3644909858703613\n",
      "Optimizing model...\n",
      "Step: 20, Action: 0, Reward: 2.1, Total Reward: -15.9, Loss: 4.097517967224121\n",
      "Optimizing model...\n",
      "Step: 21, Action: 0, Reward: 2.0, Total Reward: -13.9, Loss: 1.9314125776290894\n",
      "Optimizing model...\n",
      "Step: 22, Action: 1, Reward: 0.0, Total Reward: -13.9, Loss: 2.2741479873657227\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 23, Action: 0, Reward: 2.0, Total Reward: -11.9, Loss: 2.313934803009033\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 24, Action: 0, Reward: 2.0, Total Reward: -9.9, Loss: 5.485796928405762\n",
      "Optimizing model...\n",
      "Step: 25, Action: 0, Reward: -8.0, Total Reward: -17.9, Loss: 0.6870155334472656\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 26, Action: 0, Reward: -8.0, Total Reward: -25.9, Loss: 3.285407066345215\n",
      "Optimizing model...\n",
      "Step: 27, Action: 2, Reward: 0.25, Total Reward: -25.65, Loss: 1.8158546686172485\n",
      "Optimizing model...\n",
      "Step: 28, Action: 0, Reward: 2.1, Total Reward: -23.549999999999997, Loss: 3.6266250610351562\n",
      "Optimizing model...\n",
      "Step: 29, Action: 3, Reward: 0.25, Total Reward: -23.299999999999997, Loss: 2.195788860321045\n",
      "Optimizing model...\n",
      "Step: 30, Action: 1, Reward: 0.0, Total Reward: -23.299999999999997, Loss: 2.367281913757324\n",
      "Optimizing model...\n",
      "Step: 31, Action: 1, Reward: 0.0, Total Reward: -23.299999999999997, Loss: 0.4959959387779236\n",
      "Optimizing model...\n",
      "Step: 32, Action: 4, Reward: 0.2, Total Reward: -23.099999999999998, Loss: 0.18352404236793518\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 33, Action: 0, Reward: 2.0, Total Reward: -21.099999999999998, Loss: 4.4097089767456055\n",
      "Optimizing model...\n",
      "Step: 34, Action: 4, Reward: 0.2, Total Reward: -20.9, Loss: 2.88873028755188\n",
      "Optimizing model...\n",
      "Step: 35, Action: 1, Reward: 0.0, Total Reward: -20.9, Loss: 3.5424585342407227\n",
      "Optimizing model...\n",
      "Step: 36, Action: 0, Reward: 2.0, Total Reward: -18.9, Loss: 1.8123242855072021\n",
      "Optimizing model...\n",
      "Step: 37, Action: 3, Reward: 0.25, Total Reward: -18.65, Loss: 1.8494274616241455\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 38, Action: 0, Reward: 2.0, Total Reward: -16.65, Loss: 4.6948041915893555\n",
      "\n",
      "episode: 23, reward: -116.65\n",
      "\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: -8.0, Total Reward: -8.0, Loss: 2.0111796855926514\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 1, Action: 2, Reward: 0.25, Total Reward: -7.75, Loss: 7.585655689239502\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: 2.0, Total Reward: -5.75, Loss: 1.0679354667663574\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: 2.0, Total Reward: -3.75, Loss: 2.072902202606201\n",
      "Optimizing model...\n",
      "Step: 4, Action: 2, Reward: 0.25, Total Reward: -3.5, Loss: 2.7842671871185303\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 5, Action: 0, Reward: 2.0, Total Reward: -1.5, Loss: 1.9258079528808594\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 6, Action: 0, Reward: -8.0, Total Reward: -9.5, Loss: 0.5250294804573059\n",
      "Optimizing model...\n",
      "Step: 7, Action: 3, Reward: 0.25, Total Reward: -9.25, Loss: 1.8539478778839111\n",
      "Model acting\n",
      "\n",
      "episode: 24, reward: -109.25\n",
      "\n",
      "Optimizing model...\n",
      "Step: 0, Action: 2, Reward: 0.25, Total Reward: 0.25, Loss: 2.025608539581299\n",
      "Optimizing model...\n",
      "Step: 1, Action: 3, Reward: 0.25, Total Reward: 0.5, Loss: 6.5937700271606445\n",
      "Optimizing model...\n",
      "Step: 2, Action: 2, Reward: 0.25, Total Reward: 0.75, Loss: 6.563499450683594\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: 2.1, Total Reward: 2.85, Loss: 1.493337869644165\n",
      "Optimizing model...\n",
      "Step: 4, Action: 3, Reward: 0.35, Total Reward: 3.2, Loss: 1.7977402210235596\n",
      "Optimizing model...\n",
      "Step: 5, Action: 2, Reward: 0.25, Total Reward: 3.45, Loss: 1.8609726428985596\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 6, Action: 0, Reward: 2.0, Total Reward: 5.45, Loss: 3.718888282775879\n",
      "Optimizing model...\n",
      "Step: 7, Action: 0, Reward: 2.0, Total Reward: 7.45, Loss: 2.335514545440674\n",
      "Optimizing model...\n",
      "Step: 8, Action: 0, Reward: -8.0, Total Reward: -0.5499999999999998, Loss: 3.987157106399536\n",
      "Optimizing model...\n",
      "Step: 9, Action: 4, Reward: 0.2, Total Reward: -0.3499999999999998, Loss: 2.100006580352783\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 10, Action: 0, Reward: 2.0, Total Reward: 1.6500000000000001, Loss: 2.5143840312957764\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 11, Action: 0, Reward: 2.0, Total Reward: 3.6500000000000004, Loss: 0.496343195438385\n",
      "Optimizing model...\n",
      "Step: 12, Action: 2, Reward: 0.25, Total Reward: 3.9000000000000004, Loss: 3.9324660301208496\n",
      "Optimizing model...\n",
      "Step: 13, Action: 2, Reward: -9.75, Total Reward: -5.85, Loss: 2.0776844024658203\n",
      "Optimizing model...\n",
      "Step: 14, Action: 0, Reward: 2.0, Total Reward: -3.8499999999999996, Loss: 0.32139649987220764\n",
      "Optimizing model...\n",
      "Step: 15, Action: 4, Reward: 0.3, Total Reward: -3.55, Loss: 5.680000305175781\n",
      "Optimizing model...\n",
      "Step: 16, Action: 2, Reward: 0.25, Total Reward: -3.3, Loss: 2.1828439235687256\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 17, Action: 0, Reward: 2.0, Total Reward: -1.2999999999999998, Loss: 3.6192445755004883\n",
      "Optimizing model...\n",
      "Step: 18, Action: 2, Reward: 0.25, Total Reward: -1.0499999999999998, Loss: 5.199482440948486\n",
      "Optimizing model...\n",
      "Step: 19, Action: 4, Reward: 0.2, Total Reward: -0.8499999999999999, Loss: 3.701112747192383\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 20, Action: 0, Reward: 2.0, Total Reward: 1.1500000000000001, Loss: 0.8119865655899048\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 21, Action: 0, Reward: -8.0, Total Reward: -6.85, Loss: 3.4633405208587646\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 22, Action: 0, Reward: -8.0, Total Reward: -14.85, Loss: 0.9679155945777893\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 23, Action: 0, Reward: -7.9, Total Reward: -22.75, Loss: 5.06015682220459\n",
      "Optimizing model...\n",
      "Step: 24, Action: 3, Reward: 0.25, Total Reward: -22.5, Loss: 4.213396072387695\n",
      "Optimizing model...\n",
      "Step: 25, Action: 1, Reward: 0.0, Total Reward: -22.5, Loss: 4.309278964996338\n",
      "\n",
      "episode: 25, reward: -122.5\n",
      "\n",
      "Optimizing model...\n",
      "Step: 0, Action: 2, Reward: 0.25, Total Reward: 0.25, Loss: 5.561830520629883\n",
      "Optimizing model...\n",
      "Step: 1, Action: 1, Reward: 0.0, Total Reward: 0.25, Loss: 3.8598854541778564\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: 2.0, Total Reward: 2.25, Loss: 6.373080253601074\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: -7.9, Total Reward: -5.65, Loss: 5.158977031707764\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: -8.0, Total Reward: -13.65, Loss: 3.358621597290039\n",
      "Optimizing model...\n",
      "Step: 5, Action: 1, Reward: 0.0, Total Reward: -13.65, Loss: 0.4112831950187683\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 6, Action: 0, Reward: 2.0, Total Reward: -11.65, Loss: 6.605473041534424\n",
      "Optimizing model...\n",
      "Step: 7, Action: 4, Reward: 0.2, Total Reward: -11.450000000000001, Loss: 2.40858793258667\n",
      "Optimizing model...\n",
      "Step: 8, Action: 2, Reward: 0.25, Total Reward: -11.200000000000001, Loss: 0.35900646448135376\n",
      "Optimizing model...\n",
      "Step: 9, Action: 1, Reward: 0.0, Total Reward: -11.200000000000001, Loss: 0.8321274518966675\n",
      "\n",
      "episode: 26, reward: -111.2\n",
      "\n",
      "Optimizing model...\n",
      "Step: 0, Action: 1, Reward: 0.0, Total Reward: 0.0, Loss: 0.9027528762817383\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: 2.0, Total Reward: 2.0, Loss: 0.2522585690021515\n",
      "Optimizing model...\n",
      "Step: 2, Action: 1, Reward: 0.0, Total Reward: 2.0, Loss: 5.090566635131836\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: 2.0, Total Reward: 4.0, Loss: 1.9881815910339355\n",
      "Optimizing model...\n",
      "Step: 4, Action: 4, Reward: 0.2, Total Reward: 4.2, Loss: 2.713587760925293\n",
      "Optimizing model...\n",
      "Step: 5, Action: 0, Reward: 2.0, Total Reward: 6.2, Loss: 2.2671284675598145\n",
      "Optimizing model...\n",
      "Step: 6, Action: 4, Reward: 0.2, Total Reward: 6.4, Loss: 1.818232536315918\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 7, Action: 0, Reward: 2.0, Total Reward: 8.4, Loss: 2.165886640548706\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 8, Action: 0, Reward: 2.0, Total Reward: 10.4, Loss: 4.016775131225586\n",
      "Optimizing model...\n",
      "Step: 9, Action: 0, Reward: 2.0, Total Reward: 12.4, Loss: 2.4207491874694824\n",
      "Optimizing model...\n",
      "Step: 10, Action: 4, Reward: 0.3, Total Reward: 12.700000000000001, Loss: 2.4298934936523438\n",
      "Optimizing model...\n",
      "Step: 11, Action: 0, Reward: 2.0, Total Reward: 14.700000000000001, Loss: 0.3852896988391876\n",
      "\n",
      "episode: 27, reward: -85.3\n",
      "\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: 2.0, Total Reward: 2.0, Loss: 2.534571647644043\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: 2.0, Total Reward: 4.0, Loss: 8.534714698791504\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: 2.0, Total Reward: 6.0, Loss: 5.3335371017456055\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: 2.0, Total Reward: 8.0, Loss: 2.337625026702881\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: 2.1, Total Reward: 10.1, Loss: 6.375543594360352\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 5, Action: 0, Reward: 2.0, Total Reward: 12.1, Loss: 2.333711862564087\n",
      "Optimizing model...\n",
      "Step: 6, Action: 0, Reward: 2.0, Total Reward: 14.1, Loss: 4.241209506988525\n",
      "Optimizing model...\n",
      "Step: 7, Action: 3, Reward: 0.25, Total Reward: 14.35, Loss: 2.274214506149292\n",
      "Optimizing model...\n",
      "Step: 8, Action: 1, Reward: 0.0, Total Reward: 14.35, Loss: 3.9133338928222656\n",
      "Optimizing model...\n",
      "Step: 9, Action: 0, Reward: 2.0, Total Reward: 16.35, Loss: 0.8123153448104858\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 10, Action: 0, Reward: -8.0, Total Reward: 8.350000000000001, Loss: 3.9989776611328125\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 11, Action: 0, Reward: -7.9, Total Reward: 0.45000000000000107, Loss: 2.056140422821045\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 12, Action: 0, Reward: 2.0, Total Reward: 2.450000000000001, Loss: 1.7970808744430542\n",
      "\n",
      "episode: 28, reward: -97.55\n",
      "\n",
      "Optimizing model...\n",
      "Step: 0, Action: 2, Reward: 0.25, Total Reward: 0.25, Loss: 5.812127113342285\n",
      "Optimizing model...\n",
      "Step: 1, Action: 1, Reward: 0.0, Total Reward: 0.25, Loss: 3.757899761199951\n",
      "Optimizing model...\n",
      "Step: 2, Action: 1, Reward: 0.0, Total Reward: 0.25, Loss: 2.3749277591705322\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: 2.0, Total Reward: 2.25, Loss: 0.4855175018310547\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: -8.0, Total Reward: -5.75, Loss: 2.051509141921997\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 5, Action: 0, Reward: 2.0, Total Reward: -3.75, Loss: 1.9261064529418945\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 6, Action: 0, Reward: -8.0, Total Reward: -11.75, Loss: 2.3735404014587402\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 7, Action: 0, Reward: 2.0, Total Reward: -9.75, Loss: 6.155200958251953\n",
      "Optimizing model...\n",
      "Step: 8, Action: 0, Reward: 2.0, Total Reward: -7.75, Loss: 3.5847270488739014\n",
      "Optimizing model...\n",
      "Step: 9, Action: 3, Reward: 0.25, Total Reward: -7.5, Loss: 2.3774147033691406\n",
      "Optimizing model...\n",
      "Step: 10, Action: 4, Reward: 0.2, Total Reward: -7.3, Loss: 2.511167049407959\n",
      "Optimizing model...\n",
      "Step: 11, Action: 2, Reward: 0.25, Total Reward: -7.05, Loss: 0.31625592708587646\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 12, Action: 0, Reward: 2.0, Total Reward: -5.05, Loss: 2.3643078804016113\n",
      "\n",
      "episode: 29, reward: -105.05\n",
      "\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: 2.0, Total Reward: 2.0, Loss: 0.6630946397781372\n",
      "Optimizing model...\n",
      "Step: 1, Action: 4, Reward: 0.2, Total Reward: 2.2, Loss: 3.5687901973724365\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: 2.0, Total Reward: 4.2, Loss: 3.644881248474121\n",
      "Optimizing model...\n",
      "Step: 3, Action: 1, Reward: 0.0, Total Reward: 4.2, Loss: 3.6144676208496094\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: 2.0, Total Reward: 6.2, Loss: 3.5577096939086914\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 5, Action: 0, Reward: 2.0, Total Reward: 8.2, Loss: 2.353929042816162\n",
      "Optimizing model...\n",
      "Step: 6, Action: 4, Reward: 0.2, Total Reward: 8.399999999999999, Loss: 4.259768486022949\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 7, Action: 2, Reward: 0.25, Total Reward: 8.649999999999999, Loss: 3.7122058868408203\n",
      "Model acting\n",
      "\n",
      "episode: 30, reward: -91.35\n",
      "\n",
      "Model saved\n",
      "Optimizing model...\n",
      "Step: 0, Action: 2, Reward: 0.25, Total Reward: 0.25, Loss: 5.120771408081055\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: 2.0, Total Reward: 2.25, Loss: 5.904788970947266\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: -7.9, Total Reward: -5.65, Loss: 3.4177467823028564\n",
      "Optimizing model...\n",
      "Step: 3, Action: 2, Reward: 0.25, Total Reward: -5.4, Loss: 5.335407257080078\n",
      "Optimizing model...\n",
      "Step: 4, Action: 3, Reward: 0.25, Total Reward: -5.15, Loss: 3.7931265830993652\n",
      "Optimizing model...\n",
      "Step: 5, Action: 4, Reward: 0.2, Total Reward: -4.95, Loss: 4.93839168548584\n",
      "Optimizing model...\n",
      "Step: 6, Action: 4, Reward: -9.8, Total Reward: -14.75, Loss: 2.0111243724823\n",
      "Optimizing model...\n",
      "Step: 7, Action: 2, Reward: 0.25, Total Reward: -14.5, Loss: 0.854385495185852\n",
      "Optimizing model...\n",
      "Step: 8, Action: 4, Reward: 0.2, Total Reward: -14.3, Loss: 5.132852077484131\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 9, Action: 0, Reward: 2.0, Total Reward: -12.3, Loss: 4.14364767074585\n",
      "Optimizing model...\n",
      "Step: 10, Action: 3, Reward: 0.25, Total Reward: -12.05, Loss: 0.35742682218551636\n",
      "Optimizing model...\n",
      "Step: 11, Action: 3, Reward: 0.25, Total Reward: -11.8, Loss: 2.2563865184783936\n",
      "Optimizing model...\n",
      "Step: 12, Action: 4, Reward: 0.2, Total Reward: -11.600000000000001, Loss: 2.5087525844573975\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 13, Action: 0, Reward: 2.0, Total Reward: -9.600000000000001, Loss: 7.3407087326049805\n",
      "Optimizing model...\n",
      "Step: 14, Action: 2, Reward: 0.25, Total Reward: -9.350000000000001, Loss: 0.44071465730667114\n",
      "Model acting\n",
      "\n",
      "episode: 31, reward: -109.35\n",
      "\n",
      "Optimizing model...\n",
      "Step: 0, Action: 2, Reward: 0.25, Total Reward: 0.25, Loss: 3.694889545440674\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: 2.0, Total Reward: 2.25, Loss: 0.5209276676177979\n",
      "Optimizing model...\n",
      "Step: 2, Action: 3, Reward: 0.25, Total Reward: 2.5, Loss: 5.057306289672852\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: 2.1, Total Reward: 4.6, Loss: 4.829776763916016\n",
      "Optimizing model...\n",
      "Step: 4, Action: 4, Reward: 0.2, Total Reward: 4.8, Loss: 2.02244234085083\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 5, Action: 0, Reward: 2.0, Total Reward: 6.8, Loss: 0.668772280216217\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 6, Action: 0, Reward: -8.0, Total Reward: -1.2000000000000002, Loss: 0.680335283279419\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 7, Action: 0, Reward: 2.0, Total Reward: 0.7999999999999998, Loss: 3.499488353729248\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 8, Action: 0, Reward: -8.0, Total Reward: -7.2, Loss: 3.7679553031921387\n",
      "Model acting\n",
      "\n",
      "episode: 32, reward: -107.2\n",
      "\n",
      "Optimizing model...\n",
      "Step: 0, Action: 1, Reward: 0.0, Total Reward: 0.0, Loss: 0.5274621844291687\n",
      "Optimizing model...\n",
      "Step: 1, Action: 1, Reward: 0.0, Total Reward: 0.0, Loss: 3.3300180435180664\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: 2.0, Total Reward: 2.0, Loss: 3.305936336517334\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: 2.0, Total Reward: 4.0, Loss: 3.646005392074585\n",
      "Optimizing model...\n",
      "Step: 4, Action: 2, Reward: 0.25, Total Reward: 4.25, Loss: 5.4125847816467285\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 5, Action: 0, Reward: 2.0, Total Reward: 6.25, Loss: 5.440304279327393\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 6, Action: 0, Reward: 2.0, Total Reward: 8.25, Loss: 0.7658437490463257\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 7, Action: 0, Reward: 2.0, Total Reward: 10.25, Loss: 0.5842725038528442\n",
      "Optimizing model...\n",
      "Step: 8, Action: 4, Reward: 0.2, Total Reward: 10.45, Loss: 2.0329155921936035\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 9, Action: 0, Reward: 2.0, Total Reward: 12.45, Loss: 5.696514129638672\n",
      "Optimizing model...\n",
      "Step: 10, Action: 3, Reward: 0.25, Total Reward: 12.7, Loss: 0.9441925287246704\n",
      "Optimizing model...\n",
      "Step: 11, Action: 1, Reward: 0.0, Total Reward: 12.7, Loss: 6.445977687835693\n",
      "Optimizing model...\n",
      "Step: 12, Action: 2, Reward: 0.35, Total Reward: 13.049999999999999, Loss: 1.8931676149368286\n",
      "Optimizing model...\n",
      "Step: 13, Action: 4, Reward: 0.3, Total Reward: 13.35, Loss: 0.8041143417358398\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 14, Action: 0, Reward: 2.0, Total Reward: 15.35, Loss: 0.9306656122207642\n",
      "Optimizing model...\n",
      "Step: 15, Action: 0, Reward: 2.0, Total Reward: 17.35, Loss: 5.92288064956665\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 16, Action: 0, Reward: 2.0, Total Reward: 19.35, Loss: 1.6536660194396973\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 17, Action: 0, Reward: 2.0, Total Reward: 21.35, Loss: 4.431952476501465\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 18, Action: 0, Reward: -8.0, Total Reward: 13.350000000000001, Loss: 5.591268539428711\n",
      "Optimizing model...\n",
      "Step: 19, Action: 3, Reward: 0.25, Total Reward: 13.600000000000001, Loss: 6.6666975021362305\n",
      "\n",
      "episode: 33, reward: -86.4\n",
      "\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: 2.0, Total Reward: 2.0, Loss: 3.525285243988037\n",
      "Optimizing model...\n",
      "Step: 1, Action: 4, Reward: 0.2, Total Reward: 2.2, Loss: 3.845102310180664\n",
      "Optimizing model...\n",
      "Step: 2, Action: 4, Reward: 0.2, Total Reward: 2.4000000000000004, Loss: 2.2580819129943848\n",
      "Optimizing model...\n",
      "Step: 3, Action: 3, Reward: 0.25, Total Reward: 2.6500000000000004, Loss: 2.0060739517211914\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: 2.0, Total Reward: 4.65, Loss: 0.39687180519104004\n",
      "Optimizing model...\n",
      "Step: 5, Action: 3, Reward: 0.25, Total Reward: 4.9, Loss: 3.4757702350616455\n",
      "Optimizing model...\n",
      "Step: 6, Action: 4, Reward: 0.2, Total Reward: 5.1000000000000005, Loss: 3.483607292175293\n",
      "Optimizing model...\n",
      "Step: 7, Action: 2, Reward: 0.25, Total Reward: 5.3500000000000005, Loss: 6.5807342529296875\n",
      "Optimizing model...\n",
      "Step: 8, Action: 0, Reward: 2.0, Total Reward: 7.3500000000000005, Loss: 1.924475908279419\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 9, Action: 0, Reward: 2.0, Total Reward: 9.350000000000001, Loss: 6.112330436706543\n",
      "Optimizing model...\n",
      "Step: 10, Action: 3, Reward: 0.25, Total Reward: 9.600000000000001, Loss: 5.713464736938477\n",
      "Optimizing model...\n",
      "Step: 11, Action: 1, Reward: 0.0, Total Reward: 9.600000000000001, Loss: 6.898242950439453\n",
      "Optimizing model...\n",
      "Step: 12, Action: 0, Reward: 2.0, Total Reward: 11.600000000000001, Loss: 3.7252962589263916\n",
      "\n",
      "episode: 34, reward: -88.4\n",
      "\n",
      "Optimizing model...\n",
      "Step: 0, Action: 4, Reward: 0.2, Total Reward: 0.2, Loss: 3.7104625701904297\n",
      "Optimizing model...\n",
      "Step: 1, Action: 3, Reward: 0.25, Total Reward: 0.45, Loss: 0.644923985004425\n",
      "Optimizing model...\n",
      "Step: 2, Action: 4, Reward: 0.2, Total Reward: 0.65, Loss: 3.7920570373535156\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: 2.0, Total Reward: 2.65, Loss: 5.466196060180664\n",
      "Optimizing model...\n",
      "Step: 4, Action: 2, Reward: 0.25, Total Reward: 2.9, Loss: 4.900640487670898\n",
      "Optimizing model...\n",
      "Step: 5, Action: 1, Reward: 0.0, Total Reward: 2.9, Loss: 2.0914554595947266\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 6, Action: 0, Reward: 2.0, Total Reward: 4.9, Loss: 3.8596057891845703\n",
      "Optimizing model...\n",
      "Step: 7, Action: 4, Reward: 0.3, Total Reward: 5.2, Loss: 6.941869735717773\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 8, Action: 0, Reward: 2.0, Total Reward: 7.2, Loss: 0.9102497100830078\n",
      "Optimizing model...\n",
      "Step: 9, Action: 1, Reward: 0.0, Total Reward: 7.2, Loss: 6.921945571899414\n",
      "Optimizing model...\n",
      "Step: 10, Action: 1, Reward: 0.0, Total Reward: 7.2, Loss: 5.448166847229004\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 11, Action: 0, Reward: 2.0, Total Reward: 9.2, Loss: 2.186845302581787\n",
      "Optimizing model...\n",
      "Step: 12, Action: 1, Reward: 0.0, Total Reward: 9.2, Loss: 4.601947784423828\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 13, Action: 0, Reward: 2.0, Total Reward: 11.2, Loss: 3.364379644393921\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 14, Action: 0, Reward: -8.0, Total Reward: 3.1999999999999993, Loss: 5.114038944244385\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 15, Action: 3, Reward: 0.25, Total Reward: 3.4499999999999993, Loss: 0.6338252425193787\n",
      "Optimizing model...\n",
      "Step: 16, Action: 2, Reward: 0.25, Total Reward: 3.6999999999999993, Loss: 1.924615740776062\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 17, Action: 3, Reward: 0.25, Total Reward: 3.9499999999999993, Loss: 5.204697608947754\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 18, Action: 3, Reward: 0.25, Total Reward: 4.199999999999999, Loss: 2.187546968460083\n",
      "Optimizing model...\n",
      "Step: 19, Action: 0, Reward: 2.0, Total Reward: 6.199999999999999, Loss: 2.072381019592285\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 20, Action: 1, Reward: 0.0, Total Reward: 6.199999999999999, Loss: 2.3737716674804688\n",
      "Optimizing model...\n",
      "Step: 21, Action: 2, Reward: 0.25, Total Reward: 6.449999999999999, Loss: 3.6245944499969482\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 22, Action: 1, Reward: 0.0, Total Reward: 6.449999999999999, Loss: 5.353470802307129\n",
      "Optimizing model...\n",
      "Step: 23, Action: 4, Reward: 0.2, Total Reward: 6.6499999999999995, Loss: 2.0197558403015137\n",
      "Optimizing model...\n",
      "Step: 24, Action: 3, Reward: 0.25, Total Reward: 6.8999999999999995, Loss: 5.133118629455566\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 25, Action: 1, Reward: 0.0, Total Reward: 6.8999999999999995, Loss: 2.2447667121887207\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 26, Action: 1, Reward: 0.0, Total Reward: 6.8999999999999995, Loss: 2.2081801891326904\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 27, Action: 4, Reward: 0.2, Total Reward: 7.1, Loss: 2.169532537460327\n",
      "Optimizing model...\n",
      "Step: 28, Action: 4, Reward: -9.8, Total Reward: -2.700000000000001, Loss: 0.21090078353881836\n",
      "Optimizing model...\n",
      "Step: 29, Action: 4, Reward: -9.8, Total Reward: -12.500000000000002, Loss: 2.3363330364227295\n",
      "Optimizing model...\n",
      "Step: 30, Action: 3, Reward: 0.25, Total Reward: -12.250000000000002, Loss: 0.39268362522125244\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 31, Action: 4, Reward: 0.2, Total Reward: -12.050000000000002, Loss: 3.362562417984009\n",
      "Optimizing model...\n",
      "Step: 32, Action: 4, Reward: 0.2, Total Reward: -11.850000000000003, Loss: 3.924032688140869\n",
      "Optimizing model...\n",
      "Step: 33, Action: 1, Reward: 0.0, Total Reward: -11.850000000000003, Loss: 3.8461179733276367\n",
      "Optimizing model...\n",
      "Step: 34, Action: 4, Reward: 0.2, Total Reward: -11.650000000000004, Loss: 1.9112282991409302\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 35, Action: 4, Reward: -9.8, Total Reward: -21.450000000000003, Loss: 0.3495514392852783\n",
      "\n",
      "episode: 35, reward: -121.45\n",
      "\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: 2.0, Total Reward: 2.0, Loss: 6.275730609893799\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 1, Action: 1, Reward: 0.0, Total Reward: 2.0, Loss: 7.108059883117676\n",
      "Optimizing model...\n",
      "Step: 2, Action: 1, Reward: 0.0, Total Reward: 2.0, Loss: 5.6102399826049805\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: 2.0, Total Reward: 4.0, Loss: 0.528386652469635\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: 2.1, Total Reward: 6.1, Loss: 2.5748376846313477\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 5, Action: 0, Reward: 2.0, Total Reward: 8.1, Loss: 5.688713073730469\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 6, Action: 0, Reward: 2.0, Total Reward: 10.1, Loss: 3.8311469554901123\n",
      "Optimizing model...\n",
      "Step: 7, Action: 1, Reward: 0.1, Total Reward: 10.2, Loss: 0.7676669359207153\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 8, Action: 0, Reward: 2.0, Total Reward: 12.2, Loss: 0.4730266332626343\n",
      "Optimizing model...\n",
      "Step: 9, Action: 0, Reward: 2.0, Total Reward: 14.2, Loss: 0.2222501039505005\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 10, Action: 0, Reward: 2.0, Total Reward: 16.2, Loss: 3.7242894172668457\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 11, Action: 0, Reward: -8.0, Total Reward: 8.2, Loss: 0.5758253931999207\n",
      "Optimizing model...\n",
      "Step: 12, Action: 2, Reward: 0.25, Total Reward: 8.45, Loss: 2.285130500793457\n",
      "\n",
      "episode: 36, reward: -91.55\n",
      "\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: -8.0, Total Reward: -8.0, Loss: 5.952090263366699\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: 2.0, Total Reward: -6.0, Loss: 6.517189979553223\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: -8.0, Total Reward: -14.0, Loss: 3.5476579666137695\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: 2.0, Total Reward: -12.0, Loss: 3.626279830932617\n",
      "Optimizing model...\n",
      "Step: 4, Action: 3, Reward: 0.25, Total Reward: -11.75, Loss: 0.7844683527946472\n",
      "Optimizing model...\n",
      "Step: 5, Action: 0, Reward: 2.0, Total Reward: -9.75, Loss: 5.011595726013184\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 6, Action: 0, Reward: 2.0, Total Reward: -7.75, Loss: 5.674519062042236\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 7, Action: 0, Reward: -8.0, Total Reward: -15.75, Loss: 3.448587417602539\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 8, Action: 0, Reward: 2.0, Total Reward: -13.75, Loss: 0.5060089826583862\n",
      "Optimizing model...\n",
      "Step: 9, Action: 0, Reward: 2.0, Total Reward: -11.75, Loss: 3.9496731758117676\n",
      "Model acting\n",
      "\n",
      "episode: 37, reward: -111.75\n",
      "\n",
      "Optimizing model...\n",
      "Step: 0, Action: 1, Reward: 0.0, Total Reward: 0.0, Loss: 2.1360039710998535\n",
      "Optimizing model...\n",
      "Step: 1, Action: 4, Reward: 0.2, Total Reward: 0.2, Loss: 3.745851516723633\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: 2.0, Total Reward: 2.2, Loss: 3.634708881378174\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: -7.9, Total Reward: -5.7, Loss: 9.068559646606445\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: -8.0, Total Reward: -13.7, Loss: 0.6416410207748413\n",
      "Optimizing model...\n",
      "Step: 5, Action: 0, Reward: 2.0, Total Reward: -11.7, Loss: 6.050024509429932\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 6, Action: 0, Reward: 2.0, Total Reward: -9.7, Loss: 6.641411781311035\n",
      "\n",
      "episode: 38, reward: -109.7\n",
      "\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 3, Reward: 0.25, Total Reward: 0.25, Loss: 8.438201904296875\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 1, Action: 3, Reward: 0.25, Total Reward: 0.5, Loss: 0.45521432161331177\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 3, Reward: 0.25, Total Reward: 0.75, Loss: 3.332429885864258\n",
      "Optimizing model...\n",
      "Step: 3, Action: 2, Reward: 0.25, Total Reward: 1.0, Loss: 3.5766406059265137\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: 2.0, Total Reward: 3.0, Loss: 3.6546449661254883\n",
      "Optimizing model...\n",
      "Step: 5, Action: 1, Reward: 0.1, Total Reward: 3.1, Loss: 0.7425150275230408\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 6, Action: 0, Reward: 2.0, Total Reward: 5.1, Loss: 3.5796873569488525\n",
      "Optimizing model...\n",
      "Step: 7, Action: 1, Reward: 0.0, Total Reward: 5.1, Loss: 2.947179079055786\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 8, Action: 0, Reward: 2.0, Total Reward: 7.1, Loss: 3.821521043777466\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 9, Action: 0, Reward: 2.0, Total Reward: 9.1, Loss: 0.922971248626709\n",
      "Optimizing model...\n",
      "Step: 10, Action: 4, Reward: 0.2, Total Reward: 9.299999999999999, Loss: 0.2948179244995117\n",
      "Optimizing model...\n",
      "Step: 11, Action: 3, Reward: 0.25, Total Reward: 9.549999999999999, Loss: 3.7175939083099365\n",
      "Optimizing model...\n",
      "Step: 12, Action: 4, Reward: 0.2, Total Reward: 9.749999999999998, Loss: 7.662332057952881\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 13, Action: 2, Reward: 0.25, Total Reward: 9.999999999999998, Loss: 7.204456329345703\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 14, Action: 0, Reward: 2.0, Total Reward: 11.999999999999998, Loss: 5.7601823806762695\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 15, Action: 2, Reward: 0.25, Total Reward: 12.249999999999998, Loss: 1.940171718597412\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 16, Action: 0, Reward: 2.0, Total Reward: 14.249999999999998, Loss: 3.813469886779785\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 17, Action: 0, Reward: -8.0, Total Reward: 6.249999999999998, Loss: 0.9024612307548523\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 18, Action: 2, Reward: 0.25, Total Reward: 6.499999999999998, Loss: 3.4802050590515137\n",
      "Optimizing model...\n",
      "Step: 19, Action: 1, Reward: 0.0, Total Reward: 6.499999999999998, Loss: 5.061096668243408\n",
      "Optimizing model...\n",
      "Step: 20, Action: 4, Reward: 0.2, Total Reward: 6.699999999999998, Loss: 8.682228088378906\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 21, Action: 2, Reward: 0.25, Total Reward: 6.949999999999998, Loss: 2.3538458347320557\n",
      "Optimizing model...\n",
      "Step: 22, Action: 0, Reward: 2.0, Total Reward: 8.95, Loss: 0.4398784935474396\n",
      "Optimizing model...\n",
      "Step: 23, Action: 2, Reward: 0.25, Total Reward: 9.2, Loss: 3.396195411682129\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 24, Action: 2, Reward: 0.25, Total Reward: 9.45, Loss: 3.9856534004211426\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 25, Action: 2, Reward: -9.75, Total Reward: -0.3000000000000007, Loss: 2.0201895236968994\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 26, Action: 3, Reward: 0.25, Total Reward: -0.05000000000000071, Loss: 3.866502285003662\n",
      "Optimizing model...\n",
      "Step: 27, Action: 4, Reward: 0.2, Total Reward: 0.1499999999999993, Loss: 3.5821375846862793\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 28, Action: 3, Reward: 0.25, Total Reward: 0.3999999999999993, Loss: 3.347830057144165\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 29, Action: 2, Reward: 0.25, Total Reward: 0.6499999999999992, Loss: 6.555514335632324\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 30, Action: 3, Reward: 0.25, Total Reward: 0.8999999999999992, Loss: 5.704270362854004\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 31, Action: 3, Reward: -9.75, Total Reward: -8.850000000000001, Loss: 4.19322395324707\n",
      "Optimizing model...\n",
      "Step: 32, Action: 3, Reward: -9.75, Total Reward: -18.6, Loss: 3.988845109939575\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 33, Action: 3, Reward: 0.25, Total Reward: -18.35, Loss: 2.0219902992248535\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 34, Action: 2, Reward: 0.25, Total Reward: -18.1, Loss: 2.1926209926605225\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 35, Action: 2, Reward: -9.75, Total Reward: -27.85, Loss: 6.821583271026611\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 36, Action: 2, Reward: -9.75, Total Reward: -37.6, Loss: 3.892580509185791\n",
      "Optimizing model...\n",
      "Step: 37, Action: 2, Reward: 0.25, Total Reward: -37.35, Loss: 3.8303394317626953\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 38, Action: 2, Reward: 0.25, Total Reward: -37.1, Loss: 2.028918743133545\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 39, Action: 2, Reward: 0.25, Total Reward: -36.85, Loss: 1.8603270053863525\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 40, Action: 2, Reward: -9.65, Total Reward: -46.5, Loss: 1.950229287147522\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 41, Action: 0, Reward: 2.0, Total Reward: -44.5, Loss: 0.58609938621521\n",
      "Optimizing model...\n",
      "Step: 42, Action: 0, Reward: 2.0, Total Reward: -42.5, Loss: 6.876330375671387\n",
      "Optimizing model...\n",
      "Step: 43, Action: 2, Reward: 0.25, Total Reward: -42.25, Loss: 2.2020487785339355\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 44, Action: 0, Reward: 2.0, Total Reward: -40.25, Loss: 3.845111131668091\n",
      "Optimizing model...\n",
      "Step: 45, Action: 0, Reward: 2.0, Total Reward: -38.25, Loss: 0.6446616649627686\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 46, Action: 2, Reward: 0.25, Total Reward: -38.0, Loss: 3.9371159076690674\n",
      "Optimizing model...\n",
      "Step: 47, Action: 2, Reward: 0.25, Total Reward: -37.75, Loss: 2.303990602493286\n",
      "Optimizing model...\n",
      "Step: 48, Action: 1, Reward: 0.0, Total Reward: -37.75, Loss: 4.28691291809082\n",
      "Optimizing model...\n",
      "Step: 49, Action: 0, Reward: 2.0, Total Reward: -35.75, Loss: 3.929701805114746\n",
      "Model acting\n",
      "\n",
      "episode: 39, reward: -135.75\n",
      "\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: -8.0, Total Reward: -8.0, Loss: 0.4715633988380432\n",
      "Optimizing model...\n",
      "Step: 1, Action: 4, Reward: 0.2, Total Reward: -7.8, Loss: 6.140676021575928\n",
      "Optimizing model...\n",
      "Step: 2, Action: 3, Reward: 0.25, Total Reward: -7.55, Loss: 2.25850772857666\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 2, Reward: 0.25, Total Reward: -7.3, Loss: 3.889948606491089\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: 2.0, Total Reward: -5.3, Loss: 4.9849853515625\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 5, Action: 2, Reward: 0.25, Total Reward: -5.05, Loss: 8.17106819152832\n",
      "Optimizing model...\n",
      "Step: 6, Action: 1, Reward: 0.0, Total Reward: -5.05, Loss: 2.410159111022949\n",
      "Optimizing model...\n",
      "Step: 7, Action: 4, Reward: 0.2, Total Reward: -4.85, Loss: 4.801487922668457\n",
      "Optimizing model...\n",
      "Step: 8, Action: 4, Reward: -9.8, Total Reward: -14.65, Loss: 4.248988151550293\n",
      "\n",
      "episode: 40, reward: -114.65\n",
      "\n",
      "Model saved\n",
      "Optimizing model...\n",
      "Step: 0, Action: 3, Reward: 0.25, Total Reward: 0.25, Loss: 5.027159690856934\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: 2.0, Total Reward: 2.25, Loss: 3.5614049434661865\n",
      "Optimizing model...\n",
      "Step: 2, Action: 4, Reward: 0.2, Total Reward: 2.45, Loss: 2.3379006385803223\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: 2.1, Total Reward: 4.550000000000001, Loss: 5.359347343444824\n",
      "Optimizing model...\n",
      "Step: 4, Action: 2, Reward: 0.25, Total Reward: 4.800000000000001, Loss: 2.280968189239502\n",
      "Optimizing model...\n",
      "Step: 5, Action: 4, Reward: 0.2, Total Reward: 5.000000000000001, Loss: 2.0440664291381836\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 6, Action: 0, Reward: 2.0, Total Reward: 7.000000000000001, Loss: 3.498562812805176\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 7, Action: 0, Reward: 2.0, Total Reward: 9.0, Loss: 0.7970588207244873\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 8, Action: 0, Reward: 2.0, Total Reward: 11.0, Loss: 3.643221855163574\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 9, Action: 0, Reward: 2.0, Total Reward: 13.0, Loss: 2.498201370239258\n",
      "Optimizing model...\n",
      "Step: 10, Action: 2, Reward: 0.25, Total Reward: 13.25, Loss: 8.319122314453125\n",
      "Optimizing model...\n",
      "Step: 11, Action: 3, Reward: 0.25, Total Reward: 13.5, Loss: 2.478168487548828\n",
      "Model acting\n",
      "\n",
      "episode: 41, reward: -86.5\n",
      "\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: 2.0, Total Reward: 2.0, Loss: 3.7726552486419678\n",
      "Optimizing model...\n",
      "Step: 1, Action: 3, Reward: 0.25, Total Reward: 2.25, Loss: 2.4721386432647705\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: 2.0, Total Reward: 4.25, Loss: 1.8923430442810059\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: -8.0, Total Reward: -3.75, Loss: 3.8343255519866943\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: 2.1, Total Reward: -1.65, Loss: 0.8927615880966187\n",
      "Optimizing model...\n",
      "Step: 5, Action: 4, Reward: 0.2, Total Reward: -1.45, Loss: 5.639200210571289\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 6, Action: 0, Reward: 2.0, Total Reward: 0.55, Loss: 0.5650326609611511\n",
      "Optimizing model...\n",
      "Step: 7, Action: 1, Reward: 0.0, Total Reward: 0.55, Loss: 3.891660690307617\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 8, Action: 0, Reward: 2.0, Total Reward: 2.55, Loss: 0.1956067979335785\n",
      "Optimizing model...\n",
      "Step: 9, Action: 1, Reward: 0.0, Total Reward: 2.55, Loss: 2.3965888023376465\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 10, Action: 0, Reward: 2.1, Total Reward: 4.65, Loss: 1.0605496168136597\n",
      "Optimizing model...\n",
      "Step: 11, Action: 1, Reward: 0.0, Total Reward: 4.65, Loss: 0.412567138671875\n",
      "Optimizing model...\n",
      "Step: 12, Action: 4, Reward: 0.2, Total Reward: 4.8500000000000005, Loss: 5.467280387878418\n",
      "Optimizing model...\n",
      "Step: 13, Action: 0, Reward: 2.0, Total Reward: 6.8500000000000005, Loss: 4.04508638381958\n",
      "Optimizing model...\n",
      "Step: 14, Action: 2, Reward: 0.25, Total Reward: 7.1000000000000005, Loss: 5.449675559997559\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 15, Action: 0, Reward: 2.0, Total Reward: 9.100000000000001, Loss: 0.6398018598556519\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 16, Action: 0, Reward: -8.0, Total Reward: 1.1000000000000014, Loss: 5.103948593139648\n",
      "Optimizing model...\n",
      "Step: 17, Action: 2, Reward: 0.25, Total Reward: 1.3500000000000014, Loss: 8.889469146728516\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 18, Action: 0, Reward: 2.0, Total Reward: 3.3500000000000014, Loss: 5.582663536071777\n",
      "Optimizing model...\n",
      "Step: 19, Action: 0, Reward: -8.0, Total Reward: -4.649999999999999, Loss: 4.081992149353027\n",
      "Optimizing model...\n",
      "Step: 20, Action: 0, Reward: 2.0, Total Reward: -2.6499999999999986, Loss: 1.9479718208312988\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 21, Action: 0, Reward: -8.0, Total Reward: -10.649999999999999, Loss: 0.7077218294143677\n",
      "Optimizing model...\n",
      "Step: 22, Action: 1, Reward: 0.0, Total Reward: -10.649999999999999, Loss: 2.068631649017334\n",
      "Optimizing model...\n",
      "Step: 23, Action: 1, Reward: -10.0, Total Reward: -20.65, Loss: 5.8791184425354\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 24, Action: 0, Reward: 2.0, Total Reward: -18.65, Loss: 3.6308352947235107\n",
      "Optimizing model...\n",
      "Step: 25, Action: 2, Reward: 0.35, Total Reward: -18.299999999999997, Loss: 3.593801498413086\n",
      "Optimizing model...\n",
      "Step: 26, Action: 0, Reward: 2.0, Total Reward: -16.299999999999997, Loss: 0.3223867118358612\n",
      "Optimizing model...\n",
      "Step: 27, Action: 4, Reward: 0.2, Total Reward: -16.099999999999998, Loss: 6.433589935302734\n",
      "Optimizing model...\n",
      "Step: 28, Action: 4, Reward: 0.2, Total Reward: -15.899999999999999, Loss: 0.592710018157959\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 29, Action: 0, Reward: 2.0, Total Reward: -13.899999999999999, Loss: 7.980851173400879\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 30, Action: 0, Reward: 2.0, Total Reward: -11.899999999999999, Loss: 3.58955717086792\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 31, Action: 0, Reward: -8.0, Total Reward: -19.9, Loss: 0.46471327543258667\n",
      "Optimizing model...\n",
      "Step: 32, Action: 0, Reward: 2.1, Total Reward: -17.799999999999997, Loss: 2.240352153778076\n",
      "Optimizing model...\n",
      "Step: 33, Action: 2, Reward: 0.25, Total Reward: -17.549999999999997, Loss: 3.875217914581299\n",
      "\n",
      "episode: 42, reward: -117.55\n",
      "\n",
      "Optimizing model...\n",
      "Step: 0, Action: 3, Reward: 0.25, Total Reward: 0.25, Loss: 3.5306572914123535\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: 2.0, Total Reward: 2.25, Loss: 5.973837852478027\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: -8.0, Total Reward: -5.75, Loss: 5.369575500488281\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: -8.0, Total Reward: -13.75, Loss: 8.747066497802734\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: 2.0, Total Reward: -11.75, Loss: 2.1436429023742676\n",
      "Optimizing model...\n",
      "Step: 5, Action: 2, Reward: 0.25, Total Reward: -11.5, Loss: 4.209506511688232\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 6, Action: 0, Reward: 2.0, Total Reward: -9.5, Loss: 3.54986572265625\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 7, Action: 0, Reward: 2.0, Total Reward: -7.5, Loss: 2.1690220832824707\n",
      "Optimizing model...\n",
      "Step: 8, Action: 2, Reward: 0.25, Total Reward: -7.25, Loss: 5.3890380859375\n",
      "Model acting\n",
      "\n",
      "episode: 43, reward: -107.25\n",
      "\n",
      "Optimizing model...\n",
      "Step: 0, Action: 3, Reward: 0.25, Total Reward: 0.25, Loss: 3.775923490524292\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: 2.0, Total Reward: 2.25, Loss: 0.6131089329719543\n",
      "Optimizing model...\n",
      "Step: 2, Action: 1, Reward: 0.0, Total Reward: 2.25, Loss: 3.803849935531616\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: 2.0, Total Reward: 4.25, Loss: 0.2739490270614624\n",
      "Optimizing model...\n",
      "Step: 4, Action: 1, Reward: 0.0, Total Reward: 4.25, Loss: 5.327737808227539\n",
      "Optimizing model...\n",
      "Step: 5, Action: 0, Reward: 2.0, Total Reward: 6.25, Loss: 1.8607476949691772\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 6, Action: 0, Reward: -8.0, Total Reward: -1.75, Loss: 0.6070995330810547\n",
      "Optimizing model...\n",
      "Step: 7, Action: 0, Reward: -8.0, Total Reward: -9.75, Loss: 5.9009480476379395\n",
      "Optimizing model...\n",
      "Step: 8, Action: 3, Reward: 0.25, Total Reward: -9.5, Loss: 3.683846950531006\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 9, Action: 0, Reward: 2.0, Total Reward: -7.5, Loss: 2.1395788192749023\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 10, Action: 0, Reward: -8.0, Total Reward: -15.5, Loss: 1.0833975076675415\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 11, Action: 0, Reward: 2.0, Total Reward: -13.5, Loss: 2.3535685539245605\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 12, Action: 0, Reward: -8.0, Total Reward: -21.5, Loss: 5.696938514709473\n",
      "Optimizing model...\n",
      "Step: 13, Action: 3, Reward: 0.25, Total Reward: -21.25, Loss: 4.138994216918945\n",
      "Optimizing model...\n",
      "Step: 14, Action: 2, Reward: 0.25, Total Reward: -21.0, Loss: 6.516871452331543\n",
      "\n",
      "episode: 44, reward: -121.0\n",
      "\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: -8.0, Total Reward: -8.0, Loss: 2.2915215492248535\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: 2.1, Total Reward: -5.9, Loss: 2.5147104263305664\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: 2.0, Total Reward: -3.9000000000000004, Loss: 2.3889236450195312\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: 2.0, Total Reward: -1.9000000000000004, Loss: 3.59269380569458\n",
      "\n",
      "episode: 45, reward: -101.9\n",
      "\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: -8.0, Total Reward: -8.0, Loss: 0.9574729204177856\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: 2.0, Total Reward: -6.0, Loss: 5.210838794708252\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: 2.0, Total Reward: -4.0, Loss: 6.470173358917236\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: 2.0, Total Reward: -2.0, Loss: 1.8783109188079834\n",
      "Optimizing model...\n",
      "Step: 4, Action: 4, Reward: 0.2, Total Reward: -1.8, Loss: 2.3148298263549805\n",
      "Optimizing model...\n",
      "Step: 5, Action: 2, Reward: 0.25, Total Reward: -1.55, Loss: 0.44834184646606445\n",
      "\n",
      "episode: 46, reward: -101.55\n",
      "\n",
      "Optimizing model...\n",
      "Step: 0, Action: 4, Reward: 0.2, Total Reward: 0.2, Loss: 3.7278995513916016\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: 2.0, Total Reward: 2.2, Loss: 3.5195178985595703\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: 2.0, Total Reward: 4.2, Loss: 2.135695695877075\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: 2.0, Total Reward: 6.2, Loss: 3.4379849433898926\n",
      "Optimizing model...\n",
      "Step: 4, Action: 4, Reward: 0.2, Total Reward: 6.4, Loss: 4.148637771606445\n",
      "Optimizing model...\n",
      "Step: 5, Action: 2, Reward: 0.25, Total Reward: 6.65, Loss: 7.591088771820068\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 6, Action: 0, Reward: 2.0, Total Reward: 8.65, Loss: 3.696324348449707\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 7, Action: 0, Reward: 2.0, Total Reward: 10.65, Loss: 3.687511920928955\n",
      "Optimizing model...\n",
      "Step: 8, Action: 2, Reward: 0.25, Total Reward: 10.9, Loss: 3.9280357360839844\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 9, Action: 0, Reward: 2.0, Total Reward: 12.9, Loss: 3.5940699577331543\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 10, Action: 0, Reward: -7.9, Total Reward: 5.0, Loss: 2.21114444732666\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 11, Action: 0, Reward: 2.0, Total Reward: 7.0, Loss: 5.567314147949219\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 12, Action: 0, Reward: 2.0, Total Reward: 9.0, Loss: 2.127870559692383\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 13, Action: 0, Reward: 2.0, Total Reward: 11.0, Loss: 2.063877820968628\n",
      "Optimizing model...\n",
      "Step: 14, Action: 0, Reward: 2.0, Total Reward: 13.0, Loss: 5.002378940582275\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 15, Action: 0, Reward: 2.0, Total Reward: 15.0, Loss: 6.330549716949463\n",
      "Optimizing model...\n",
      "Step: 16, Action: 4, Reward: 0.2, Total Reward: 15.2, Loss: 0.3131749629974365\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 17, Action: 0, Reward: 2.0, Total Reward: 17.2, Loss: 5.519345760345459\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 18, Action: 0, Reward: 2.0, Total Reward: 19.2, Loss: 7.2978620529174805\n",
      "Optimizing model...\n",
      "Step: 19, Action: 0, Reward: 2.0, Total Reward: 21.2, Loss: 2.006418228149414\n",
      "Model acting\n",
      "\n",
      "episode: 47, reward: -78.8\n",
      "\n",
      "Optimizing model...\n",
      "Step: 0, Action: 4, Reward: 0.2, Total Reward: 0.2, Loss: 5.434065818786621\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: 2.0, Total Reward: 2.2, Loss: 2.2834343910217285\n",
      "Optimizing model...\n",
      "Step: 2, Action: 4, Reward: 0.2, Total Reward: 2.4000000000000004, Loss: 3.6265993118286133\n",
      "Optimizing model...\n",
      "Step: 3, Action: 1, Reward: 0.0, Total Reward: 2.4000000000000004, Loss: 7.542725563049316\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: 2.0, Total Reward: 4.4, Loss: 5.63060188293457\n",
      "Optimizing model...\n",
      "Step: 5, Action: 1, Reward: 0.0, Total Reward: 4.4, Loss: 2.172429084777832\n",
      "Optimizing model...\n",
      "Step: 6, Action: 2, Reward: 0.25, Total Reward: 4.65, Loss: 5.134791851043701\n",
      "Optimizing model...\n",
      "Step: 7, Action: 0, Reward: 2.0, Total Reward: 6.65, Loss: 3.562131881713867\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 8, Action: 0, Reward: -8.0, Total Reward: -1.3499999999999996, Loss: 3.4608240127563477\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 9, Action: 0, Reward: -8.0, Total Reward: -9.35, Loss: 4.886499404907227\n",
      "Optimizing model...\n",
      "Step: 10, Action: 0, Reward: -8.0, Total Reward: -17.35, Loss: 0.4736820161342621\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 11, Action: 0, Reward: 2.0, Total Reward: -15.350000000000001, Loss: 5.083631992340088\n",
      "Optimizing model...\n",
      "Step: 12, Action: 2, Reward: 0.25, Total Reward: -15.100000000000001, Loss: 5.840785980224609\n",
      "Optimizing model...\n",
      "Step: 13, Action: 2, Reward: 0.25, Total Reward: -14.850000000000001, Loss: 4.005017280578613\n",
      "Optimizing model...\n",
      "Step: 14, Action: 2, Reward: -9.75, Total Reward: -24.6, Loss: 0.8275357484817505\n",
      "Optimizing model...\n",
      "Step: 15, Action: 2, Reward: 0.25, Total Reward: -24.35, Loss: 2.3557002544403076\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 16, Action: 0, Reward: 2.0, Total Reward: -22.35, Loss: 2.398521661758423\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 17, Action: 0, Reward: 2.0, Total Reward: -20.35, Loss: 2.2337403297424316\n",
      "Optimizing model...\n",
      "Step: 18, Action: 3, Reward: 0.25, Total Reward: -20.1, Loss: 3.9557251930236816\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 19, Action: 0, Reward: 2.0, Total Reward: -18.1, Loss: 2.599111557006836\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 20, Action: 0, Reward: 2.0, Total Reward: -16.1, Loss: 6.714597225189209\n",
      "Optimizing model...\n",
      "Step: 21, Action: 2, Reward: 0.25, Total Reward: -15.850000000000001, Loss: 2.3645803928375244\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 22, Action: 0, Reward: 2.0, Total Reward: -13.850000000000001, Loss: 5.798395156860352\n",
      "Optimizing model...\n",
      "Step: 23, Action: 2, Reward: 0.25, Total Reward: -13.600000000000001, Loss: 0.23235563933849335\n",
      "Optimizing model...\n",
      "Step: 24, Action: 4, Reward: 0.2, Total Reward: -13.400000000000002, Loss: 1.9842504262924194\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 25, Action: 0, Reward: 2.0, Total Reward: -11.400000000000002, Loss: 0.6991859674453735\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 26, Action: 0, Reward: -8.0, Total Reward: -19.400000000000002, Loss: 3.751312732696533\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 27, Action: 0, Reward: 2.0, Total Reward: -17.400000000000002, Loss: 3.7078003883361816\n",
      "\n",
      "episode: 48, reward: -117.4\n",
      "\n",
      "Optimizing model...\n",
      "Step: 0, Action: 2, Reward: 0.25, Total Reward: 0.25, Loss: 3.401362895965576\n",
      "Optimizing model...\n",
      "Step: 1, Action: 2, Reward: 0.25, Total Reward: 0.5, Loss: 3.8457274436950684\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: 2.0, Total Reward: 2.5, Loss: 1.8012641668319702\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: 2.0, Total Reward: 4.5, Loss: 2.3928327560424805\n",
      "Optimizing model...\n",
      "Step: 4, Action: 2, Reward: 0.25, Total Reward: 4.75, Loss: 4.107701301574707\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 5, Action: 0, Reward: 2.0, Total Reward: 6.75, Loss: 2.199230670928955\n",
      "Optimizing model...\n",
      "Step: 6, Action: 2, Reward: 0.25, Total Reward: 7.0, Loss: 2.518075942993164\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 7, Action: 0, Reward: 2.0, Total Reward: 9.0, Loss: 3.3166298866271973\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 8, Action: 0, Reward: 2.0, Total Reward: 11.0, Loss: 4.092440605163574\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 9, Action: 0, Reward: 2.0, Total Reward: 13.0, Loss: 2.111473560333252\n",
      "Optimizing model...\n",
      "Step: 10, Action: 3, Reward: 0.25, Total Reward: 13.25, Loss: 3.7548232078552246\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 11, Action: 0, Reward: 2.0, Total Reward: 15.25, Loss: 2.3068785667419434\n",
      "Optimizing model...\n",
      "Step: 12, Action: 0, Reward: 2.0, Total Reward: 17.25, Loss: 2.2732973098754883\n",
      "Model acting\n",
      "\n",
      "episode: 49, reward: -82.75\n",
      "\n",
      "Optimizing model...\n",
      "Step: 0, Action: 3, Reward: 0.25, Total Reward: 0.25, Loss: 3.6288766860961914\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: 2.0, Total Reward: 2.25, Loss: 2.76670503616333\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: 2.0, Total Reward: 4.25, Loss: 0.6260486841201782\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: -7.9, Total Reward: -3.6500000000000004, Loss: 5.456733703613281\n",
      "Optimizing model...\n",
      "Step: 4, Action: 1, Reward: 0.0, Total Reward: -3.6500000000000004, Loss: 3.531402111053467\n",
      "Optimizing model...\n",
      "Step: 5, Action: 3, Reward: 0.25, Total Reward: -3.4000000000000004, Loss: 5.284750938415527\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 6, Action: 0, Reward: 2.0, Total Reward: -1.4000000000000004, Loss: 3.345918655395508\n",
      "Optimizing model...\n",
      "Step: 7, Action: 4, Reward: 0.2, Total Reward: -1.2000000000000004, Loss: 4.13346004486084\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 8, Action: 0, Reward: 2.0, Total Reward: 0.7999999999999996, Loss: 4.030924320220947\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 9, Action: 0, Reward: -8.0, Total Reward: -7.2, Loss: 2.261582374572754\n",
      "Optimizing model...\n",
      "Step: 10, Action: 4, Reward: 0.2, Total Reward: -7.0, Loss: 2.461461067199707\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 11, Action: 0, Reward: 2.0, Total Reward: -5.0, Loss: 4.148412227630615\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 12, Action: 0, Reward: -8.0, Total Reward: -13.0, Loss: 6.873963356018066\n",
      "Optimizing model...\n",
      "Step: 13, Action: 2, Reward: 0.25, Total Reward: -12.75, Loss: 5.12141752243042\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 14, Action: 0, Reward: 2.0, Total Reward: -10.75, Loss: 0.42924386262893677\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 15, Action: 0, Reward: -8.0, Total Reward: -18.75, Loss: 6.5233025550842285\n",
      "Model acting\n",
      "\n",
      "episode: 50, reward: -118.75\n",
      "\n",
      "Model saved\n",
      "Optimizing model...\n",
      "Step: 0, Action: 1, Reward: 0.0, Total Reward: 0.0, Loss: 0.980776309967041\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: 2.0, Total Reward: 2.0, Loss: 5.229694843292236\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: 2.0, Total Reward: 4.0, Loss: 5.124496936798096\n",
      "Optimizing model...\n",
      "Step: 3, Action: 3, Reward: 0.25, Total Reward: 4.25, Loss: 0.9286422729492188\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: 2.0, Total Reward: 6.25, Loss: 2.3104965686798096\n",
      "Optimizing model...\n",
      "Step: 5, Action: 2, Reward: 0.25, Total Reward: 6.5, Loss: 4.593114852905273\n",
      "Optimizing model...\n",
      "Step: 6, Action: 1, Reward: 0.1, Total Reward: 6.6, Loss: 5.835586071014404\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 7, Action: 0, Reward: 2.0, Total Reward: 8.6, Loss: 3.88716983795166\n",
      "Optimizing model...\n",
      "Step: 8, Action: 2, Reward: 0.25, Total Reward: 8.85, Loss: 3.903852939605713\n",
      "Optimizing model...\n",
      "Step: 9, Action: 1, Reward: 0.0, Total Reward: 8.85, Loss: 3.9939255714416504\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 10, Action: 0, Reward: 2.0, Total Reward: 10.85, Loss: 2.1200013160705566\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 11, Action: 0, Reward: -8.0, Total Reward: 2.8499999999999996, Loss: 0.6106184124946594\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 12, Action: 0, Reward: 2.0, Total Reward: 4.85, Loss: 2.209451198577881\n",
      "\n",
      "episode: 51, reward: -95.15\n",
      "\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: 2.0, Total Reward: 2.0, Loss: 0.7370744943618774\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: -8.0, Total Reward: -6.0, Loss: 2.4160208702087402\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: -8.0, Total Reward: -14.0, Loss: 4.3277387619018555\n",
      "Optimizing model...\n",
      "Step: 3, Action: 1, Reward: 0.0, Total Reward: -14.0, Loss: 4.737292766571045\n",
      "Optimizing model...\n",
      "Step: 4, Action: 1, Reward: -10.0, Total Reward: -24.0, Loss: 8.566510200500488\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 5, Action: 0, Reward: 2.0, Total Reward: -22.0, Loss: 4.0173797607421875\n",
      "Optimizing model...\n",
      "Step: 6, Action: 0, Reward: 2.0, Total Reward: -20.0, Loss: 4.93425989151001\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 7, Action: 0, Reward: 2.0, Total Reward: -18.0, Loss: 3.5661048889160156\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 8, Action: 0, Reward: 2.0, Total Reward: -16.0, Loss: 0.35116201639175415\n",
      "\n",
      "episode: 52, reward: -116.0\n",
      "\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: -7.9, Total Reward: -7.9, Loss: 3.505455255508423\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: -8.0, Total Reward: -15.9, Loss: 2.099592924118042\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: -8.0, Total Reward: -23.9, Loss: 0.6755220890045166\n",
      "Optimizing model...\n",
      "Step: 3, Action: 2, Reward: 0.25, Total Reward: -23.65, Loss: 8.71114730834961\n",
      "Optimizing model...\n",
      "Step: 4, Action: 4, Reward: 0.2, Total Reward: -23.45, Loss: 2.6161675453186035\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 5, Action: 0, Reward: 2.0, Total Reward: -21.45, Loss: 5.250585079193115\n",
      "Model acting\n",
      "\n",
      "episode: 53, reward: -121.45\n",
      "\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: 2.0, Total Reward: 2.0, Loss: 2.1992316246032715\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: 2.0, Total Reward: 4.0, Loss: 3.8000073432922363\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: 2.0, Total Reward: 6.0, Loss: 5.457691669464111\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: -8.0, Total Reward: -2.0, Loss: 3.053438663482666\n",
      "Optimizing model...\n",
      "Step: 4, Action: 1, Reward: 0.0, Total Reward: -2.0, Loss: 3.776527166366577\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 5, Action: 0, Reward: 2.0, Total Reward: 0.0, Loss: 5.056597709655762\n",
      "Optimizing model...\n",
      "Step: 6, Action: 0, Reward: 2.0, Total Reward: 2.0, Loss: 7.041598320007324\n",
      "Optimizing model...\n",
      "Step: 7, Action: 4, Reward: 0.2, Total Reward: 2.2, Loss: 2.0403852462768555\n",
      "Optimizing model...\n",
      "Step: 8, Action: 3, Reward: 0.25, Total Reward: 2.45, Loss: 3.7419395446777344\n",
      "Optimizing model...\n",
      "Step: 9, Action: 4, Reward: 0.2, Total Reward: 2.6500000000000004, Loss: 2.1892542839050293\n",
      "\n",
      "episode: 54, reward: -97.35\n",
      "\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: -8.0, Total Reward: -8.0, Loss: 1.0398123264312744\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: 2.0, Total Reward: -6.0, Loss: 2.568143844604492\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: 2.0, Total Reward: -4.0, Loss: 2.4186854362487793\n",
      "Optimizing model...\n",
      "Step: 3, Action: 4, Reward: 0.2, Total Reward: -3.8, Loss: 3.6696391105651855\n",
      "Optimizing model...\n",
      "Step: 4, Action: 1, Reward: 0.0, Total Reward: -3.8, Loss: 0.7070477604866028\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 5, Action: 0, Reward: 2.0, Total Reward: -1.7999999999999998, Loss: 5.028589248657227\n",
      "Optimizing model...\n",
      "Step: 6, Action: 2, Reward: 0.25, Total Reward: -1.5499999999999998, Loss: 4.969914436340332\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 7, Action: 0, Reward: 2.0, Total Reward: 0.4500000000000002, Loss: 5.255220890045166\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 8, Action: 0, Reward: 2.0, Total Reward: 2.45, Loss: 4.63247013092041\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 9, Action: 0, Reward: 2.0, Total Reward: 4.45, Loss: 4.255858421325684\n",
      "Optimizing model...\n",
      "Step: 10, Action: 0, Reward: 2.1, Total Reward: 6.550000000000001, Loss: 4.120840549468994\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 11, Action: 0, Reward: -8.0, Total Reward: -1.4499999999999993, Loss: 5.40325927734375\n",
      "Optimizing model...\n",
      "Step: 12, Action: 0, Reward: 2.0, Total Reward: 0.5500000000000007, Loss: 3.966062307357788\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 13, Action: 0, Reward: 2.0, Total Reward: 2.5500000000000007, Loss: 6.544891834259033\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 14, Action: 0, Reward: 2.0, Total Reward: 4.550000000000001, Loss: 4.383419990539551\n",
      "Optimizing model...\n",
      "Step: 15, Action: 2, Reward: 0.35, Total Reward: 4.9, Loss: 6.651052474975586\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 16, Action: 0, Reward: 2.0, Total Reward: 6.9, Loss: 3.8067996501922607\n",
      "Optimizing model...\n",
      "Step: 17, Action: 4, Reward: 0.2, Total Reward: 7.1000000000000005, Loss: 7.1393232345581055\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 18, Action: 0, Reward: 2.0, Total Reward: 9.100000000000001, Loss: 2.4687812328338623\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 19, Action: 0, Reward: 2.0, Total Reward: 11.100000000000001, Loss: 4.084990978240967\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 20, Action: 0, Reward: -8.0, Total Reward: 3.1000000000000014, Loss: 5.33163595199585\n",
      "Optimizing model...\n",
      "Step: 21, Action: 1, Reward: 0.0, Total Reward: 3.1000000000000014, Loss: 3.9392426013946533\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 22, Action: 0, Reward: 2.0, Total Reward: 5.100000000000001, Loss: 3.6457290649414062\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 23, Action: 0, Reward: 2.0, Total Reward: 7.100000000000001, Loss: 2.295292615890503\n",
      "Optimizing model...\n",
      "Step: 24, Action: 1, Reward: 0.1, Total Reward: 7.200000000000001, Loss: 3.657689094543457\n",
      "Optimizing model...\n",
      "Step: 25, Action: 4, Reward: 0.2, Total Reward: 7.400000000000001, Loss: 0.6538935303688049\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 26, Action: 0, Reward: 2.0, Total Reward: 9.400000000000002, Loss: 4.811490058898926\n",
      "Model acting\n",
      "\n",
      "episode: 55, reward: -90.6\n",
      "\n",
      "Optimizing model...\n",
      "Step: 0, Action: 3, Reward: 0.25, Total Reward: 0.25, Loss: 7.239987373352051\n",
      "Optimizing model...\n",
      "Step: 1, Action: 2, Reward: 0.25, Total Reward: 0.5, Loss: 0.7949384450912476\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: 2.0, Total Reward: 2.5, Loss: 5.175063133239746\n",
      "Optimizing model...\n",
      "Step: 3, Action: 3, Reward: 0.35, Total Reward: 2.85, Loss: 2.0631051063537598\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 4, Action: 2, Reward: 0.25, Total Reward: 3.1, Loss: 0.7531327605247498\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 5, Action: 2, Reward: 0.25, Total Reward: 3.35, Loss: 2.061216354370117\n",
      "Optimizing model...\n",
      "Step: 6, Action: 4, Reward: 0.2, Total Reward: 3.5500000000000003, Loss: 3.534722089767456\n",
      "Optimizing model...\n",
      "Step: 7, Action: 2, Reward: 0.25, Total Reward: 3.8000000000000003, Loss: 2.7149744033813477\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 8, Action: 0, Reward: 2.0, Total Reward: 5.800000000000001, Loss: 6.382036209106445\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 9, Action: 0, Reward: 2.0, Total Reward: 7.800000000000001, Loss: 2.0733370780944824\n",
      "Optimizing model...\n",
      "Step: 10, Action: 3, Reward: 0.25, Total Reward: 8.05, Loss: 0.7788794040679932\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 11, Action: 0, Reward: 2.0, Total Reward: 10.05, Loss: 2.947732925415039\n",
      "Model acting\n",
      "\n",
      "episode: 56, reward: -89.95\n",
      "\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: -8.0, Total Reward: -8.0, Loss: 3.2955198287963867\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 1, Action: 2, Reward: 0.25, Total Reward: -7.75, Loss: 6.890474319458008\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: 2.0, Total Reward: -5.75, Loss: 3.9231367111206055\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: 2.0, Total Reward: -3.75, Loss: 0.9552326798439026\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: 2.0, Total Reward: -1.75, Loss: 0.8635187745094299\n",
      "Model acting\n",
      "\n",
      "episode: 57, reward: -101.75\n",
      "\n",
      "Optimizing model...\n",
      "Step: 0, Action: 4, Reward: 0.2, Total Reward: 0.2, Loss: 4.6096954345703125\n",
      "Optimizing model...\n",
      "Step: 1, Action: 2, Reward: 0.25, Total Reward: 0.45, Loss: 2.117305278778076\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: 2.1, Total Reward: 2.5500000000000003, Loss: 2.3714470863342285\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: 2.0, Total Reward: 4.550000000000001, Loss: 2.270512104034424\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: 2.0, Total Reward: 6.550000000000001, Loss: 2.0737123489379883\n",
      "Optimizing model...\n",
      "Step: 5, Action: 0, Reward: 2.0, Total Reward: 8.55, Loss: 5.468404293060303\n",
      "Optimizing model...\n",
      "Step: 6, Action: 1, Reward: 0.0, Total Reward: 8.55, Loss: 3.902926445007324\n",
      "Optimizing model...\n",
      "Step: 7, Action: 4, Reward: 0.2, Total Reward: 8.75, Loss: 8.290160179138184\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 8, Action: 0, Reward: 2.0, Total Reward: 10.75, Loss: 3.2842204570770264\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 9, Action: 0, Reward: -8.0, Total Reward: 2.75, Loss: 3.245940923690796\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 10, Action: 0, Reward: 2.0, Total Reward: 4.75, Loss: 3.5588245391845703\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 11, Action: 0, Reward: 2.0, Total Reward: 6.75, Loss: 2.117202043533325\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 12, Action: 0, Reward: 2.0, Total Reward: 8.75, Loss: 2.481022596359253\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 13, Action: 0, Reward: 2.1, Total Reward: 10.85, Loss: 3.5879290103912354\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 14, Action: 0, Reward: -8.0, Total Reward: 2.8499999999999996, Loss: 3.839906692504883\n",
      "Optimizing model...\n",
      "Step: 15, Action: 3, Reward: 0.25, Total Reward: 3.0999999999999996, Loss: 0.6551258563995361\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 16, Action: 0, Reward: 2.0, Total Reward: 5.1, Loss: 7.638162136077881\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 17, Action: 0, Reward: 2.0, Total Reward: 7.1, Loss: 2.334752082824707\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 18, Action: 0, Reward: 2.0, Total Reward: 9.1, Loss: 1.0079923868179321\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 19, Action: 0, Reward: -7.9, Total Reward: 1.1999999999999993, Loss: 1.0176435708999634\n",
      "Optimizing model...\n",
      "Step: 20, Action: 3, Reward: 0.25, Total Reward: 1.4499999999999993, Loss: 3.8149893283843994\n",
      "Optimizing model...\n",
      "Step: 21, Action: 4, Reward: 0.2, Total Reward: 1.6499999999999992, Loss: 5.399774551391602\n",
      "\n",
      "episode: 58, reward: -98.35\n",
      "\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: -8.0, Total Reward: -8.0, Loss: 2.6112711429595947\n",
      "Optimizing model...\n",
      "Step: 1, Action: 3, Reward: 0.25, Total Reward: -7.75, Loss: 3.4911038875579834\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: 2.0, Total Reward: -5.75, Loss: 0.9583094120025635\n",
      "Optimizing model...\n",
      "Step: 3, Action: 4, Reward: 0.2, Total Reward: -5.55, Loss: 0.6165429353713989\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: 2.0, Total Reward: -3.55, Loss: 2.2918171882629395\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 5, Action: 0, Reward: 2.0, Total Reward: -1.5499999999999998, Loss: 3.8255209922790527\n",
      "Optimizing model...\n",
      "Step: 6, Action: 2, Reward: 0.25, Total Reward: -1.2999999999999998, Loss: 0.3855298161506653\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 7, Action: 0, Reward: 2.0, Total Reward: 0.7000000000000002, Loss: 2.2318601608276367\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 8, Action: 0, Reward: 2.0, Total Reward: 2.7, Loss: 2.3521084785461426\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 9, Action: 0, Reward: -8.0, Total Reward: -5.3, Loss: 4.378157138824463\n",
      "Optimizing model...\n",
      "Step: 10, Action: 4, Reward: 0.2, Total Reward: -5.1, Loss: 5.073895454406738\n",
      "Optimizing model...\n",
      "Step: 11, Action: 4, Reward: 0.2, Total Reward: -4.8999999999999995, Loss: 3.8691067695617676\n",
      "Optimizing model...\n",
      "Step: 12, Action: 3, Reward: 0.25, Total Reward: -4.6499999999999995, Loss: 2.235330104827881\n",
      "Optimizing model...\n",
      "Step: 13, Action: 0, Reward: 2.0, Total Reward: -2.6499999999999995, Loss: 2.2055535316467285\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 14, Action: 0, Reward: 2.0, Total Reward: -0.6499999999999995, Loss: 3.7373149394989014\n",
      "Optimizing model...\n",
      "Step: 15, Action: 4, Reward: 0.2, Total Reward: -0.44999999999999946, Loss: 2.358675956726074\n",
      "Optimizing model...\n",
      "Step: 16, Action: 1, Reward: 0.0, Total Reward: -0.44999999999999946, Loss: 7.274080276489258\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 17, Action: 0, Reward: 2.0, Total Reward: 1.5500000000000005, Loss: 2.5876243114471436\n",
      "Optimizing model...\n",
      "Step: 18, Action: 1, Reward: 0.0, Total Reward: 1.5500000000000005, Loss: 1.9675341844558716\n",
      "Model acting\n",
      "\n",
      "episode: 59, reward: -98.45\n",
      "\n",
      "Optimizing model...\n",
      "Step: 0, Action: 4, Reward: 0.2, Total Reward: 0.2, Loss: 2.0627362728118896\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: 2.0, Total Reward: 2.2, Loss: 0.8447278141975403\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: 2.0, Total Reward: 4.2, Loss: 5.1019673347473145\n",
      "Optimizing model...\n",
      "Step: 3, Action: 2, Reward: 0.25, Total Reward: 4.45, Loss: 0.36766332387924194\n",
      "\n",
      "episode: 60, reward: -95.55\n",
      "\n",
      "Model saved\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: 2.0, Total Reward: 2.0, Loss: 4.225197792053223\n",
      "Optimizing model...\n",
      "Step: 1, Action: 4, Reward: 0.2, Total Reward: 2.2, Loss: 2.4045872688293457\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: 2.0, Total Reward: 4.2, Loss: 1.8796782493591309\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: -7.9, Total Reward: -3.7, Loss: 7.732478141784668\n",
      "Optimizing model...\n",
      "Step: 4, Action: 2, Reward: 0.25, Total Reward: -3.45, Loss: 2.606912136077881\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 5, Action: 0, Reward: 2.0, Total Reward: -1.4500000000000002, Loss: 5.007022380828857\n",
      "Model acting\n",
      "\n",
      "episode: 61, reward: -101.45\n",
      "\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: 2.0, Total Reward: 2.0, Loss: 12.274551391601562\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: -8.0, Total Reward: -6.0, Loss: 9.560724258422852\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: 2.0, Total Reward: -4.0, Loss: 1.7751350402832031\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: 2.0, Total Reward: -2.0, Loss: 4.033770561218262\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: 2.1, Total Reward: 0.10000000000000009, Loss: 3.7205891609191895\n",
      "Optimizing model...\n",
      "Step: 5, Action: 4, Reward: 0.2, Total Reward: 0.3000000000000001, Loss: 5.30248498916626\n",
      "\n",
      "episode: 62, reward: -99.7\n",
      "\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: -8.0, Total Reward: -8.0, Loss: 0.5120968818664551\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: -8.0, Total Reward: -16.0, Loss: 2.61130690574646\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: -7.9, Total Reward: -23.9, Loss: 7.374061584472656\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: -8.0, Total Reward: -31.9, Loss: 2.3895630836486816\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: -8.0, Total Reward: -39.9, Loss: 4.031999588012695\n",
      "Model acting\n",
      "\n",
      "episode: 63, reward: -139.9\n",
      "\n",
      "Optimizing model...\n",
      "Step: 0, Action: 1, Reward: 0.0, Total Reward: 0.0, Loss: 5.14821720123291\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: 2.0, Total Reward: 2.0, Loss: 6.142635822296143\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: 2.0, Total Reward: 4.0, Loss: 3.8862924575805664\n",
      "Optimizing model...\n",
      "Step: 3, Action: 2, Reward: 0.25, Total Reward: 4.25, Loss: 0.774088442325592\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: 2.0, Total Reward: 6.25, Loss: 2.440392017364502\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 5, Action: 0, Reward: -8.0, Total Reward: -1.75, Loss: 3.9513468742370605\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 6, Action: 0, Reward: -8.0, Total Reward: -9.75, Loss: 10.427183151245117\n",
      "Optimizing model...\n",
      "Step: 7, Action: 4, Reward: 0.2, Total Reward: -9.55, Loss: 0.9318640232086182\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 8, Action: 0, Reward: 2.0, Total Reward: -7.550000000000001, Loss: 2.3123648166656494\n",
      "Optimizing model...\n",
      "Step: 9, Action: 2, Reward: 0.25, Total Reward: -7.300000000000001, Loss: 0.8592298030853271\n",
      "Optimizing model...\n",
      "Step: 10, Action: 4, Reward: 0.2, Total Reward: -7.1000000000000005, Loss: 1.9478535652160645\n",
      "Optimizing model...\n",
      "Step: 11, Action: 2, Reward: 0.25, Total Reward: -6.8500000000000005, Loss: 0.9424037933349609\n",
      "\n",
      "episode: 64, reward: -106.85\n",
      "\n",
      "Optimizing model...\n",
      "Step: 0, Action: 3, Reward: 0.25, Total Reward: 0.25, Loss: 4.766383171081543\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: 2.0, Total Reward: 2.25, Loss: 0.5403226613998413\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: 2.0, Total Reward: 4.25, Loss: 0.4664064645767212\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: -8.0, Total Reward: -3.75, Loss: 3.8501110076904297\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: -8.0, Total Reward: -11.75, Loss: 4.09322452545166\n",
      "Optimizing model...\n",
      "Step: 5, Action: 0, Reward: -8.0, Total Reward: -19.75, Loss: 3.5800557136535645\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 6, Action: 0, Reward: -7.9, Total Reward: -27.65, Loss: 0.5970344543457031\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 7, Action: 0, Reward: 2.0, Total Reward: -25.65, Loss: 5.455937385559082\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 8, Action: 0, Reward: 2.1, Total Reward: -23.549999999999997, Loss: 5.296050548553467\n",
      "Optimizing model...\n",
      "Step: 9, Action: 4, Reward: 0.3, Total Reward: -23.249999999999996, Loss: 5.419462203979492\n",
      "Optimizing model...\n",
      "Step: 10, Action: 4, Reward: 0.2, Total Reward: -23.049999999999997, Loss: 0.9680511355400085\n",
      "Model acting\n",
      "\n",
      "episode: 65, reward: -123.05\n",
      "\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: -8.0, Total Reward: -8.0, Loss: 2.1611790657043457\n",
      "Optimizing model...\n",
      "Step: 1, Action: 3, Reward: 0.35, Total Reward: -7.65, Loss: 3.554774761199951\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: 2.0, Total Reward: -5.65, Loss: 8.288996696472168\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: 2.0, Total Reward: -3.6500000000000004, Loss: 2.5052266120910645\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: 2.0, Total Reward: -1.6500000000000004, Loss: 3.666733741760254\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 5, Action: 0, Reward: 2.0, Total Reward: 0.34999999999999964, Loss: 1.997816562652588\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 6, Action: 0, Reward: 2.0, Total Reward: 2.3499999999999996, Loss: 3.856529474258423\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 7, Action: 0, Reward: -8.0, Total Reward: -5.65, Loss: 2.047112464904785\n",
      "Optimizing model...\n",
      "Step: 8, Action: 3, Reward: 0.25, Total Reward: -5.4, Loss: 3.6225037574768066\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 9, Action: 0, Reward: 2.0, Total Reward: -3.4000000000000004, Loss: 1.9792720079421997\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 10, Action: 0, Reward: 2.0, Total Reward: -1.4000000000000004, Loss: 2.2308104038238525\n",
      "Optimizing model...\n",
      "Step: 11, Action: 2, Reward: 0.25, Total Reward: -1.1500000000000004, Loss: 2.278964042663574\n",
      "Optimizing model...\n",
      "Step: 12, Action: 2, Reward: -9.75, Total Reward: -10.9, Loss: 3.2661938667297363\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 13, Action: 0, Reward: 2.0, Total Reward: -8.9, Loss: 2.4373626708984375\n",
      "Optimizing model...\n",
      "Step: 14, Action: 1, Reward: 0.0, Total Reward: -8.9, Loss: 6.8238935470581055\n",
      "Optimizing model...\n",
      "Step: 15, Action: 4, Reward: 0.2, Total Reward: -8.700000000000001, Loss: 1.4030624628067017\n",
      "Optimizing model...\n",
      "Step: 16, Action: 1, Reward: 0.0, Total Reward: -8.700000000000001, Loss: 8.573884010314941\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 17, Action: 0, Reward: 2.0, Total Reward: -6.700000000000001, Loss: 5.2238616943359375\n",
      "Optimizing model...\n",
      "Step: 18, Action: 1, Reward: 0.0, Total Reward: -6.700000000000001, Loss: 4.413882255554199\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 19, Action: 0, Reward: 2.0, Total Reward: -4.700000000000001, Loss: 3.9125723838806152\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 20, Action: 3, Reward: 0.25, Total Reward: -4.450000000000001, Loss: 0.6332657337188721\n",
      "Optimizing model...\n",
      "Step: 21, Action: 2, Reward: 0.25, Total Reward: -4.200000000000001, Loss: 1.8998639583587646\n",
      "Optimizing model...\n",
      "Step: 22, Action: 0, Reward: 2.0, Total Reward: -2.200000000000001, Loss: 4.6130523681640625\n",
      "Optimizing model...\n",
      "Step: 23, Action: 3, Reward: 0.25, Total Reward: -1.950000000000001, Loss: 8.28030014038086\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 24, Action: 3, Reward: -9.75, Total Reward: -11.700000000000001, Loss: 3.813652992248535\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 25, Action: 3, Reward: 0.25, Total Reward: -11.450000000000001, Loss: 1.916982650756836\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 26, Action: 3, Reward: 0.25, Total Reward: -11.200000000000001, Loss: 3.473083019256592\n",
      "Optimizing model...\n",
      "Step: 27, Action: 0, Reward: 2.0, Total Reward: -9.200000000000001, Loss: 3.862636089324951\n",
      "Optimizing model...\n",
      "Step: 28, Action: 3, Reward: 0.25, Total Reward: -8.950000000000001, Loss: 1.9936387538909912\n",
      "Optimizing model...\n",
      "Step: 29, Action: 0, Reward: 2.0, Total Reward: -6.950000000000001, Loss: 3.8426876068115234\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 30, Action: 0, Reward: 2.0, Total Reward: -4.950000000000001, Loss: 3.8775720596313477\n",
      "Model acting\n",
      "\n",
      "episode: 66, reward: -104.95\n",
      "\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: 2.0, Total Reward: 2.0, Loss: 4.125161170959473\n",
      "Optimizing model...\n",
      "Step: 1, Action: 4, Reward: 0.2, Total Reward: 2.2, Loss: 6.455917835235596\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: 2.0, Total Reward: 4.2, Loss: 6.818678855895996\n",
      "Optimizing model...\n",
      "Step: 3, Action: 2, Reward: 0.25, Total Reward: 4.45, Loss: 4.770545482635498\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: 2.0, Total Reward: 6.45, Loss: 5.143083572387695\n",
      "Optimizing model...\n",
      "Step: 5, Action: 2, Reward: 0.25, Total Reward: 6.7, Loss: 5.152173042297363\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 6, Action: 4, Reward: 0.2, Total Reward: 6.9, Loss: 3.8613343238830566\n",
      "Optimizing model...\n",
      "Step: 7, Action: 3, Reward: 0.25, Total Reward: 7.15, Loss: 4.811197757720947\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 8, Action: 4, Reward: 0.4, Total Reward: 7.550000000000001, Loss: 2.231276273727417\n",
      "Optimizing model...\n",
      "Step: 9, Action: 2, Reward: 0.25, Total Reward: 7.800000000000001, Loss: 2.1264467239379883\n",
      "Optimizing model...\n",
      "Step: 10, Action: 3, Reward: 0.25, Total Reward: 8.05, Loss: 5.81180477142334\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 11, Action: 4, Reward: 0.2, Total Reward: 8.25, Loss: 4.435555458068848\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 12, Action: 4, Reward: -9.8, Total Reward: -1.5500000000000007, Loss: 8.055109977722168\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 13, Action: 4, Reward: -9.8, Total Reward: -11.350000000000001, Loss: 7.228483200073242\n",
      "Optimizing model...\n",
      "Step: 14, Action: 4, Reward: -9.8, Total Reward: -21.150000000000002, Loss: 3.8268988132476807\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 15, Action: 4, Reward: -9.8, Total Reward: -30.950000000000003, Loss: 5.046238899230957\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 16, Action: 4, Reward: -9.8, Total Reward: -40.75, Loss: 3.7953596115112305\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 17, Action: 4, Reward: -9.8, Total Reward: -50.55, Loss: 3.96766996383667\n",
      "Optimizing model...\n",
      "Step: 18, Action: 2, Reward: 0.25, Total Reward: -50.3, Loss: 11.272112846374512\n",
      "Optimizing model...\n",
      "Step: 19, Action: 2, Reward: -9.75, Total Reward: -60.05, Loss: 5.284869194030762\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 20, Action: 0, Reward: 2.0, Total Reward: -58.05, Loss: 4.470601558685303\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 21, Action: 4, Reward: 0.2, Total Reward: -57.849999999999994, Loss: 4.007513523101807\n",
      "Optimizing model...\n",
      "Step: 22, Action: 3, Reward: 0.25, Total Reward: -57.599999999999994, Loss: 0.5097720623016357\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 23, Action: 0, Reward: 2.0, Total Reward: -55.599999999999994, Loss: 2.0919125080108643\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 24, Action: 4, Reward: 0.2, Total Reward: -55.39999999999999, Loss: 2.28731369972229\n",
      "\n",
      "episode: 67, reward: -155.39999999999998\n",
      "\n",
      "Model acting\n",
      "\n",
      "episode: 68, reward: -100\n",
      "\n",
      "Optimizing model...\n",
      "Step: 0, Action: 2, Reward: 0.35, Total Reward: 0.35, Loss: 3.934905529022217\n",
      "Optimizing model...\n",
      "Step: 1, Action: 3, Reward: 0.25, Total Reward: 0.6, Loss: 3.36447811126709\n",
      "Optimizing model...\n",
      "Step: 2, Action: 1, Reward: 0.1, Total Reward: 0.7, Loss: 1.125500202178955\n",
      "Optimizing model...\n",
      "Step: 3, Action: 3, Reward: 0.35, Total Reward: 1.0499999999999998, Loss: 4.0054121017456055\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: 2.0, Total Reward: 3.05, Loss: 3.515906810760498\n",
      "Optimizing model...\n",
      "Step: 5, Action: 0, Reward: 2.0, Total Reward: 5.05, Loss: 5.38863468170166\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 6, Action: 0, Reward: -8.0, Total Reward: -2.95, Loss: 2.1364331245422363\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 7, Action: 0, Reward: -8.0, Total Reward: -10.95, Loss: 2.5209617614746094\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 8, Action: 0, Reward: -8.0, Total Reward: -18.95, Loss: 4.130069732666016\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 9, Action: 0, Reward: 2.0, Total Reward: -16.95, Loss: 2.8773646354675293\n",
      "Optimizing model...\n",
      "Step: 10, Action: 0, Reward: -8.0, Total Reward: -24.95, Loss: 4.002921104431152\n",
      "Optimizing model...\n",
      "Step: 11, Action: 3, Reward: 0.25, Total Reward: -24.7, Loss: 0.7505847215652466\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 12, Action: 0, Reward: 2.0, Total Reward: -22.7, Loss: 2.3385767936706543\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 13, Action: 0, Reward: 2.0, Total Reward: -20.7, Loss: 1.9286903142929077\n",
      "Optimizing model...\n",
      "Step: 14, Action: 0, Reward: -8.0, Total Reward: -28.7, Loss: 3.546576976776123\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 15, Action: 0, Reward: 2.0, Total Reward: -26.7, Loss: 4.945075511932373\n",
      "Model acting\n",
      "\n",
      "episode: 69, reward: -126.7\n",
      "\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: -8.0, Total Reward: -8.0, Loss: 3.8144688606262207\n",
      "Optimizing model...\n",
      "Step: 1, Action: 1, Reward: 0.0, Total Reward: -8.0, Loss: 8.24999713897705\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: 2.0, Total Reward: -6.0, Loss: 7.586057662963867\n",
      "Optimizing model...\n",
      "Step: 3, Action: 2, Reward: 0.25, Total Reward: -5.75, Loss: 2.152317523956299\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: 2.1, Total Reward: -3.65, Loss: 3.300424337387085\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 5, Action: 0, Reward: -8.0, Total Reward: -11.65, Loss: 6.763692855834961\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 6, Action: 0, Reward: -8.0, Total Reward: -19.65, Loss: 1.9553825855255127\n",
      "Optimizing model...\n",
      "Step: 7, Action: 0, Reward: -8.0, Total Reward: -27.65, Loss: 5.072936058044434\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 8, Action: 0, Reward: 2.0, Total Reward: -25.65, Loss: 4.444226264953613\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 9, Action: 0, Reward: 2.0, Total Reward: -23.65, Loss: 7.756946563720703\n",
      "Optimizing model...\n",
      "Step: 10, Action: 3, Reward: 0.25, Total Reward: -23.4, Loss: 2.295832872390747\n",
      "Optimizing model...\n",
      "Step: 11, Action: 4, Reward: 0.2, Total Reward: -23.2, Loss: 3.2781777381896973\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 12, Action: 0, Reward: 2.0, Total Reward: -21.2, Loss: 5.263357162475586\n",
      "Optimizing model...\n",
      "Step: 13, Action: 2, Reward: 0.25, Total Reward: -20.95, Loss: 2.472687244415283\n",
      "Model acting\n",
      "\n",
      "episode: 70, reward: -120.95\n",
      "\n",
      "Model saved\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: 2.0, Total Reward: 2.0, Loss: 3.509615898132324\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: 2.0, Total Reward: 4.0, Loss: 4.626976013183594\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: -8.0, Total Reward: -4.0, Loss: 5.355807781219482\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: 2.0, Total Reward: -2.0, Loss: 2.34971022605896\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: -8.0, Total Reward: -10.0, Loss: 3.1964621543884277\n",
      "Optimizing model...\n",
      "Step: 5, Action: 2, Reward: 0.35, Total Reward: -9.65, Loss: 7.159711837768555\n",
      "Optimizing model...\n",
      "Step: 6, Action: 4, Reward: 0.2, Total Reward: -9.450000000000001, Loss: 4.209117412567139\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 7, Action: 0, Reward: 2.0, Total Reward: -7.450000000000001, Loss: 4.438603401184082\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 8, Action: 0, Reward: -8.0, Total Reward: -15.450000000000001, Loss: 2.4340620040893555\n",
      "Optimizing model...\n",
      "Step: 9, Action: 4, Reward: 0.2, Total Reward: -15.250000000000002, Loss: 0.49736690521240234\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 10, Action: 0, Reward: 2.0, Total Reward: -13.250000000000002, Loss: 6.4639892578125\n",
      "Optimizing model...\n",
      "Step: 11, Action: 3, Reward: 0.25, Total Reward: -13.000000000000002, Loss: 7.161661148071289\n",
      "Optimizing model...\n",
      "Step: 12, Action: 1, Reward: 0.0, Total Reward: -13.000000000000002, Loss: 6.94997501373291\n",
      "Model acting\n",
      "\n",
      "episode: 71, reward: -113.0\n",
      "\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: 2.0, Total Reward: 2.0, Loss: 5.376171112060547\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: -8.0, Total Reward: -6.0, Loss: 4.195079803466797\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: -8.0, Total Reward: -14.0, Loss: 2.27801775932312\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: 2.0, Total Reward: -12.0, Loss: 4.304159164428711\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: -8.0, Total Reward: -20.0, Loss: 2.107487440109253\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 5, Action: 0, Reward: -8.0, Total Reward: -28.0, Loss: 8.079205513000488\n",
      "Optimizing model...\n",
      "Step: 6, Action: 1, Reward: 0.0, Total Reward: -28.0, Loss: 3.7215967178344727\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 7, Action: 0, Reward: 2.0, Total Reward: -26.0, Loss: 1.16244375705719\n",
      "Model acting\n",
      "\n",
      "episode: 72, reward: -126.0\n",
      "\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: 2.0, Total Reward: 2.0, Loss: 5.19953727722168\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: 2.0, Total Reward: 4.0, Loss: 4.103940963745117\n",
      "Optimizing model...\n",
      "Step: 2, Action: 4, Reward: 0.2, Total Reward: 4.2, Loss: 5.410560607910156\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: 2.0, Total Reward: 6.2, Loss: 2.2319419384002686\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: 2.0, Total Reward: 8.2, Loss: 5.235936164855957\n",
      "Optimizing model...\n",
      "Step: 5, Action: 2, Reward: 0.25, Total Reward: 8.45, Loss: 6.128867149353027\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 6, Action: 0, Reward: 2.0, Total Reward: 10.45, Loss: 5.621726036071777\n",
      "Optimizing model...\n",
      "Step: 7, Action: 2, Reward: 0.25, Total Reward: 10.7, Loss: 8.728550910949707\n",
      "Optimizing model...\n",
      "Step: 8, Action: 4, Reward: 0.2, Total Reward: 10.899999999999999, Loss: 2.1394765377044678\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 9, Action: 4, Reward: 0.2, Total Reward: 11.099999999999998, Loss: 1.9380288124084473\n",
      "Model acting\n",
      "\n",
      "episode: 73, reward: -88.9\n",
      "\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: -8.0, Total Reward: -8.0, Loss: 2.668900489807129\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: 2.0, Total Reward: -6.0, Loss: 2.5259194374084473\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: 2.0, Total Reward: -4.0, Loss: 2.629120349884033\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: 2.0, Total Reward: -2.0, Loss: 3.0680997371673584\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: -8.0, Total Reward: -10.0, Loss: 2.311129331588745\n",
      "Model acting\n",
      "\n",
      "episode: 74, reward: -110.0\n",
      "\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: 2.0, Total Reward: 2.0, Loss: 2.4577722549438477\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: 2.0, Total Reward: 4.0, Loss: 5.7281317710876465\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: 2.0, Total Reward: 6.0, Loss: 2.2465121746063232\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: 2.0, Total Reward: 8.0, Loss: 0.8189178705215454\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: 2.0, Total Reward: 10.0, Loss: 2.3666810989379883\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 5, Action: 0, Reward: 2.0, Total Reward: 12.0, Loss: 2.382429361343384\n",
      "Optimizing model...\n",
      "Step: 6, Action: 3, Reward: 0.35, Total Reward: 12.35, Loss: 5.834409713745117\n",
      "Optimizing model...\n",
      "Step: 7, Action: 1, Reward: 0.0, Total Reward: 12.35, Loss: 8.575555801391602\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 8, Action: 0, Reward: 2.0, Total Reward: 14.35, Loss: 5.69602632522583\n",
      "Optimizing model...\n",
      "Step: 9, Action: 3, Reward: 0.25, Total Reward: 14.6, Loss: 2.646650791168213\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 10, Action: 0, Reward: 2.0, Total Reward: 16.6, Loss: 6.562143325805664\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 11, Action: 0, Reward: 2.0, Total Reward: 18.6, Loss: 3.9716639518737793\n",
      "Optimizing model...\n",
      "Step: 12, Action: 1, Reward: 0.0, Total Reward: 18.6, Loss: 0.37436437606811523\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 13, Action: 0, Reward: 2.0, Total Reward: 20.6, Loss: 2.500399112701416\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 14, Action: 0, Reward: -8.0, Total Reward: 12.600000000000001, Loss: 5.975584983825684\n",
      "Optimizing model...\n",
      "Step: 15, Action: 0, Reward: -8.0, Total Reward: 4.600000000000001, Loss: 3.663424491882324\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 16, Action: 0, Reward: -8.0, Total Reward: -3.3999999999999986, Loss: 0.6916682720184326\n",
      "Model acting\n",
      "\n",
      "episode: 75, reward: -103.4\n",
      "\n",
      "Optimizing model...\n",
      "Step: 0, Action: 2, Reward: 0.25, Total Reward: 0.25, Loss: 5.599852561950684\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: 2.0, Total Reward: 2.25, Loss: 0.3826182186603546\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: -7.9, Total Reward: -5.65, Loss: 1.302424430847168\n",
      "Optimizing model...\n",
      "Step: 3, Action: 2, Reward: 0.25, Total Reward: -5.4, Loss: 2.44299578666687\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: 2.1, Total Reward: -3.3000000000000003, Loss: 4.2700700759887695\n",
      "Optimizing model...\n",
      "Step: 5, Action: 2, Reward: 0.25, Total Reward: -3.0500000000000003, Loss: 0.8600857853889465\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 6, Action: 0, Reward: 2.0, Total Reward: -1.0500000000000003, Loss: 3.9812254905700684\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 7, Action: 0, Reward: -8.0, Total Reward: -9.05, Loss: 3.5931954383850098\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 8, Action: 0, Reward: 2.0, Total Reward: -7.050000000000001, Loss: 6.224876403808594\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 9, Action: 0, Reward: 2.0, Total Reward: -5.050000000000001, Loss: 0.6496878862380981\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 10, Action: 0, Reward: 2.0, Total Reward: -3.0500000000000007, Loss: 5.571475028991699\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 11, Action: 0, Reward: 2.1, Total Reward: -0.9500000000000006, Loss: 2.4118175506591797\n",
      "Optimizing model...\n",
      "Step: 12, Action: 0, Reward: -8.0, Total Reward: -8.950000000000001, Loss: 5.995789527893066\n",
      "Optimizing model...\n",
      "Step: 13, Action: 3, Reward: 0.25, Total Reward: -8.700000000000001, Loss: 3.9170289039611816\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 14, Action: 0, Reward: 2.0, Total Reward: -6.700000000000001, Loss: 3.7293787002563477\n",
      "Optimizing model...\n",
      "Step: 15, Action: 1, Reward: 0.0, Total Reward: -6.700000000000001, Loss: 3.7791481018066406\n",
      "Optimizing model...\n",
      "Step: 16, Action: 3, Reward: 0.25, Total Reward: -6.450000000000001, Loss: 0.7718660831451416\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 17, Action: 0, Reward: 2.0, Total Reward: -4.450000000000001, Loss: 2.880666732788086\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 18, Action: 0, Reward: 2.0, Total Reward: -2.450000000000001, Loss: 4.025155067443848\n",
      "Optimizing model...\n",
      "Step: 19, Action: 3, Reward: 0.25, Total Reward: -2.200000000000001, Loss: 1.473179578781128\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 20, Action: 0, Reward: 2.0, Total Reward: -0.20000000000000107, Loss: 3.9953958988189697\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 21, Action: 0, Reward: 2.0, Total Reward: 1.799999999999999, Loss: 2.3520054817199707\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 22, Action: 0, Reward: 2.0, Total Reward: 3.799999999999999, Loss: 4.6614251136779785\n",
      "Model acting\n",
      "\n",
      "episode: 76, reward: -96.2\n",
      "\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: -8.0, Total Reward: -8.0, Loss: 3.872837543487549\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: 2.0, Total Reward: -6.0, Loss: 3.742602825164795\n",
      "Optimizing model...\n",
      "Step: 2, Action: 2, Reward: 0.25, Total Reward: -5.75, Loss: 4.053144454956055\n",
      "Optimizing model...\n",
      "Step: 3, Action: 4, Reward: 0.2, Total Reward: -5.55, Loss: 1.8678853511810303\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: 2.0, Total Reward: -3.55, Loss: 3.941221237182617\n",
      "Model acting\n",
      "\n",
      "episode: 77, reward: -103.55\n",
      "\n",
      "Optimizing model...\n",
      "Step: 0, Action: 1, Reward: 0.0, Total Reward: 0.0, Loss: 1.5302319526672363\n",
      "Optimizing model...\n",
      "Step: 1, Action: 4, Reward: 0.2, Total Reward: 0.2, Loss: 2.1940016746520996\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: 2.0, Total Reward: 2.2, Loss: 4.043938159942627\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: -8.0, Total Reward: -5.8, Loss: 3.9099268913269043\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: 2.0, Total Reward: -3.8, Loss: 4.0847625732421875\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 5, Action: 0, Reward: -8.0, Total Reward: -11.8, Loss: 3.4581961631774902\n",
      "Optimizing model...\n",
      "Step: 6, Action: 4, Reward: 0.3, Total Reward: -11.5, Loss: 5.782557487487793\n",
      "Optimizing model...\n",
      "Step: 7, Action: 4, Reward: 0.2, Total Reward: -11.3, Loss: 2.5242135524749756\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 8, Action: 0, Reward: 2.0, Total Reward: -9.3, Loss: 3.6066112518310547\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 9, Action: 0, Reward: -8.0, Total Reward: -17.3, Loss: 5.169810771942139\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 10, Action: 0, Reward: 2.0, Total Reward: -15.3, Loss: 11.335858345031738\n",
      "Model acting\n",
      "\n",
      "episode: 78, reward: -115.3\n",
      "\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: 2.0, Total Reward: 2.0, Loss: 2.314397096633911\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: -8.0, Total Reward: -6.0, Loss: 4.179586410522461\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: -8.0, Total Reward: -14.0, Loss: 5.158266544342041\n",
      "Optimizing model...\n",
      "Step: 3, Action: 1, Reward: 0.0, Total Reward: -14.0, Loss: 5.758121490478516\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: 2.0, Total Reward: -12.0, Loss: 4.336932182312012\n",
      "Optimizing model...\n",
      "Step: 5, Action: 2, Reward: 0.25, Total Reward: -11.75, Loss: 7.038876533508301\n",
      "Optimizing model...\n",
      "Step: 6, Action: 2, Reward: 0.25, Total Reward: -11.5, Loss: 3.7009644508361816\n",
      "\n",
      "episode: 79, reward: -111.5\n",
      "\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 3, Reward: 0.25, Total Reward: 0.25, Loss: 3.4247961044311523\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 1, Action: 3, Reward: -9.75, Total Reward: -9.5, Loss: 2.0619373321533203\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 3, Reward: -9.75, Total Reward: -19.25, Loss: 3.358332872390747\n",
      "Optimizing model...\n",
      "Step: 3, Action: 1, Reward: 0.0, Total Reward: -19.25, Loss: 3.4867186546325684\n",
      "Optimizing model...\n",
      "Step: 4, Action: 2, Reward: 0.25, Total Reward: -19.0, Loss: 3.9527461528778076\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 5, Action: 3, Reward: 0.25, Total Reward: -18.75, Loss: 5.4813737869262695\n",
      "Optimizing model...\n",
      "Step: 6, Action: 0, Reward: 2.0, Total Reward: -16.75, Loss: 4.125613689422607\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 7, Action: 0, Reward: 2.0, Total Reward: -14.75, Loss: 4.19014835357666\n",
      "Optimizing model...\n",
      "Step: 8, Action: 3, Reward: 0.25, Total Reward: -14.5, Loss: 3.896509885787964\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 9, Action: 0, Reward: 2.0, Total Reward: -12.5, Loss: 1.0524284839630127\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 10, Action: 0, Reward: 2.0, Total Reward: -10.5, Loss: 4.809257507324219\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 11, Action: 0, Reward: 2.0, Total Reward: -8.5, Loss: 6.151388645172119\n",
      "Optimizing model...\n",
      "Step: 12, Action: 4, Reward: 0.2, Total Reward: -8.3, Loss: 5.552524566650391\n",
      "Optimizing model...\n",
      "Step: 13, Action: 0, Reward: 2.0, Total Reward: -6.300000000000001, Loss: 5.542908191680908\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 14, Action: 0, Reward: 2.0, Total Reward: -4.300000000000001, Loss: 3.650486469268799\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 15, Action: 4, Reward: 0.3, Total Reward: -4.000000000000001, Loss: 0.613020658493042\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 16, Action: 0, Reward: 2.0, Total Reward: -2.000000000000001, Loss: 7.353116989135742\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 17, Action: 4, Reward: 0.2, Total Reward: -1.800000000000001, Loss: 5.7089104652404785\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 18, Action: 0, Reward: 2.0, Total Reward: 0.19999999999999907, Loss: 2.590369701385498\n",
      "Optimizing model...\n",
      "Step: 19, Action: 2, Reward: 0.25, Total Reward: 0.44999999999999907, Loss: 8.658000946044922\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 20, Action: 4, Reward: 0.2, Total Reward: 0.649999999999999, Loss: 5.1736226081848145\n",
      "Model acting\n",
      "\n",
      "episode: 80, reward: -99.35\n",
      "\n",
      "Model saved\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 4, Reward: 0.2, Total Reward: 0.2, Loss: 3.8038747310638428\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: 2.0, Total Reward: 2.2, Loss: 1.990478754043579\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 4, Reward: 0.2, Total Reward: 2.4000000000000004, Loss: 2.9419097900390625\n",
      "Optimizing model...\n",
      "Step: 3, Action: 3, Reward: 0.35, Total Reward: 2.7500000000000004, Loss: 2.4777872562408447\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: 2.0, Total Reward: 4.75, Loss: 3.2323689460754395\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 5, Action: 0, Reward: 2.0, Total Reward: 6.75, Loss: 6.8874359130859375\n",
      "Optimizing model...\n",
      "Step: 6, Action: 2, Reward: 0.25, Total Reward: 7.0, Loss: 4.219140529632568\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 7, Action: 0, Reward: 2.0, Total Reward: 9.0, Loss: 3.6383843421936035\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 8, Action: 2, Reward: 0.25, Total Reward: 9.25, Loss: 3.688711166381836\n",
      "Model acting\n",
      "\n",
      "episode: 81, reward: -90.75\n",
      "\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: -8.0, Total Reward: -8.0, Loss: 2.65999698638916\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: 2.0, Total Reward: -6.0, Loss: 2.268789291381836\n",
      "Optimizing model...\n",
      "Step: 2, Action: 4, Reward: 0.2, Total Reward: -5.8, Loss: 2.6730945110321045\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: 2.0, Total Reward: -3.8, Loss: 2.2991158962249756\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: 2.1, Total Reward: -1.6999999999999997, Loss: 7.457150459289551\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 5, Action: 4, Reward: 0.2, Total Reward: -1.4999999999999998, Loss: 0.9634719491004944\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 6, Action: 0, Reward: 2.0, Total Reward: 0.5000000000000002, Loss: 7.290200233459473\n",
      "Model acting\n",
      "\n",
      "episode: 82, reward: -99.5\n",
      "\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: 2.0, Total Reward: 2.0, Loss: 3.6019320487976074\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: 2.0, Total Reward: 4.0, Loss: 2.2481632232666016\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: 2.0, Total Reward: 6.0, Loss: 2.406705856323242\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: 2.0, Total Reward: 8.0, Loss: 5.789735794067383\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: 2.0, Total Reward: 10.0, Loss: 4.104717254638672\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 5, Action: 0, Reward: -8.0, Total Reward: 2.0, Loss: 5.649727821350098\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 6, Action: 0, Reward: 2.0, Total Reward: 4.0, Loss: 2.2648215293884277\n",
      "Optimizing model...\n",
      "Step: 7, Action: 4, Reward: 0.2, Total Reward: 4.2, Loss: 3.4457039833068848\n",
      "\n",
      "episode: 83, reward: -95.8\n",
      "\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: 2.0, Total Reward: 2.0, Loss: 3.7306699752807617\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: 2.0, Total Reward: 4.0, Loss: 4.115389347076416\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: 2.0, Total Reward: 6.0, Loss: 5.7982072830200195\n",
      "Optimizing model...\n",
      "Step: 3, Action: 4, Reward: 0.2, Total Reward: 6.2, Loss: 5.776137351989746\n",
      "Optimizing model...\n",
      "Step: 4, Action: 4, Reward: -9.6, Total Reward: -3.3999999999999995, Loss: 4.852643966674805\n",
      "Optimizing model...\n",
      "Step: 5, Action: 0, Reward: 2.0, Total Reward: -1.3999999999999995, Loss: 5.591204643249512\n",
      "Optimizing model...\n",
      "Step: 6, Action: 1, Reward: 0.0, Total Reward: -1.3999999999999995, Loss: 4.615729808807373\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 7, Action: 0, Reward: 2.0, Total Reward: 0.6000000000000005, Loss: 2.2872469425201416\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 8, Action: 0, Reward: -8.0, Total Reward: -7.3999999999999995, Loss: 2.4937925338745117\n",
      "Optimizing model...\n",
      "Step: 9, Action: 0, Reward: -8.0, Total Reward: -15.399999999999999, Loss: 2.4646997451782227\n",
      "Model acting\n",
      "\n",
      "episode: 84, reward: -115.4\n",
      "\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: 2.0, Total Reward: 2.0, Loss: 6.148881435394287\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: -8.0, Total Reward: -6.0, Loss: 2.1625096797943115\n",
      "Optimizing model...\n",
      "Step: 2, Action: 3, Reward: 0.25, Total Reward: -5.75, Loss: 0.765303909778595\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: 2.1, Total Reward: -3.65, Loss: 3.3719704151153564\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: 2.0, Total Reward: -1.65, Loss: 5.323516845703125\n",
      "Optimizing model...\n",
      "Step: 5, Action: 3, Reward: 0.25, Total Reward: -1.4, Loss: 2.310061454772949\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 6, Action: 0, Reward: 2.0, Total Reward: 0.6000000000000001, Loss: 8.332463264465332\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 7, Action: 0, Reward: -8.0, Total Reward: -7.4, Loss: 7.0193939208984375\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 8, Action: 0, Reward: 2.0, Total Reward: -5.4, Loss: 4.993854999542236\n",
      "Optimizing model...\n",
      "Step: 9, Action: 1, Reward: 0.1, Total Reward: -5.300000000000001, Loss: 8.899626731872559\n",
      "Optimizing model...\n",
      "Step: 10, Action: 1, Reward: 0.0, Total Reward: -5.300000000000001, Loss: 7.542089462280273\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 11, Action: 0, Reward: 2.0, Total Reward: -3.3000000000000007, Loss: 2.2795050144195557\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 12, Action: 0, Reward: 2.0, Total Reward: -1.3000000000000007, Loss: 3.5171608924865723\n",
      "Optimizing model...\n",
      "Step: 13, Action: 3, Reward: 0.35, Total Reward: -0.9500000000000007, Loss: 2.3654141426086426\n",
      "Optimizing model...\n",
      "Step: 14, Action: 1, Reward: 0.0, Total Reward: -0.9500000000000007, Loss: 3.962189197540283\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 15, Action: 0, Reward: 2.0, Total Reward: 1.0499999999999994, Loss: 5.897060394287109\n",
      "Optimizing model...\n",
      "Step: 16, Action: 0, Reward: -8.0, Total Reward: -6.950000000000001, Loss: 2.4749209880828857\n",
      "Model acting\n",
      "\n",
      "episode: 85, reward: -106.95\n",
      "\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: 2.0, Total Reward: 2.0, Loss: 1.9254660606384277\n",
      "Optimizing model...\n",
      "Step: 1, Action: 4, Reward: 0.2, Total Reward: 2.2, Loss: 2.3139572143554688\n",
      "Optimizing model...\n",
      "Step: 2, Action: 4, Reward: 0.2, Total Reward: 2.4000000000000004, Loss: 3.6548562049865723\n",
      "Optimizing model...\n",
      "Step: 3, Action: 4, Reward: -9.8, Total Reward: -7.4, Loss: 3.945399045944214\n",
      "Optimizing model...\n",
      "Step: 4, Action: 2, Reward: 0.25, Total Reward: -7.15, Loss: 3.381652355194092\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 5, Action: 0, Reward: 2.0, Total Reward: -5.15, Loss: 2.470296859741211\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 6, Action: 0, Reward: 2.0, Total Reward: -3.1500000000000004, Loss: 1.4096845388412476\n",
      "Optimizing model...\n",
      "Step: 7, Action: 2, Reward: 0.25, Total Reward: -2.9000000000000004, Loss: 3.8825576305389404\n",
      "Optimizing model...\n",
      "Step: 8, Action: 3, Reward: 0.25, Total Reward: -2.6500000000000004, Loss: 3.0591254234313965\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 9, Action: 0, Reward: 2.0, Total Reward: -0.6500000000000004, Loss: 2.5943214893341064\n",
      "Model acting\n",
      "\n",
      "episode: 86, reward: -100.65\n",
      "\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: -8.0, Total Reward: -8.0, Loss: 2.5926101207733154\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: 2.0, Total Reward: -6.0, Loss: 7.010611057281494\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: 2.0, Total Reward: -4.0, Loss: 5.32636833190918\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: 2.0, Total Reward: -2.0, Loss: 2.6546950340270996\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: 2.0, Total Reward: 0.0, Loss: 5.309687614440918\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 5, Action: 0, Reward: -8.0, Total Reward: -8.0, Loss: 7.308651447296143\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 6, Action: 0, Reward: 2.0, Total Reward: -6.0, Loss: 2.0405731201171875\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 7, Action: 0, Reward: 2.0, Total Reward: -4.0, Loss: 1.141244649887085\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 8, Action: 0, Reward: 2.0, Total Reward: -2.0, Loss: 4.6425371170043945\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 9, Action: 0, Reward: -8.0, Total Reward: -10.0, Loss: 0.8647281527519226\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 10, Action: 0, Reward: -8.0, Total Reward: -18.0, Loss: 3.5873334407806396\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 11, Action: 0, Reward: 2.0, Total Reward: -16.0, Loss: 3.6574902534484863\n",
      "Model acting\n",
      "\n",
      "episode: 87, reward: -116.0\n",
      "\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: 2.0, Total Reward: 2.0, Loss: 3.5981879234313965\n",
      "Optimizing model...\n",
      "Step: 1, Action: 3, Reward: 0.25, Total Reward: 2.25, Loss: 4.524097919464111\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: 2.0, Total Reward: 4.25, Loss: 2.1780009269714355\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: -8.0, Total Reward: -3.75, Loss: 4.987009048461914\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 4, Action: 3, Reward: 0.25, Total Reward: -3.5, Loss: 0.7221922278404236\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 5, Action: 3, Reward: -9.75, Total Reward: -13.25, Loss: 7.45905876159668\n",
      "Model acting\n",
      "\n",
      "episode: 88, reward: -113.25\n",
      "\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 3, Reward: 0.25, Total Reward: 0.25, Loss: 2.005770206451416\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: 2.0, Total Reward: 2.25, Loss: 4.67495059967041\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 3, Reward: 0.25, Total Reward: 2.5, Loss: 4.92385196685791\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: 2.0, Total Reward: 4.5, Loss: 4.327415466308594\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: -7.9, Total Reward: -3.4000000000000004, Loss: 9.065585136413574\n",
      "Optimizing model...\n",
      "Step: 5, Action: 3, Reward: 0.25, Total Reward: -3.1500000000000004, Loss: 5.416869163513184\n",
      "\n",
      "episode: 89, reward: -103.15\n",
      "\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: -8.0, Total Reward: -8.0, Loss: 4.449699401855469\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 1, Action: 3, Reward: 0.25, Total Reward: -7.75, Loss: 3.7307257652282715\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: 2.0, Total Reward: -5.75, Loss: 2.8831398487091064\n",
      "Optimizing model...\n",
      "Step: 3, Action: 3, Reward: 0.25, Total Reward: -5.5, Loss: 2.408491611480713\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: 2.0, Total Reward: -3.5, Loss: 0.6621793508529663\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 5, Action: 3, Reward: 0.25, Total Reward: -3.25, Loss: 3.775236129760742\n",
      "Optimizing model...\n",
      "Step: 6, Action: 1, Reward: 0.0, Total Reward: -3.25, Loss: 3.475959062576294\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 7, Action: 3, Reward: 0.25, Total Reward: -3.0, Loss: 6.122087478637695\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 8, Action: 3, Reward: -9.75, Total Reward: -12.75, Loss: 3.749763011932373\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 9, Action: 3, Reward: 0.25, Total Reward: -12.5, Loss: 1.4214593172073364\n",
      "Model acting\n",
      "\n",
      "episode: 90, reward: -112.5\n",
      "\n",
      "Model saved\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: 2.1, Total Reward: 2.1, Loss: 6.341566562652588\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: 2.1, Total Reward: 4.2, Loss: 7.3125200271606445\n",
      "Optimizing model...\n",
      "Step: 2, Action: 3, Reward: 0.25, Total Reward: 4.45, Loss: 5.782205581665039\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: 2.0, Total Reward: 6.45, Loss: 4.184125900268555\n",
      "Optimizing model...\n",
      "Step: 4, Action: 4, Reward: 0.2, Total Reward: 6.65, Loss: 6.816817283630371\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 5, Action: 3, Reward: 0.25, Total Reward: 6.9, Loss: 6.732501983642578\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 6, Action: 3, Reward: 0.25, Total Reward: 7.15, Loss: 5.345821380615234\n",
      "Optimizing model...\n",
      "Step: 7, Action: 2, Reward: 0.35, Total Reward: 7.5, Loss: 2.68912410736084\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 8, Action: 4, Reward: 0.2, Total Reward: 7.7, Loss: 0.9639970064163208\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 9, Action: 4, Reward: -9.8, Total Reward: -2.1000000000000005, Loss: 5.532544136047363\n",
      "\n",
      "episode: 91, reward: -102.1\n",
      "\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 4, Reward: 0.2, Total Reward: 0.2, Loss: 4.144100666046143\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 1, Action: 4, Reward: 0.2, Total Reward: 0.4, Loss: 5.605459213256836\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 4, Reward: -9.8, Total Reward: -9.4, Loss: 2.566744804382324\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 4, Reward: 0.2, Total Reward: -9.200000000000001, Loss: 5.7928361892700195\n",
      "Optimizing model...\n",
      "Step: 4, Action: 2, Reward: 0.25, Total Reward: -8.950000000000001, Loss: 2.908766269683838\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 5, Action: 4, Reward: 0.2, Total Reward: -8.750000000000002, Loss: 3.960745334625244\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 6, Action: 4, Reward: -9.8, Total Reward: -18.550000000000004, Loss: 5.966443061828613\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 7, Action: 4, Reward: -9.8, Total Reward: -28.350000000000005, Loss: 6.938601493835449\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 8, Action: 4, Reward: -9.8, Total Reward: -38.150000000000006, Loss: 2.520297050476074\n",
      "Optimizing model...\n",
      "Step: 9, Action: 0, Reward: 2.1, Total Reward: -36.050000000000004, Loss: 2.670999526977539\n",
      "Optimizing model...\n",
      "Step: 10, Action: 1, Reward: 0.1, Total Reward: -35.95, Loss: 2.4063048362731934\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 11, Action: 4, Reward: 0.2, Total Reward: -35.75, Loss: 7.335019111633301\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 12, Action: 2, Reward: 0.25, Total Reward: -35.5, Loss: 5.637331008911133\n",
      "Optimizing model...\n",
      "Step: 13, Action: 4, Reward: 0.2, Total Reward: -35.3, Loss: 3.9188647270202637\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 14, Action: 2, Reward: 0.25, Total Reward: -35.05, Loss: 3.9639618396759033\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 15, Action: 0, Reward: 2.0, Total Reward: -33.05, Loss: 4.313560962677002\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 16, Action: 0, Reward: 2.0, Total Reward: -31.049999999999997, Loss: 2.091773509979248\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 17, Action: 0, Reward: 2.0, Total Reward: -29.049999999999997, Loss: 2.3072566986083984\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 18, Action: 0, Reward: 2.0, Total Reward: -27.049999999999997, Loss: 2.7512216567993164\n",
      "Model acting\n",
      "\n",
      "episode: 92, reward: -127.05\n",
      "\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: 2.0, Total Reward: 2.0, Loss: 4.892801284790039\n",
      "Optimizing model...\n",
      "Step: 1, Action: 3, Reward: 0.25, Total Reward: 2.25, Loss: 4.0319719314575195\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: 2.0, Total Reward: 4.25, Loss: 5.104135513305664\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: 2.0, Total Reward: 6.25, Loss: 0.9538936614990234\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: 2.0, Total Reward: 8.25, Loss: 3.6098473072052\n",
      "\n",
      "episode: 93, reward: -91.75\n",
      "\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: -8.0, Total Reward: -8.0, Loss: 7.768868446350098\n",
      "Optimizing model...\n",
      "Step: 1, Action: 3, Reward: 0.25, Total Reward: -7.75, Loss: 4.092155456542969\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: 2.1, Total Reward: -5.65, Loss: 6.605714321136475\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: 2.0, Total Reward: -3.6500000000000004, Loss: 7.5939764976501465\n",
      "Optimizing model...\n",
      "Step: 4, Action: 1, Reward: 0.0, Total Reward: -3.6500000000000004, Loss: 4.297040939331055\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 5, Action: 0, Reward: 2.0, Total Reward: -1.6500000000000004, Loss: 1.106518268585205\n",
      "\n",
      "episode: 94, reward: -101.65\n",
      "\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: -8.0, Total Reward: -8.0, Loss: 3.6442954540252686\n",
      "Optimizing model...\n",
      "Step: 1, Action: 4, Reward: 0.2, Total Reward: -7.8, Loss: 2.3585071563720703\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: 2.0, Total Reward: -5.8, Loss: 2.4453306198120117\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: -8.0, Total Reward: -13.8, Loss: 5.074277877807617\n",
      "Optimizing model...\n",
      "Step: 4, Action: 4, Reward: 0.2, Total Reward: -13.600000000000001, Loss: 4.0611090660095215\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 5, Action: 0, Reward: 2.0, Total Reward: -11.600000000000001, Loss: 2.3335399627685547\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 6, Action: 0, Reward: 2.0, Total Reward: -9.600000000000001, Loss: 7.108401298522949\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 7, Action: 0, Reward: -8.0, Total Reward: -17.6, Loss: 2.566345691680908\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 8, Action: 0, Reward: 2.0, Total Reward: -15.600000000000001, Loss: 4.741445541381836\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 9, Action: 0, Reward: 2.0, Total Reward: -13.600000000000001, Loss: 2.648177146911621\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 10, Action: 0, Reward: 2.0, Total Reward: -11.600000000000001, Loss: 4.681256294250488\n",
      "\n",
      "episode: 95, reward: -111.6\n",
      "\n",
      "Optimizing model...\n",
      "Step: 0, Action: 2, Reward: 0.25, Total Reward: 0.25, Loss: 3.7156176567077637\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: 2.0, Total Reward: 2.25, Loss: 4.824984550476074\n",
      "Optimizing model...\n",
      "Step: 2, Action: 2, Reward: 0.25, Total Reward: 2.5, Loss: 3.4542298316955566\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: 2.0, Total Reward: 4.5, Loss: 3.793412208557129\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: 2.0, Total Reward: 6.5, Loss: 2.597547769546509\n",
      "Optimizing model...\n",
      "Step: 5, Action: 0, Reward: 2.0, Total Reward: 8.5, Loss: 2.6641957759857178\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 6, Action: 0, Reward: 2.0, Total Reward: 10.5, Loss: 6.422464370727539\n",
      "Optimizing model...\n",
      "Step: 7, Action: 3, Reward: 0.25, Total Reward: 10.75, Loss: 8.456418991088867\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 8, Action: 0, Reward: 2.0, Total Reward: 12.75, Loss: 1.0554239749908447\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 9, Action: 0, Reward: 2.0, Total Reward: 14.75, Loss: 6.061321258544922\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 10, Action: 0, Reward: -8.0, Total Reward: 6.75, Loss: 4.744768142700195\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 11, Action: 0, Reward: 2.0, Total Reward: 8.75, Loss: 5.974736213684082\n",
      "Optimizing model...\n",
      "Step: 12, Action: 1, Reward: 0.0, Total Reward: 8.75, Loss: 5.09832763671875\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 13, Action: 0, Reward: 2.0, Total Reward: 10.75, Loss: 7.20462703704834\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 14, Action: 0, Reward: 2.0, Total Reward: 12.75, Loss: 3.7248711585998535\n",
      "Optimizing model...\n",
      "Step: 15, Action: 3, Reward: 0.25, Total Reward: 13.0, Loss: 0.9354205131530762\n",
      "Model acting\n",
      "\n",
      "episode: 96, reward: -87.0\n",
      "\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: -7.9, Total Reward: -7.9, Loss: 4.44671630859375\n",
      "Optimizing model...\n",
      "Step: 1, Action: 1, Reward: 0.0, Total Reward: -7.9, Loss: 2.654036521911621\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: 2.0, Total Reward: -5.9, Loss: 0.9608734846115112\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: 2.0, Total Reward: -3.9000000000000004, Loss: 5.812151908874512\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: 2.0, Total Reward: -1.9000000000000004, Loss: 6.487834930419922\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 5, Action: 0, Reward: 2.0, Total Reward: 0.09999999999999964, Loss: 3.737473726272583\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 6, Action: 0, Reward: -8.0, Total Reward: -7.9, Loss: 2.4727768898010254\n",
      "Model acting\n",
      "\n",
      "episode: 97, reward: -107.9\n",
      "\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: -7.9, Total Reward: -7.9, Loss: 4.533127784729004\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: -8.0, Total Reward: -15.9, Loss: 4.13193416595459\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: 2.0, Total Reward: -13.9, Loss: 2.143479108810425\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: -8.0, Total Reward: -21.9, Loss: 4.689202308654785\n",
      "Model acting\n",
      "\n",
      "episode: 98, reward: -121.9\n",
      "\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: -8.0, Total Reward: -8.0, Loss: 2.069553852081299\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: 2.0, Total Reward: -6.0, Loss: 4.109499931335449\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: 2.0, Total Reward: -4.0, Loss: 0.9669814705848694\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: 2.0, Total Reward: -2.0, Loss: 2.5423264503479004\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: -8.0, Total Reward: -10.0, Loss: 0.970000684261322\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 5, Action: 0, Reward: 2.0, Total Reward: -8.0, Loss: 2.548421859741211\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 6, Action: 0, Reward: 2.0, Total Reward: -6.0, Loss: 5.049121856689453\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 7, Action: 0, Reward: 2.0, Total Reward: -4.0, Loss: 4.085931777954102\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 8, Action: 0, Reward: 2.0, Total Reward: -2.0, Loss: 5.2987775802612305\n",
      "Model acting\n",
      "\n",
      "episode: 99, reward: -102.0\n",
      "\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: -8.0, Total Reward: -8.0, Loss: 3.776728868484497\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: 2.0, Total Reward: -6.0, Loss: 2.4172043800354004\n",
      "Optimizing model...\n",
      "Step: 2, Action: 3, Reward: 0.25, Total Reward: -5.75, Loss: 6.850473880767822\n",
      "Optimizing model...\n",
      "Step: 3, Action: 3, Reward: 0.25, Total Reward: -5.5, Loss: 7.176220893859863\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: 2.0, Total Reward: -3.5, Loss: 0.763879656791687\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 5, Action: 0, Reward: -8.0, Total Reward: -11.5, Loss: 4.120095729827881\n",
      "Optimizing model...\n",
      "Step: 6, Action: 3, Reward: 0.25, Total Reward: -11.25, Loss: 7.926916122436523\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 7, Action: 0, Reward: 2.0, Total Reward: -9.25, Loss: 2.474534511566162\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 8, Action: 0, Reward: 2.0, Total Reward: -7.25, Loss: 4.558643341064453\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 9, Action: 0, Reward: 2.0, Total Reward: -5.25, Loss: 7.318183898925781\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 10, Action: 0, Reward: -8.0, Total Reward: -13.25, Loss: 2.6288301944732666\n",
      "Optimizing model...\n",
      "Step: 11, Action: 4, Reward: 0.2, Total Reward: -13.05, Loss: 4.326051235198975\n",
      "Optimizing model...\n",
      "Step: 12, Action: 0, Reward: 2.0, Total Reward: -11.05, Loss: 7.146816253662109\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 13, Action: 0, Reward: -8.0, Total Reward: -19.05, Loss: 3.055453062057495\n",
      "Optimizing model...\n",
      "Step: 14, Action: 0, Reward: -8.0, Total Reward: -27.05, Loss: 3.7839765548706055\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 15, Action: 0, Reward: 2.0, Total Reward: -25.05, Loss: 2.095208168029785\n",
      "Optimizing model...\n",
      "Step: 16, Action: 4, Reward: 0.2, Total Reward: -24.85, Loss: 6.694628715515137\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 17, Action: 0, Reward: 2.0, Total Reward: -22.85, Loss: 5.563671588897705\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 18, Action: 0, Reward: 2.0, Total Reward: -20.85, Loss: 7.272220611572266\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 19, Action: 0, Reward: 2.0, Total Reward: -18.85, Loss: 2.9602537155151367\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 20, Action: 0, Reward: 2.0, Total Reward: -16.85, Loss: 0.5899297595024109\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 21, Action: 0, Reward: 2.0, Total Reward: -14.850000000000001, Loss: 5.132302284240723\n",
      "Model acting\n",
      "\n",
      "episode: 100, reward: -114.85\n",
      "\n",
      "Model saved\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: 2.0, Total Reward: 2.0, Loss: 0.7372491955757141\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: 2.0, Total Reward: 4.0, Loss: 3.166234016418457\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: -8.0, Total Reward: -4.0, Loss: 6.122424125671387\n",
      "Optimizing model...\n",
      "Step: 3, Action: 2, Reward: 0.35, Total Reward: -3.65, Loss: 2.5435309410095215\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: 2.0, Total Reward: -1.65, Loss: 9.063634872436523\n",
      "Optimizing model...\n",
      "Step: 5, Action: 2, Reward: 0.25, Total Reward: -1.4, Loss: 4.329747676849365\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 6, Action: 0, Reward: 2.0, Total Reward: 0.6000000000000001, Loss: 2.5453312397003174\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 7, Action: 0, Reward: 2.0, Total Reward: 2.6, Loss: 2.6452035903930664\n",
      "Model acting\n",
      "\n",
      "episode: 101, reward: -97.4\n",
      "\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: 2.0, Total Reward: 2.0, Loss: 3.6980502605438232\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: -8.0, Total Reward: -6.0, Loss: 3.83627986907959\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: 2.0, Total Reward: -4.0, Loss: 5.377098083496094\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: -8.0, Total Reward: -12.0, Loss: 3.251310348510742\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: 2.0, Total Reward: -10.0, Loss: 5.381831169128418\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 5, Action: 0, Reward: -8.0, Total Reward: -18.0, Loss: 2.6366517543792725\n",
      "Optimizing model...\n",
      "Step: 6, Action: 2, Reward: 0.25, Total Reward: -17.75, Loss: 7.051126003265381\n",
      "Model acting\n",
      "\n",
      "episode: 102, reward: -117.75\n",
      "\n",
      "Optimizing model...\n",
      "Step: 0, Action: 4, Reward: 0.2, Total Reward: 0.2, Loss: 1.8469598293304443\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 1, Action: 3, Reward: 0.35, Total Reward: 0.55, Loss: 2.7467265129089355\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 3, Reward: 0.25, Total Reward: 0.8, Loss: 7.45355749130249\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: 2.0, Total Reward: 2.8, Loss: 3.8490612506866455\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 4, Action: 3, Reward: 0.25, Total Reward: 3.05, Loss: 2.2608678340911865\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 5, Action: 3, Reward: 0.25, Total Reward: 3.3, Loss: 2.3459677696228027\n",
      "Optimizing model...\n",
      "Step: 6, Action: 0, Reward: 2.0, Total Reward: 5.3, Loss: 6.20763635635376\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 7, Action: 3, Reward: 0.25, Total Reward: 5.55, Loss: 1.115645408630371\n",
      "Optimizing model...\n",
      "Step: 8, Action: 2, Reward: 0.25, Total Reward: 5.8, Loss: 3.791454792022705\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 9, Action: 3, Reward: 0.25, Total Reward: 6.05, Loss: 2.399144172668457\n",
      "Model acting\n",
      "\n",
      "episode: 103, reward: -93.95\n",
      "\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: -8.0, Total Reward: -8.0, Loss: 5.609101295471191\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 1, Action: 3, Reward: 0.25, Total Reward: -7.75, Loss: 4.623175621032715\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: 2.1, Total Reward: -5.65, Loss: 5.348330497741699\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: 2.0, Total Reward: -3.6500000000000004, Loss: 2.6533026695251465\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: 2.0, Total Reward: -1.6500000000000004, Loss: 7.072280406951904\n",
      "Optimizing model...\n",
      "Step: 5, Action: 1, Reward: 0.0, Total Reward: -1.6500000000000004, Loss: 5.42842960357666\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 6, Action: 0, Reward: 2.0, Total Reward: 0.34999999999999964, Loss: 3.8916711807250977\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 7, Action: 0, Reward: 2.0, Total Reward: 2.3499999999999996, Loss: 1.1023249626159668\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 8, Action: 0, Reward: -8.0, Total Reward: -5.65, Loss: 2.2619898319244385\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 9, Action: 0, Reward: 2.0, Total Reward: -3.6500000000000004, Loss: 7.021785736083984\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 10, Action: 0, Reward: -8.0, Total Reward: -11.65, Loss: 9.001646041870117\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 11, Action: 0, Reward: 2.0, Total Reward: -9.65, Loss: 3.8990049362182617\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 12, Action: 0, Reward: -8.0, Total Reward: -17.65, Loss: 5.4234466552734375\n",
      "Optimizing model...\n",
      "Step: 13, Action: 1, Reward: 0.0, Total Reward: -17.65, Loss: 5.268385887145996\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 14, Action: 0, Reward: 2.0, Total Reward: -15.649999999999999, Loss: 3.958946704864502\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 15, Action: 0, Reward: -8.0, Total Reward: -23.65, Loss: 0.4779461622238159\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 16, Action: 0, Reward: 2.0, Total Reward: -21.65, Loss: 5.6467671394348145\n",
      "Optimizing model...\n",
      "Step: 17, Action: 4, Reward: 0.3, Total Reward: -21.349999999999998, Loss: 0.9860737323760986\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 18, Action: 0, Reward: 2.0, Total Reward: -19.349999999999998, Loss: 5.519863605499268\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 19, Action: 0, Reward: 2.0, Total Reward: -17.349999999999998, Loss: 0.9582282900810242\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 20, Action: 0, Reward: -7.9, Total Reward: -25.25, Loss: 11.913692474365234\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 21, Action: 0, Reward: -8.0, Total Reward: -33.25, Loss: 2.73684024810791\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 22, Action: 0, Reward: -8.0, Total Reward: -41.25, Loss: 4.400683879852295\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 23, Action: 0, Reward: 2.0, Total Reward: -39.25, Loss: 4.194266319274902\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 24, Action: 0, Reward: 2.0, Total Reward: -37.25, Loss: 4.023497104644775\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 25, Action: 0, Reward: -8.0, Total Reward: -45.25, Loss: 2.286281108856201\n",
      "Optimizing model...\n",
      "Step: 26, Action: 1, Reward: 0.0, Total Reward: -45.25, Loss: 6.101192474365234\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 27, Action: 0, Reward: 2.0, Total Reward: -43.25, Loss: 10.764518737792969\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 28, Action: 0, Reward: -8.0, Total Reward: -51.25, Loss: 0.7671934366226196\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 29, Action: 2, Reward: 0.25, Total Reward: -51.0, Loss: 2.450991630554199\n",
      "Optimizing model...\n",
      "Step: 30, Action: 2, Reward: 0.25, Total Reward: -50.75, Loss: 5.602993965148926\n",
      "Optimizing model...\n",
      "Step: 31, Action: 0, Reward: 2.0, Total Reward: -48.75, Loss: 5.374642372131348\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 32, Action: 2, Reward: 0.25, Total Reward: -48.5, Loss: 2.543076992034912\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 33, Action: 2, Reward: 0.25, Total Reward: -48.25, Loss: 5.109766006469727\n",
      "Model acting\n",
      "\n",
      "episode: 104, reward: -148.25\n",
      "\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 2, Reward: 0.25, Total Reward: 0.25, Loss: 2.5701584815979004\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: 2.0, Total Reward: 2.25, Loss: 3.8047592639923096\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 2, Reward: 0.25, Total Reward: 2.5, Loss: 1.108916997909546\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: 2.1, Total Reward: 4.6, Loss: 2.472930908203125\n",
      "Optimizing model...\n",
      "Step: 4, Action: 4, Reward: 0.2, Total Reward: 4.8, Loss: 4.540657043457031\n",
      "Optimizing model...\n",
      "Step: 5, Action: 0, Reward: 2.0, Total Reward: 6.8, Loss: 2.477712392807007\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 6, Action: 2, Reward: 0.25, Total Reward: 7.05, Loss: 2.0719857215881348\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 7, Action: 2, Reward: 0.25, Total Reward: 7.3, Loss: 0.8061662316322327\n",
      "Optimizing model...\n",
      "Step: 8, Action: 1, Reward: 0.0, Total Reward: 7.3, Loss: 4.016359806060791\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 9, Action: 2, Reward: 0.25, Total Reward: 7.55, Loss: 4.004734039306641\n",
      "Optimizing model...\n",
      "Step: 10, Action: 1, Reward: 0.0, Total Reward: 7.55, Loss: 5.663669586181641\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 11, Action: 2, Reward: 0.25, Total Reward: 7.8, Loss: 5.555304527282715\n",
      "Optimizing model...\n",
      "Step: 12, Action: 1, Reward: 0.0, Total Reward: 7.8, Loss: 2.5117745399475098\n",
      "Optimizing model...\n",
      "Step: 13, Action: 4, Reward: 0.2, Total Reward: 8.0, Loss: 5.195433616638184\n",
      "Optimizing model...\n",
      "Step: 14, Action: 4, Reward: 0.2, Total Reward: 8.2, Loss: 5.9640793800354\n",
      "Optimizing model...\n",
      "Step: 15, Action: 4, Reward: 0.2, Total Reward: 8.399999999999999, Loss: 2.7207159996032715\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 16, Action: 0, Reward: 2.0, Total Reward: 10.399999999999999, Loss: 6.680231094360352\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 17, Action: 0, Reward: -8.0, Total Reward: 2.3999999999999986, Loss: 5.59916877746582\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 18, Action: 2, Reward: 0.25, Total Reward: 2.6499999999999986, Loss: 3.5637848377227783\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 19, Action: 0, Reward: 2.0, Total Reward: 4.649999999999999, Loss: 6.679079532623291\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 20, Action: 0, Reward: -8.0, Total Reward: -3.3500000000000014, Loss: 2.4174976348876953\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 21, Action: 0, Reward: -8.0, Total Reward: -11.350000000000001, Loss: 4.298242092132568\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 22, Action: 0, Reward: -8.0, Total Reward: -19.35, Loss: 2.585179328918457\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 23, Action: 0, Reward: -8.0, Total Reward: -27.35, Loss: 7.307295322418213\n",
      "\n",
      "episode: 105, reward: -127.35\n",
      "\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: -8.0, Total Reward: -8.0, Loss: 4.393345355987549\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: -8.0, Total Reward: -16.0, Loss: 4.340011119842529\n",
      "Optimizing model...\n",
      "Step: 2, Action: 2, Reward: 0.25, Total Reward: -15.75, Loss: 5.902898788452148\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: 2.0, Total Reward: -13.75, Loss: 0.6745559573173523\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: 2.0, Total Reward: -11.75, Loss: 5.382542610168457\n",
      "Model acting\n",
      "\n",
      "episode: 106, reward: -111.75\n",
      "\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: -8.0, Total Reward: -8.0, Loss: 1.0504701137542725\n",
      "Optimizing model...\n",
      "Step: 1, Action: 4, Reward: 0.2, Total Reward: -7.8, Loss: 3.9593262672424316\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: 2.0, Total Reward: -5.8, Loss: 0.9503344297409058\n",
      "Optimizing model...\n",
      "Step: 3, Action: 3, Reward: 0.25, Total Reward: -5.55, Loss: 0.996317982673645\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: 2.0, Total Reward: -3.55, Loss: 6.810033321380615\n",
      "Optimizing model...\n",
      "Step: 5, Action: 1, Reward: 0.0, Total Reward: -3.55, Loss: 8.583826065063477\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 6, Action: 0, Reward: 2.0, Total Reward: -1.5499999999999998, Loss: 6.677894592285156\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 7, Action: 0, Reward: 2.0, Total Reward: 0.4500000000000002, Loss: 3.622002601623535\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 8, Action: 0, Reward: -8.0, Total Reward: -7.55, Loss: 3.692063808441162\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 9, Action: 0, Reward: 2.0, Total Reward: -5.55, Loss: 1.088415265083313\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 10, Action: 0, Reward: -7.9, Total Reward: -13.45, Loss: 4.9885640144348145\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 11, Action: 0, Reward: 2.0, Total Reward: -11.45, Loss: 2.568023681640625\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 12, Action: 0, Reward: 2.0, Total Reward: -9.45, Loss: 5.53688907623291\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 13, Action: 0, Reward: 2.0, Total Reward: -7.449999999999999, Loss: 1.1605799198150635\n",
      "Optimizing model...\n",
      "Step: 14, Action: 0, Reward: -8.0, Total Reward: -15.45, Loss: 2.1339914798736572\n",
      "Optimizing model...\n",
      "Step: 15, Action: 3, Reward: 0.25, Total Reward: -15.2, Loss: 7.822075843811035\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 16, Action: 0, Reward: 2.0, Total Reward: -13.2, Loss: 5.178000450134277\n",
      "Optimizing model...\n",
      "Step: 17, Action: 0, Reward: 2.0, Total Reward: -11.2, Loss: 2.36879301071167\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 18, Action: 0, Reward: -8.0, Total Reward: -19.2, Loss: 2.3065972328186035\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 19, Action: 0, Reward: 2.0, Total Reward: -17.2, Loss: 4.126595497131348\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 20, Action: 0, Reward: -8.0, Total Reward: -25.2, Loss: 3.7293381690979004\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 21, Action: 0, Reward: 2.0, Total Reward: -23.2, Loss: 4.214426040649414\n",
      "Optimizing model...\n",
      "Step: 22, Action: 0, Reward: -8.0, Total Reward: -31.2, Loss: 5.796043872833252\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 23, Action: 0, Reward: -8.0, Total Reward: -39.2, Loss: 0.7014684677124023\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 24, Action: 0, Reward: 2.0, Total Reward: -37.2, Loss: 5.726687431335449\n",
      "Optimizing model...\n",
      "Step: 25, Action: 3, Reward: 0.25, Total Reward: -36.95, Loss: 4.1509904861450195\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 26, Action: 0, Reward: 2.0, Total Reward: -34.95, Loss: 0.8655039072036743\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 27, Action: 0, Reward: 2.0, Total Reward: -32.95, Loss: 4.105937957763672\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 28, Action: 0, Reward: -8.0, Total Reward: -40.95, Loss: 9.083808898925781\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 29, Action: 0, Reward: 2.0, Total Reward: -38.95, Loss: 5.545269012451172\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 30, Action: 0, Reward: 2.0, Total Reward: -36.95, Loss: 2.128331184387207\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 31, Action: 0, Reward: 2.0, Total Reward: -34.95, Loss: 5.0851569175720215\n",
      "Optimizing model...\n",
      "Step: 32, Action: 3, Reward: 0.25, Total Reward: -34.7, Loss: 7.143808364868164\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 33, Action: 0, Reward: 2.0, Total Reward: -32.7, Loss: 4.976291179656982\n",
      "Optimizing model...\n",
      "Step: 34, Action: 2, Reward: 0.25, Total Reward: -32.45, Loss: 7.519508361816406\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 35, Action: 0, Reward: 2.0, Total Reward: -30.450000000000003, Loss: 2.3637001514434814\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 36, Action: 0, Reward: -8.0, Total Reward: -38.45, Loss: 6.98095703125\n",
      "Optimizing model...\n",
      "Step: 37, Action: 3, Reward: 0.35, Total Reward: -38.1, Loss: 2.4179563522338867\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 38, Action: 0, Reward: 2.0, Total Reward: -36.1, Loss: 6.808731555938721\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 39, Action: 0, Reward: 2.0, Total Reward: -34.1, Loss: 6.595269203186035\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 40, Action: 0, Reward: -8.0, Total Reward: -42.1, Loss: 7.004641056060791\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 41, Action: 0, Reward: -8.0, Total Reward: -50.1, Loss: 2.466731548309326\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 42, Action: 0, Reward: 2.0, Total Reward: -48.1, Loss: 3.9813313484191895\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 43, Action: 0, Reward: -8.0, Total Reward: -56.1, Loss: 4.255740642547607\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 44, Action: 0, Reward: -8.0, Total Reward: -64.1, Loss: 2.9401793479919434\n",
      "Model acting\n",
      "\n",
      "episode: 107, reward: -164.1\n",
      "\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: 2.0, Total Reward: 2.0, Loss: 3.561861276626587\n",
      "Optimizing model...\n",
      "Step: 1, Action: 3, Reward: 0.25, Total Reward: 2.25, Loss: 6.279633522033691\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: 2.0, Total Reward: 4.25, Loss: 3.807681083679199\n",
      "Optimizing model...\n",
      "Step: 3, Action: 4, Reward: 0.2, Total Reward: 4.45, Loss: 3.7794113159179688\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: 2.1, Total Reward: 6.550000000000001, Loss: 7.921634674072266\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 5, Action: 0, Reward: -8.0, Total Reward: -1.4499999999999993, Loss: 2.2878808975219727\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 6, Action: 0, Reward: 2.0, Total Reward: 0.5500000000000007, Loss: 0.6991207599639893\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 7, Action: 0, Reward: -8.0, Total Reward: -7.449999999999999, Loss: 4.077086925506592\n",
      "Optimizing model...\n",
      "Step: 8, Action: 3, Reward: 0.25, Total Reward: -7.199999999999999, Loss: 2.7811031341552734\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 9, Action: 0, Reward: 2.0, Total Reward: -5.199999999999999, Loss: 3.673895835876465\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 10, Action: 0, Reward: 2.0, Total Reward: -3.1999999999999993, Loss: 6.942083358764648\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 11, Action: 0, Reward: -8.0, Total Reward: -11.2, Loss: 3.7786598205566406\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 12, Action: 0, Reward: -8.0, Total Reward: -19.2, Loss: 2.17626953125\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 13, Action: 0, Reward: 2.0, Total Reward: -17.2, Loss: 7.485065460205078\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 14, Action: 0, Reward: 2.0, Total Reward: -15.2, Loss: 2.358151912689209\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 15, Action: 0, Reward: -8.0, Total Reward: -23.2, Loss: 4.252017021179199\n",
      "Model acting\n",
      "\n",
      "episode: 108, reward: -123.2\n",
      "\n",
      "Optimizing model...\n",
      "Step: 0, Action: 3, Reward: 0.25, Total Reward: 0.25, Loss: 6.849802017211914\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: 2.0, Total Reward: 2.25, Loss: 3.9290387630462646\n",
      "Optimizing model...\n",
      "Step: 2, Action: 2, Reward: 0.35, Total Reward: 2.6, Loss: 5.914026260375977\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: 2.0, Total Reward: 4.6, Loss: 0.8311291337013245\n",
      "Optimizing model...\n",
      "Step: 4, Action: 3, Reward: 0.25, Total Reward: 4.85, Loss: 6.508113861083984\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 5, Action: 0, Reward: 2.0, Total Reward: 6.85, Loss: 3.963780164718628\n",
      "Optimizing model...\n",
      "Step: 6, Action: 4, Reward: 0.2, Total Reward: 7.05, Loss: 6.095792770385742\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 7, Action: 0, Reward: 2.0, Total Reward: 9.05, Loss: 7.217474460601807\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 8, Action: 0, Reward: 2.0, Total Reward: 11.05, Loss: 2.0623984336853027\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 9, Action: 0, Reward: 2.0, Total Reward: 13.05, Loss: 1.952730417251587\n",
      "Optimizing model...\n",
      "Step: 10, Action: 0, Reward: -8.0, Total Reward: 5.050000000000001, Loss: 5.596700668334961\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 11, Action: 0, Reward: 2.2, Total Reward: 7.250000000000001, Loss: 3.7866740226745605\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 12, Action: 0, Reward: -8.0, Total Reward: -0.7499999999999991, Loss: 2.4120590686798096\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 13, Action: 0, Reward: -8.0, Total Reward: -8.75, Loss: 5.405041694641113\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 14, Action: 0, Reward: 2.0, Total Reward: -6.75, Loss: 1.1892154216766357\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 15, Action: 0, Reward: 2.0, Total Reward: -4.75, Loss: 1.3267805576324463\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 16, Action: 0, Reward: 2.0, Total Reward: -2.75, Loss: 5.845299243927002\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 17, Action: 0, Reward: -8.0, Total Reward: -10.75, Loss: 3.8081324100494385\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 18, Action: 0, Reward: 2.0, Total Reward: -8.75, Loss: 5.343151569366455\n",
      "Optimizing model...\n",
      "Step: 19, Action: 2, Reward: 0.25, Total Reward: -8.5, Loss: 2.677964687347412\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 20, Action: 0, Reward: 2.0, Total Reward: -6.5, Loss: 2.2060837745666504\n",
      "Optimizing model...\n",
      "Step: 21, Action: 0, Reward: 2.0, Total Reward: -4.5, Loss: 3.99503755569458\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 22, Action: 0, Reward: 2.0, Total Reward: -2.5, Loss: 4.072004318237305\n",
      "Optimizing model...\n",
      "Step: 23, Action: 3, Reward: 0.25, Total Reward: -2.25, Loss: 5.149417877197266\n",
      "Model acting\n",
      "\n",
      "episode: 109, reward: -102.25\n",
      "\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: 2.0, Total Reward: 2.0, Loss: 5.881691932678223\n",
      "Optimizing model...\n",
      "Step: 1, Action: 1, Reward: 0.0, Total Reward: 2.0, Loss: 0.7375665903091431\n",
      "Optimizing model...\n",
      "Step: 2, Action: 3, Reward: 0.25, Total Reward: 2.25, Loss: 4.069641590118408\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: 2.0, Total Reward: 4.25, Loss: 2.0281982421875\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: -8.0, Total Reward: -3.75, Loss: 5.54686975479126\n",
      "Optimizing model...\n",
      "Step: 5, Action: 4, Reward: 0.2, Total Reward: -3.55, Loss: 2.7748985290527344\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 6, Action: 0, Reward: 2.0, Total Reward: -1.5499999999999998, Loss: 2.806762456893921\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 7, Action: 0, Reward: 2.0, Total Reward: 0.4500000000000002, Loss: 4.081584453582764\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 8, Action: 4, Reward: 0.2, Total Reward: 0.6500000000000001, Loss: 4.607059478759766\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 9, Action: 0, Reward: 2.0, Total Reward: 2.6500000000000004, Loss: 5.061140060424805\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 10, Action: 0, Reward: -8.0, Total Reward: -5.35, Loss: 6.273331642150879\n",
      "\n",
      "episode: 110, reward: -105.35\n",
      "\n",
      "Model saved\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: -8.0, Total Reward: -8.0, Loss: 3.7453646659851074\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: -8.0, Total Reward: -16.0, Loss: 3.7313408851623535\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 3, Reward: 0.25, Total Reward: -15.75, Loss: 0.9017972946166992\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 1, Reward: 0.0, Total Reward: -15.75, Loss: 2.7376291751861572\n",
      "Model acting\n",
      "\n",
      "episode: 111, reward: -115.75\n",
      "\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: 2.0, Total Reward: 2.0, Loss: 9.32542610168457\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: 2.0, Total Reward: 4.0, Loss: 2.78420352935791\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: 2.0, Total Reward: 6.0, Loss: 5.408838272094727\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: 2.0, Total Reward: 8.0, Loss: 4.8345160484313965\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: 2.0, Total Reward: 10.0, Loss: 5.392548561096191\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 5, Action: 0, Reward: -7.9, Total Reward: 2.0999999999999996, Loss: 0.9392092823982239\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 6, Action: 0, Reward: 2.0, Total Reward: 4.1, Loss: 4.351078510284424\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 7, Action: 0, Reward: 2.0, Total Reward: 6.1, Loss: 2.063231945037842\n",
      "\n",
      "episode: 112, reward: -93.9\n",
      "\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: -8.0, Total Reward: -8.0, Loss: 5.631007194519043\n",
      "Optimizing model...\n",
      "Step: 1, Action: 1, Reward: 0.0, Total Reward: -8.0, Loss: 4.866180419921875\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: 2.0, Total Reward: -6.0, Loss: 5.551939010620117\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: -8.0, Total Reward: -14.0, Loss: 2.500457763671875\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: -8.0, Total Reward: -22.0, Loss: 5.732292652130127\n",
      "Optimizing model...\n",
      "Step: 5, Action: 0, Reward: -8.0, Total Reward: -30.0, Loss: 5.408918380737305\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 6, Action: 0, Reward: 2.0, Total Reward: -28.0, Loss: 2.423556089401245\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 7, Action: 0, Reward: 2.0, Total Reward: -26.0, Loss: 4.05230712890625\n",
      "Model acting\n",
      "\n",
      "episode: 113, reward: -126.0\n",
      "\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: -8.0, Total Reward: -8.0, Loss: 2.587388515472412\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: 2.0, Total Reward: -6.0, Loss: 4.376017093658447\n",
      "Optimizing model...\n",
      "Step: 2, Action: 2, Reward: 0.25, Total Reward: -5.75, Loss: 7.86411714553833\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: 2.0, Total Reward: -3.75, Loss: 2.859299898147583\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: 2.0, Total Reward: -1.75, Loss: 5.59529972076416\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 5, Action: 0, Reward: 2.0, Total Reward: 0.25, Loss: 2.4409050941467285\n",
      "Model acting\n",
      "\n",
      "episode: 114, reward: -99.75\n",
      "\n",
      "Optimizing model...\n",
      "Step: 0, Action: 3, Reward: 0.25, Total Reward: 0.25, Loss: 4.884744644165039\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: 2.0, Total Reward: 2.25, Loss: 7.143224716186523\n",
      "Optimizing model...\n",
      "Step: 2, Action: 1, Reward: 0.0, Total Reward: 2.25, Loss: 7.543018817901611\n",
      "Optimizing model...\n",
      "Step: 3, Action: 4, Reward: 0.2, Total Reward: 2.45, Loss: 8.767751693725586\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: 2.0, Total Reward: 4.45, Loss: 2.7191834449768066\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 5, Action: 0, Reward: -8.0, Total Reward: -3.55, Loss: 4.668170928955078\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 6, Action: 0, Reward: -8.0, Total Reward: -11.55, Loss: 2.5572423934936523\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 7, Action: 0, Reward: 2.0, Total Reward: -9.55, Loss: 5.398631572723389\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 8, Action: 0, Reward: 2.0, Total Reward: -7.550000000000001, Loss: 7.4121246337890625\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 9, Action: 0, Reward: -8.0, Total Reward: -15.55, Loss: 6.0147600173950195\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 10, Action: 0, Reward: 2.0, Total Reward: -13.55, Loss: 4.063769817352295\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 11, Action: 0, Reward: 2.0, Total Reward: -11.55, Loss: 5.1843671798706055\n",
      "Optimizing model...\n",
      "Step: 12, Action: 0, Reward: 2.0, Total Reward: -9.55, Loss: 2.005636692047119\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 13, Action: 0, Reward: 2.0, Total Reward: -7.550000000000001, Loss: 5.4321184158325195\n",
      "Model acting\n",
      "\n",
      "episode: 115, reward: -107.55\n",
      "\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: -8.0, Total Reward: -8.0, Loss: 2.3780994415283203\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: -8.0, Total Reward: -16.0, Loss: 3.9872546195983887\n",
      "Optimizing model...\n",
      "Step: 2, Action: 1, Reward: 0.0, Total Reward: -16.0, Loss: 5.319830894470215\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: 2.0, Total Reward: -14.0, Loss: 9.0138578414917\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: 2.0, Total Reward: -12.0, Loss: 8.758764266967773\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 5, Action: 0, Reward: 2.0, Total Reward: -10.0, Loss: 4.138643264770508\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 6, Action: 0, Reward: 2.0, Total Reward: -8.0, Loss: 7.295239448547363\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 7, Action: 0, Reward: -8.0, Total Reward: -16.0, Loss: 5.954099655151367\n",
      "Model acting\n",
      "\n",
      "episode: 116, reward: -116.0\n",
      "\n",
      "Optimizing model...\n",
      "Step: 0, Action: 4, Reward: 0.2, Total Reward: 0.2, Loss: 1.0930589437484741\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: 2.0, Total Reward: 2.2, Loss: 4.353912353515625\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: 2.0, Total Reward: 4.2, Loss: 8.75961685180664\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: -8.0, Total Reward: -3.8, Loss: 8.879241943359375\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: -7.9, Total Reward: -11.7, Loss: 2.3322858810424805\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 5, Action: 0, Reward: -8.0, Total Reward: -19.7, Loss: 2.787477493286133\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 6, Action: 0, Reward: -8.0, Total Reward: -27.7, Loss: 3.828308343887329\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 7, Action: 0, Reward: -8.0, Total Reward: -35.7, Loss: 3.941037178039551\n",
      "Model acting\n",
      "\n",
      "episode: 117, reward: -135.7\n",
      "\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: -8.0, Total Reward: -8.0, Loss: 2.634052276611328\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: 2.0, Total Reward: -6.0, Loss: 6.644225597381592\n",
      "Optimizing model...\n",
      "Step: 2, Action: 1, Reward: 0.0, Total Reward: -6.0, Loss: 3.590947151184082\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: 2.0, Total Reward: -4.0, Loss: 4.119056224822998\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: 2.0, Total Reward: -2.0, Loss: 7.001511096954346\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 5, Action: 0, Reward: -8.0, Total Reward: -10.0, Loss: 6.971366882324219\n",
      "Model acting\n",
      "\n",
      "episode: 118, reward: -110.0\n",
      "\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: 2.0, Total Reward: 2.0, Loss: 5.585311412811279\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: -8.0, Total Reward: -6.0, Loss: 4.0276875495910645\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: -8.0, Total Reward: -14.0, Loss: 3.447592258453369\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: -8.0, Total Reward: -22.0, Loss: 5.380943775177002\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: -8.0, Total Reward: -30.0, Loss: 3.910008668899536\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 5, Action: 0, Reward: -8.0, Total Reward: -38.0, Loss: 5.430097579956055\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 6, Action: 0, Reward: 2.0, Total Reward: -36.0, Loss: 5.931740760803223\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 7, Action: 0, Reward: 2.0, Total Reward: -34.0, Loss: 0.9923462271690369\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 8, Action: 0, Reward: 2.0, Total Reward: -32.0, Loss: 2.033234119415283\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 9, Action: 0, Reward: 2.0, Total Reward: -30.0, Loss: 2.5858848094940186\n",
      "Optimizing model...\n",
      "Step: 10, Action: 4, Reward: 0.2, Total Reward: -29.8, Loss: 2.5823521614074707\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 11, Action: 0, Reward: 2.0, Total Reward: -27.8, Loss: 3.2196669578552246\n",
      "Model acting\n",
      "\n",
      "episode: 119, reward: -127.8\n",
      "\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: 2.0, Total Reward: 2.0, Loss: 2.704847812652588\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: 2.0, Total Reward: 4.0, Loss: 6.921921730041504\n",
      "Optimizing model...\n",
      "Step: 2, Action: 3, Reward: 0.25, Total Reward: 4.25, Loss: 5.807857036590576\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: 2.0, Total Reward: 6.25, Loss: 6.523184776306152\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: -7.9, Total Reward: -1.6500000000000004, Loss: 4.6727399826049805\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 5, Action: 0, Reward: 2.1, Total Reward: 0.44999999999999973, Loss: 6.46074914932251\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 6, Action: 0, Reward: 2.0, Total Reward: 2.4499999999999997, Loss: 3.9441440105438232\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 7, Action: 0, Reward: 2.0, Total Reward: 4.449999999999999, Loss: 0.8717976808547974\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 8, Action: 0, Reward: -7.9, Total Reward: -3.450000000000001, Loss: 3.7448859214782715\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 9, Action: 0, Reward: -8.0, Total Reward: -11.450000000000001, Loss: 5.222984313964844\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 10, Action: 0, Reward: -8.0, Total Reward: -19.450000000000003, Loss: 4.24774169921875\n",
      "Optimizing model...\n",
      "Step: 11, Action: 0, Reward: 2.0, Total Reward: -17.450000000000003, Loss: 4.922542572021484\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 12, Action: 0, Reward: -8.0, Total Reward: -25.450000000000003, Loss: 3.751488208770752\n",
      "Optimizing model...\n",
      "Step: 13, Action: 1, Reward: 0.0, Total Reward: -25.450000000000003, Loss: 3.9288721084594727\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 14, Action: 0, Reward: 2.0, Total Reward: -23.450000000000003, Loss: 2.638486862182617\n",
      "Optimizing model...\n",
      "Step: 15, Action: 1, Reward: 0.0, Total Reward: -23.450000000000003, Loss: 4.303496360778809\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 16, Action: 0, Reward: 2.0, Total Reward: -21.450000000000003, Loss: 1.0754501819610596\n",
      "Optimizing model...\n",
      "Step: 17, Action: 2, Reward: 0.25, Total Reward: -21.200000000000003, Loss: 7.191829204559326\n",
      "Model acting\n",
      "\n",
      "episode: 120, reward: -121.2\n",
      "\n",
      "Model saved\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: 2.0, Total Reward: 2.0, Loss: 0.9237573146820068\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: -8.0, Total Reward: -6.0, Loss: 7.443476676940918\n",
      "Optimizing model...\n",
      "Step: 2, Action: 2, Reward: 0.25, Total Reward: -5.75, Loss: 1.886808156967163\n",
      "Optimizing model...\n",
      "Step: 3, Action: 3, Reward: 0.25, Total Reward: -5.5, Loss: 2.49454402923584\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: 2.0, Total Reward: -3.5, Loss: 2.3256723880767822\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 5, Action: 0, Reward: -7.9, Total Reward: -11.4, Loss: 2.589261293411255\n",
      "Optimizing model...\n",
      "Step: 6, Action: 4, Reward: 0.2, Total Reward: -11.200000000000001, Loss: 5.7788591384887695\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 7, Action: 0, Reward: 2.0, Total Reward: -9.200000000000001, Loss: 0.7651277184486389\n",
      "Optimizing model...\n",
      "Step: 8, Action: 0, Reward: 2.0, Total Reward: -7.200000000000001, Loss: 2.2247424125671387\n",
      "Optimizing model...\n",
      "Step: 9, Action: 4, Reward: 0.2, Total Reward: -7.000000000000001, Loss: 5.805864334106445\n",
      "\n",
      "episode: 121, reward: -107.0\n",
      "\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: 2.0, Total Reward: 2.0, Loss: 4.543728828430176\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: 2.0, Total Reward: 4.0, Loss: 2.609248638153076\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: 2.0, Total Reward: 6.0, Loss: 1.23282790184021\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: 2.0, Total Reward: 8.0, Loss: 2.2249794006347656\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: 2.0, Total Reward: 10.0, Loss: 3.9930455684661865\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 5, Action: 0, Reward: -8.0, Total Reward: 2.0, Loss: 5.350008010864258\n",
      "Model acting\n",
      "\n",
      "episode: 122, reward: -98.0\n",
      "\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: -8.0, Total Reward: -8.0, Loss: 3.862154245376587\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: 2.0, Total Reward: -6.0, Loss: 4.237079620361328\n",
      "Optimizing model...\n",
      "Step: 2, Action: 1, Reward: 0.0, Total Reward: -6.0, Loss: 7.319336891174316\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: 2.1, Total Reward: -3.9, Loss: 5.24775505065918\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: 2.0, Total Reward: -1.9, Loss: 2.170549154281616\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 5, Action: 0, Reward: 2.0, Total Reward: 0.10000000000000009, Loss: 6.250436782836914\n",
      "Optimizing model...\n",
      "Step: 6, Action: 1, Reward: 0.0, Total Reward: 0.10000000000000009, Loss: 6.297446250915527\n",
      "\n",
      "episode: 123, reward: -99.9\n",
      "\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: -8.0, Total Reward: -8.0, Loss: 8.611288070678711\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: 2.0, Total Reward: -6.0, Loss: 4.281961441040039\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: -8.0, Total Reward: -14.0, Loss: 7.386632919311523\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: -8.0, Total Reward: -22.0, Loss: 2.403836250305176\n",
      "Optimizing model...\n",
      "Step: 4, Action: 3, Reward: 0.25, Total Reward: -21.75, Loss: 2.2921323776245117\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 5, Action: 0, Reward: 2.0, Total Reward: -19.75, Loss: 6.265681266784668\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 6, Action: 0, Reward: 2.0, Total Reward: -17.75, Loss: 7.534544467926025\n",
      "\n",
      "episode: 124, reward: -117.75\n",
      "\n",
      "Optimizing model...\n",
      "Step: 0, Action: 4, Reward: 0.2, Total Reward: 0.2, Loss: 6.119850158691406\n",
      "Optimizing model...\n",
      "Step: 1, Action: 4, Reward: 0.2, Total Reward: 0.4, Loss: 8.805469512939453\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: 2.0, Total Reward: 2.4, Loss: 3.7865519523620605\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: 2.0, Total Reward: 4.4, Loss: 7.147831439971924\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: -7.9, Total Reward: -3.5, Loss: 4.342698097229004\n",
      "Optimizing model...\n",
      "Step: 5, Action: 2, Reward: 0.35, Total Reward: -3.15, Loss: 10.447277069091797\n",
      "Optimizing model...\n",
      "Step: 6, Action: 1, Reward: 0.0, Total Reward: -3.15, Loss: 3.864311695098877\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 7, Action: 0, Reward: 2.0, Total Reward: -1.15, Loss: 6.619144439697266\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 8, Action: 0, Reward: 2.0, Total Reward: 0.8500000000000001, Loss: 5.209955215454102\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 9, Action: 0, Reward: -8.0, Total Reward: -7.15, Loss: 5.396054744720459\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 10, Action: 0, Reward: -8.0, Total Reward: -15.15, Loss: 9.449548721313477\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 11, Action: 0, Reward: 2.0, Total Reward: -13.15, Loss: 4.908510684967041\n",
      "Optimizing model...\n",
      "Step: 12, Action: 0, Reward: -8.0, Total Reward: -21.15, Loss: 0.9614717960357666\n",
      "Model acting\n",
      "\n",
      "episode: 125, reward: -121.15\n",
      "\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: 2.0, Total Reward: 2.0, Loss: 4.046712875366211\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: -8.0, Total Reward: -6.0, Loss: 4.110246181488037\n",
      "Optimizing model...\n",
      "Step: 2, Action: 2, Reward: 0.25, Total Reward: -5.75, Loss: 2.587557792663574\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: 2.0, Total Reward: -3.75, Loss: 2.953481435775757\n",
      "Optimizing model...\n",
      "Step: 4, Action: 4, Reward: 0.3, Total Reward: -3.45, Loss: 5.494353294372559\n",
      "Model acting\n",
      "\n",
      "episode: 126, reward: -103.45\n",
      "\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: -8.0, Total Reward: -8.0, Loss: 8.161705017089844\n",
      "Optimizing model...\n",
      "Step: 1, Action: 2, Reward: 0.25, Total Reward: -7.75, Loss: 3.801941394805908\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: 2.0, Total Reward: -5.75, Loss: 0.7159072160720825\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: -8.0, Total Reward: -13.75, Loss: 7.094243049621582\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: 2.0, Total Reward: -11.75, Loss: 9.047422409057617\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 5, Action: 0, Reward: 2.1, Total Reward: -9.65, Loss: 2.878833055496216\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 6, Action: 0, Reward: 2.0, Total Reward: -7.65, Loss: 4.2891950607299805\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 7, Action: 0, Reward: 2.0, Total Reward: -5.65, Loss: 2.902754783630371\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 8, Action: 0, Reward: 2.0, Total Reward: -3.6500000000000004, Loss: 2.880382776260376\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 9, Action: 0, Reward: 2.0, Total Reward: -1.6500000000000004, Loss: 3.876823902130127\n",
      "Optimizing model...\n",
      "Step: 10, Action: 1, Reward: 0.0, Total Reward: -1.6500000000000004, Loss: 2.9845170974731445\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 11, Action: 0, Reward: 2.0, Total Reward: 0.34999999999999964, Loss: 2.4825310707092285\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 12, Action: 0, Reward: 2.0, Total Reward: 2.3499999999999996, Loss: 1.5535982847213745\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 13, Action: 0, Reward: 2.0, Total Reward: 4.35, Loss: 2.8329429626464844\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 14, Action: 0, Reward: 2.0, Total Reward: 6.35, Loss: 9.150668144226074\n",
      "Optimizing model...\n",
      "Step: 15, Action: 1, Reward: 0.0, Total Reward: 6.35, Loss: 6.609832763671875\n",
      "Optimizing model...\n",
      "Step: 16, Action: 2, Reward: 0.25, Total Reward: 6.6, Loss: 7.5876383781433105\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 17, Action: 0, Reward: 2.0, Total Reward: 8.6, Loss: 2.5388832092285156\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 18, Action: 0, Reward: 2.0, Total Reward: 10.6, Loss: 4.96222448348999\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 19, Action: 0, Reward: -7.9, Total Reward: 2.6999999999999993, Loss: 1.0536692142486572\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 20, Action: 1, Reward: 0.0, Total Reward: 2.6999999999999993, Loss: 4.168198585510254\n",
      "Model acting\n",
      "\n",
      "episode: 127, reward: -97.3\n",
      "\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: 2.0, Total Reward: 2.0, Loss: 5.775351047515869\n",
      "Optimizing model...\n",
      "Step: 1, Action: 1, Reward: 0.0, Total Reward: 2.0, Loss: 2.825308322906494\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: 2.0, Total Reward: 4.0, Loss: 3.4445509910583496\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: -7.9, Total Reward: -3.9000000000000004, Loss: 3.709190845489502\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: -8.0, Total Reward: -11.9, Loss: 1.4919757843017578\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 5, Action: 0, Reward: 2.0, Total Reward: -9.9, Loss: 4.372541904449463\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 6, Action: 0, Reward: 2.0, Total Reward: -7.9, Loss: 4.831684112548828\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 7, Action: 0, Reward: 2.0, Total Reward: -5.9, Loss: 7.30460262298584\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 8, Action: 0, Reward: -7.9, Total Reward: -13.8, Loss: 2.3026299476623535\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 9, Action: 0, Reward: -8.0, Total Reward: -21.8, Loss: 3.0140984058380127\n",
      "Optimizing model...\n",
      "Step: 10, Action: 1, Reward: 0.0, Total Reward: -21.8, Loss: 1.566114902496338\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 11, Action: 0, Reward: 2.0, Total Reward: -19.8, Loss: 2.457460880279541\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 12, Action: 0, Reward: 2.0, Total Reward: -17.8, Loss: 1.8020325899124146\n",
      "Model acting\n",
      "\n",
      "episode: 128, reward: -117.8\n",
      "\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: -8.0, Total Reward: -8.0, Loss: 4.6734771728515625\n",
      "Optimizing model...\n",
      "Step: 1, Action: 4, Reward: 0.2, Total Reward: -7.8, Loss: 5.931982040405273\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: 2.1, Total Reward: -5.699999999999999, Loss: 0.5454927682876587\n",
      "Optimizing model...\n",
      "Step: 3, Action: 4, Reward: 0.2, Total Reward: -5.499999999999999, Loss: 5.40048360824585\n",
      "Optimizing model...\n",
      "Step: 4, Action: 1, Reward: 0.0, Total Reward: -5.499999999999999, Loss: 2.2557284832000732\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 5, Action: 0, Reward: 2.0, Total Reward: -3.499999999999999, Loss: 2.7303013801574707\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 6, Action: 0, Reward: 2.0, Total Reward: -1.4999999999999991, Loss: 5.198361396789551\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 7, Action: 0, Reward: 2.0, Total Reward: 0.5000000000000009, Loss: 7.175370693206787\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 8, Action: 0, Reward: 2.0, Total Reward: 2.500000000000001, Loss: 5.544449329376221\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 9, Action: 0, Reward: 2.1, Total Reward: 4.600000000000001, Loss: 4.3928656578063965\n",
      "\n",
      "episode: 129, reward: -95.4\n",
      "\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: 2.0, Total Reward: 2.0, Loss: 5.2401580810546875\n",
      "Optimizing model...\n",
      "Step: 1, Action: 4, Reward: 0.2, Total Reward: 2.2, Loss: 3.7633132934570312\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: 2.0, Total Reward: 4.2, Loss: 1.218436360359192\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: -8.0, Total Reward: -3.8, Loss: 7.301671028137207\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: 2.0, Total Reward: -1.7999999999999998, Loss: 4.156820297241211\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 5, Action: 0, Reward: 2.0, Total Reward: 0.20000000000000018, Loss: 4.4976043701171875\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 6, Action: 0, Reward: -8.0, Total Reward: -7.8, Loss: 7.0170159339904785\n",
      "\n",
      "episode: 130, reward: -107.8\n",
      "\n",
      "Model saved\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: -8.0, Total Reward: -8.0, Loss: 2.1198716163635254\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: 2.0, Total Reward: -6.0, Loss: 5.623201847076416\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: 2.0, Total Reward: -4.0, Loss: 4.3624114990234375\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: -8.0, Total Reward: -12.0, Loss: 5.122167587280273\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: 2.0, Total Reward: -10.0, Loss: 3.508885383605957\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 5, Action: 0, Reward: 2.0, Total Reward: -8.0, Loss: 5.978381156921387\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 6, Action: 0, Reward: -8.0, Total Reward: -16.0, Loss: 4.530285358428955\n",
      "Model acting\n",
      "\n",
      "episode: 131, reward: -116.0\n",
      "\n",
      "Optimizing model...\n",
      "Step: 0, Action: 3, Reward: 0.25, Total Reward: 0.25, Loss: 4.486118316650391\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: 2.0, Total Reward: 2.25, Loss: 4.490277290344238\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: 2.0, Total Reward: 4.25, Loss: 5.594294548034668\n",
      "Optimizing model...\n",
      "Step: 3, Action: 3, Reward: 0.25, Total Reward: 4.5, Loss: 2.7007274627685547\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: 2.0, Total Reward: 6.5, Loss: 6.816992282867432\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 5, Action: 0, Reward: -8.0, Total Reward: -1.5, Loss: 5.583308219909668\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 6, Action: 0, Reward: -8.0, Total Reward: -9.5, Loss: 1.6113123893737793\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 7, Action: 0, Reward: -8.0, Total Reward: -17.5, Loss: 4.075596809387207\n",
      "\n",
      "episode: 132, reward: -117.5\n",
      "\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: -8.0, Total Reward: -8.0, Loss: 5.852280616760254\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: 2.0, Total Reward: -6.0, Loss: 2.603452205657959\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: -7.9, Total Reward: -13.9, Loss: 1.262991189956665\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: -8.0, Total Reward: -21.9, Loss: 4.039952278137207\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: 2.0, Total Reward: -19.9, Loss: 8.436156272888184\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 5, Action: 0, Reward: -8.0, Total Reward: -27.9, Loss: 4.926776885986328\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 6, Action: 0, Reward: -8.0, Total Reward: -35.9, Loss: 5.64981746673584\n",
      "\n",
      "episode: 133, reward: -135.9\n",
      "\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: -8.0, Total Reward: -8.0, Loss: 3.578805446624756\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: -8.0, Total Reward: -16.0, Loss: 2.6085171699523926\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: 2.0, Total Reward: -14.0, Loss: 2.152255058288574\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: 2.0, Total Reward: -12.0, Loss: 4.647357940673828\n",
      "Model acting\n",
      "\n",
      "episode: 134, reward: -112.0\n",
      "\n",
      "Optimizing model...\n",
      "Step: 0, Action: 3, Reward: 0.25, Total Reward: 0.25, Loss: 2.109009265899658\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: 2.0, Total Reward: 2.25, Loss: 1.0344690084457397\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: 2.0, Total Reward: 4.25, Loss: 4.348130226135254\n",
      "Optimizing model...\n",
      "Step: 3, Action: 1, Reward: 0.0, Total Reward: 4.25, Loss: 5.9590044021606445\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: 2.0, Total Reward: 6.25, Loss: 4.270805358886719\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 5, Action: 0, Reward: 2.0, Total Reward: 8.25, Loss: 4.885109901428223\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 6, Action: 0, Reward: -8.0, Total Reward: 0.25, Loss: 0.6529843807220459\n",
      "Optimizing model...\n",
      "Step: 7, Action: 4, Reward: 0.2, Total Reward: 0.45, Loss: 0.9340863823890686\n",
      "Optimizing model...\n",
      "Step: 8, Action: 3, Reward: 0.25, Total Reward: 0.7, Loss: 2.532095432281494\n",
      "Optimizing model...\n",
      "Step: 9, Action: 3, Reward: -9.75, Total Reward: -9.05, Loss: 7.340372085571289\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 10, Action: 0, Reward: 2.0, Total Reward: -7.050000000000001, Loss: 5.749914169311523\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 11, Action: 0, Reward: 2.0, Total Reward: -5.050000000000001, Loss: 6.659646034240723\n",
      "Model acting\n",
      "\n",
      "episode: 135, reward: -105.05\n",
      "\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: -8.0, Total Reward: -8.0, Loss: 0.9483917951583862\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: 2.0, Total Reward: -6.0, Loss: 3.223421096801758\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: 2.0, Total Reward: -4.0, Loss: 5.336369037628174\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: 2.0, Total Reward: -2.0, Loss: 7.659798622131348\n",
      "Model acting\n",
      "\n",
      "episode: 136, reward: -102.0\n",
      "\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: 2.0, Total Reward: 2.0, Loss: 6.192447185516357\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: 2.0, Total Reward: 4.0, Loss: 8.530452728271484\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: 2.0, Total Reward: 6.0, Loss: 0.7448506355285645\n",
      "Optimizing model...\n",
      "Step: 3, Action: 3, Reward: 0.25, Total Reward: 6.25, Loss: 2.6140875816345215\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: 2.0, Total Reward: 8.25, Loss: 7.015932559967041\n",
      "Model acting\n",
      "\n",
      "episode: 137, reward: -91.75\n",
      "\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: -8.0, Total Reward: -8.0, Loss: 6.219459533691406\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: 2.0, Total Reward: -6.0, Loss: 8.434993743896484\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: 2.0, Total Reward: -4.0, Loss: 4.06650447845459\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: 2.0, Total Reward: -2.0, Loss: 4.541393280029297\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: -8.0, Total Reward: -10.0, Loss: 5.6031012535095215\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 5, Action: 0, Reward: 2.0, Total Reward: -8.0, Loss: 2.7341480255126953\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 6, Action: 0, Reward: 2.0, Total Reward: -6.0, Loss: 3.8949036598205566\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 7, Action: 0, Reward: 2.0, Total Reward: -4.0, Loss: 2.8242642879486084\n",
      "Optimizing model...\n",
      "Step: 8, Action: 3, Reward: 0.25, Total Reward: -3.75, Loss: 3.1949315071105957\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 9, Action: 0, Reward: 2.0, Total Reward: -1.75, Loss: 9.774667739868164\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 10, Action: 0, Reward: -8.0, Total Reward: -9.75, Loss: 5.800989151000977\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 11, Action: 0, Reward: 2.0, Total Reward: -7.75, Loss: 4.936317443847656\n",
      "Optimizing model...\n",
      "Step: 12, Action: 4, Reward: 0.2, Total Reward: -7.55, Loss: 7.130102157592773\n",
      "Model acting\n",
      "\n",
      "episode: 138, reward: -107.55\n",
      "\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: -8.0, Total Reward: -8.0, Loss: 5.617689609527588\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: -8.0, Total Reward: -16.0, Loss: 8.658756256103516\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 4, Reward: 0.3, Total Reward: -15.7, Loss: 9.233455657958984\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 4, Reward: 0.2, Total Reward: -15.5, Loss: 5.827556610107422\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: 2.0, Total Reward: -13.5, Loss: 7.190669536590576\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 5, Action: 1, Reward: 0.0, Total Reward: -13.5, Loss: 5.861445903778076\n",
      "Model acting\n",
      "\n",
      "episode: 139, reward: -113.5\n",
      "\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: -8.0, Total Reward: -8.0, Loss: 1.0944907665252686\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: 2.0, Total Reward: -6.0, Loss: 5.812138080596924\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 2, Reward: 0.25, Total Reward: -5.75, Loss: 2.844558000564575\n",
      "Optimizing model...\n",
      "Step: 3, Action: 1, Reward: 0.0, Total Reward: -5.75, Loss: 5.246708393096924\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 4, Action: 4, Reward: 0.2, Total Reward: -5.55, Loss: 4.3873677253723145\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 5, Action: 2, Reward: 0.25, Total Reward: -5.3, Loss: 4.618374347686768\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 6, Action: 4, Reward: 0.2, Total Reward: -5.1, Loss: 6.774069786071777\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 7, Action: 4, Reward: -9.7, Total Reward: -14.799999999999999, Loss: 5.536903381347656\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 8, Action: 4, Reward: -9.8, Total Reward: -24.6, Loss: 3.6688778400421143\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 9, Action: 4, Reward: -9.8, Total Reward: -34.400000000000006, Loss: 4.44158935546875\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 10, Action: 4, Reward: -9.8, Total Reward: -44.2, Loss: 4.7159199714660645\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 11, Action: 0, Reward: 2.0, Total Reward: -42.2, Loss: 2.7008161544799805\n",
      "Optimizing model...\n",
      "Step: 12, Action: 2, Reward: 0.35, Total Reward: -41.85, Loss: 4.149867057800293\n",
      "Optimizing model...\n",
      "Step: 13, Action: 3, Reward: 0.25, Total Reward: -41.6, Loss: 4.124246597290039\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 14, Action: 4, Reward: 0.2, Total Reward: -41.4, Loss: 5.730850696563721\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 15, Action: 4, Reward: -9.8, Total Reward: -51.2, Loss: 2.8074393272399902\n",
      "Optimizing model...\n",
      "Step: 16, Action: 2, Reward: 0.25, Total Reward: -50.95, Loss: 5.31277322769165\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 17, Action: 4, Reward: 0.2, Total Reward: -50.75, Loss: 2.4103481769561768\n",
      "Model acting\n",
      "\n",
      "episode: 140, reward: -150.75\n",
      "\n",
      "Model saved\n",
      "Optimizing model...\n",
      "Step: 0, Action: 3, Reward: 0.25, Total Reward: 0.25, Loss: 3.1065845489501953\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: 2.0, Total Reward: 2.25, Loss: 2.7432236671447754\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: 2.0, Total Reward: 4.25, Loss: 2.476515769958496\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 4, Reward: 0.2, Total Reward: 4.45, Loss: 0.8465425968170166\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: 2.0, Total Reward: 6.45, Loss: 6.779782295227051\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 5, Action: 4, Reward: 0.3, Total Reward: 6.75, Loss: 2.3936221599578857\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 6, Action: 4, Reward: 0.2, Total Reward: 6.95, Loss: 1.1078518629074097\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 7, Action: 4, Reward: 0.2, Total Reward: 7.15, Loss: 4.189242362976074\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 8, Action: 4, Reward: 0.2, Total Reward: 7.3500000000000005, Loss: 3.4783077239990234\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 9, Action: 4, Reward: -9.8, Total Reward: -2.45, Loss: 4.545630931854248\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 10, Action: 4, Reward: 0.2, Total Reward: -2.25, Loss: 6.269680023193359\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 11, Action: 4, Reward: -9.8, Total Reward: -12.05, Loss: 5.702907562255859\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 12, Action: 4, Reward: -9.8, Total Reward: -21.85, Loss: 2.995375871658325\n",
      "Optimizing model...\n",
      "Step: 13, Action: 1, Reward: 0.0, Total Reward: -21.85, Loss: 3.4649996757507324\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 14, Action: 4, Reward: 0.2, Total Reward: -21.650000000000002, Loss: 5.1287970542907715\n",
      "Optimizing model...\n",
      "Step: 15, Action: 1, Reward: 0.0, Total Reward: -21.650000000000002, Loss: 1.0049405097961426\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 16, Action: 4, Reward: 0.2, Total Reward: -21.450000000000003, Loss: 4.628898620605469\n",
      "Optimizing model...\n",
      "Step: 17, Action: 2, Reward: 0.25, Total Reward: -21.200000000000003, Loss: 4.574661731719971\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 18, Action: 4, Reward: 0.2, Total Reward: -21.000000000000004, Loss: 2.293424129486084\n",
      "Optimizing model...\n",
      "Step: 19, Action: 1, Reward: 0.0, Total Reward: -21.000000000000004, Loss: 9.644193649291992\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 20, Action: 4, Reward: 0.2, Total Reward: -20.800000000000004, Loss: 6.308309555053711\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 21, Action: 0, Reward: 2.0, Total Reward: -18.800000000000004, Loss: 4.378801345825195\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 22, Action: 0, Reward: 2.0, Total Reward: -16.800000000000004, Loss: 2.838632583618164\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 23, Action: 4, Reward: 0.2, Total Reward: -16.600000000000005, Loss: 7.969289779663086\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 24, Action: 4, Reward: 0.2, Total Reward: -16.400000000000006, Loss: 6.275256633758545\n",
      "Optimizing model...\n",
      "Step: 25, Action: 1, Reward: 0.1, Total Reward: -16.300000000000004, Loss: 5.54160213470459\n",
      "Optimizing model...\n",
      "Step: 26, Action: 3, Reward: 0.25, Total Reward: -16.050000000000004, Loss: 5.510526657104492\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 27, Action: 4, Reward: 0.2, Total Reward: -15.850000000000005, Loss: 2.3221542835235596\n",
      "Optimizing model...\n",
      "Step: 28, Action: 4, Reward: -9.8, Total Reward: -25.650000000000006, Loss: 4.444769859313965\n",
      "Optimizing model...\n",
      "Step: 29, Action: 1, Reward: 0.0, Total Reward: -25.650000000000006, Loss: 8.86154556274414\n",
      "Model acting\n",
      "\n",
      "episode: 141, reward: -125.65\n",
      "\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 2, Reward: 0.25, Total Reward: 0.25, Loss: 5.635396480560303\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 1, Action: 2, Reward: 0.25, Total Reward: 0.5, Loss: 2.3944060802459717\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: 2.0, Total Reward: 2.5, Loss: 1.10263991355896\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: 2.0, Total Reward: 4.5, Loss: 3.7266483306884766\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: 2.0, Total Reward: 6.5, Loss: 2.2888498306274414\n",
      "Optimizing model...\n",
      "Step: 5, Action: 3, Reward: 0.25, Total Reward: 6.75, Loss: 4.355628490447998\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 6, Action: 2, Reward: 0.25, Total Reward: 7.0, Loss: 3.934061288833618\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 7, Action: 2, Reward: -9.75, Total Reward: -2.75, Loss: 2.7370238304138184\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 8, Action: 2, Reward: -9.75, Total Reward: -12.5, Loss: 5.403995513916016\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 9, Action: 2, Reward: -9.75, Total Reward: -22.25, Loss: 7.1975483894348145\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 10, Action: 2, Reward: -9.75, Total Reward: -32.0, Loss: 3.993699073791504\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 11, Action: 0, Reward: 2.0, Total Reward: -30.0, Loss: 6.268701553344727\n",
      "Optimizing model...\n",
      "Step: 12, Action: 3, Reward: 0.25, Total Reward: -29.75, Loss: 5.246454238891602\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 13, Action: 0, Reward: 2.0, Total Reward: -27.75, Loss: 4.0987749099731445\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 14, Action: 0, Reward: -8.0, Total Reward: -35.75, Loss: 4.957592010498047\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 15, Action: 0, Reward: -8.0, Total Reward: -43.75, Loss: 3.537036657333374\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 16, Action: 0, Reward: 2.0, Total Reward: -41.75, Loss: 8.609560012817383\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 17, Action: 0, Reward: 2.0, Total Reward: -39.75, Loss: 7.8214521408081055\n",
      "Optimizing model...\n",
      "Step: 18, Action: 3, Reward: 0.35, Total Reward: -39.4, Loss: 2.6041126251220703\n",
      "Optimizing model...\n",
      "Step: 19, Action: 2, Reward: 0.25, Total Reward: -39.15, Loss: 8.509747505187988\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 20, Action: 0, Reward: 2.0, Total Reward: -37.15, Loss: 2.7199084758758545\n",
      "Model acting\n",
      "\n",
      "episode: 142, reward: -137.15\n",
      "\n",
      "Optimizing model...\n",
      "Step: 0, Action: 1, Reward: 0.0, Total Reward: 0.0, Loss: 2.713341236114502\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: 2.0, Total Reward: 2.0, Loss: 2.3797550201416016\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: -8.0, Total Reward: -6.0, Loss: 4.11762809753418\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: 2.0, Total Reward: -4.0, Loss: 5.0243377685546875\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: 2.0, Total Reward: -2.0, Loss: 8.647320747375488\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 5, Action: 0, Reward: 2.0, Total Reward: 0.0, Loss: 5.339747428894043\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 6, Action: 0, Reward: -8.0, Total Reward: -8.0, Loss: 2.399024486541748\n",
      "Optimizing model...\n",
      "Step: 7, Action: 4, Reward: 0.2, Total Reward: -7.8, Loss: 7.407917022705078\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 8, Action: 0, Reward: 2.0, Total Reward: -5.8, Loss: 8.980493545532227\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 9, Action: 0, Reward: 2.0, Total Reward: -3.8, Loss: 2.522368907928467\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 10, Action: 0, Reward: -8.0, Total Reward: -11.8, Loss: 6.195775985717773\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 11, Action: 0, Reward: 2.0, Total Reward: -9.8, Loss: 3.880112886428833\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 12, Action: 0, Reward: -8.0, Total Reward: -17.8, Loss: 7.702951431274414\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 13, Action: 0, Reward: 2.1, Total Reward: -15.700000000000001, Loss: 2.847414493560791\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 14, Action: 0, Reward: 2.0, Total Reward: -13.700000000000001, Loss: 5.478851795196533\n",
      "Optimizing model...\n",
      "Step: 15, Action: 3, Reward: 0.25, Total Reward: -13.450000000000001, Loss: 5.468992233276367\n",
      "Optimizing model...\n",
      "Step: 16, Action: 1, Reward: 0.0, Total Reward: -13.450000000000001, Loss: 7.887846946716309\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 17, Action: 0, Reward: 2.0, Total Reward: -11.450000000000001, Loss: 2.607908248901367\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 18, Action: 0, Reward: -8.0, Total Reward: -19.450000000000003, Loss: 5.628383159637451\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 19, Action: 0, Reward: -8.0, Total Reward: -27.450000000000003, Loss: 5.657586097717285\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 20, Action: 0, Reward: 2.0, Total Reward: -25.450000000000003, Loss: 8.379518508911133\n",
      "Model acting\n",
      "\n",
      "episode: 143, reward: -125.45\n",
      "\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: -8.0, Total Reward: -8.0, Loss: 4.181787490844727\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: 2.0, Total Reward: -6.0, Loss: 1.474846601486206\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: 2.0, Total Reward: -4.0, Loss: 5.4835968017578125\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: 2.0, Total Reward: -2.0, Loss: 4.4939284324646\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: -8.0, Total Reward: -10.0, Loss: 4.786870002746582\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 5, Action: 0, Reward: 2.0, Total Reward: -8.0, Loss: 3.9283246994018555\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 6, Action: 0, Reward: 2.0, Total Reward: -6.0, Loss: 3.9619951248168945\n",
      "Optimizing model...\n",
      "Step: 7, Action: 0, Reward: 2.0, Total Reward: -4.0, Loss: 0.9765743017196655\n",
      "Optimizing model...\n",
      "Step: 8, Action: 3, Reward: 0.25, Total Reward: -3.75, Loss: 4.978177547454834\n",
      "Model acting\n",
      "\n",
      "episode: 144, reward: -103.75\n",
      "\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: -8.0, Total Reward: -8.0, Loss: 6.068563461303711\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: 2.0, Total Reward: -6.0, Loss: 8.515983581542969\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: 2.0, Total Reward: -4.0, Loss: 7.510515213012695\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: 2.0, Total Reward: -2.0, Loss: 1.5816912651062012\n",
      "Optimizing model...\n",
      "Step: 4, Action: 1, Reward: 0.0, Total Reward: -2.0, Loss: 7.658017635345459\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 5, Action: 4, Reward: 0.2, Total Reward: -1.8, Loss: 6.987857818603516\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 6, Action: 0, Reward: 2.0, Total Reward: 0.19999999999999996, Loss: 8.379646301269531\n",
      "Optimizing model...\n",
      "Step: 7, Action: 2, Reward: 0.25, Total Reward: 0.44999999999999996, Loss: 2.4352223873138428\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 8, Action: 0, Reward: 2.0, Total Reward: 2.45, Loss: 2.7789740562438965\n",
      "Optimizing model...\n",
      "Step: 9, Action: 2, Reward: 0.25, Total Reward: 2.7, Loss: 2.651576042175293\n",
      "Optimizing model...\n",
      "Step: 10, Action: 0, Reward: 2.0, Total Reward: 4.7, Loss: 2.6306686401367188\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 11, Action: 0, Reward: 2.0, Total Reward: 6.7, Loss: 2.6693685054779053\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 12, Action: 0, Reward: 2.0, Total Reward: 8.7, Loss: 4.567084789276123\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 13, Action: 0, Reward: 2.0, Total Reward: 10.7, Loss: 7.309220314025879\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 14, Action: 0, Reward: 2.0, Total Reward: 12.7, Loss: 4.065641403198242\n",
      "Model acting\n",
      "\n",
      "episode: 145, reward: -87.3\n",
      "\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: -8.0, Total Reward: -8.0, Loss: 8.692034721374512\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: 2.0, Total Reward: -6.0, Loss: 4.527181148529053\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: 2.0, Total Reward: -4.0, Loss: 2.391263246536255\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: 2.0, Total Reward: -2.0, Loss: 5.049499034881592\n",
      "Model acting\n",
      "\n",
      "episode: 146, reward: -102.0\n",
      "\n",
      "Optimizing model...\n",
      "Step: 0, Action: 1, Reward: 0.0, Total Reward: 0.0, Loss: 4.626595497131348\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: 2.0, Total Reward: 2.0, Loss: 9.781728744506836\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: -8.0, Total Reward: -6.0, Loss: 6.262075901031494\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: 2.0, Total Reward: -4.0, Loss: 2.3226184844970703\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: 2.0, Total Reward: -2.0, Loss: 3.7081875801086426\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 5, Action: 0, Reward: -8.0, Total Reward: -10.0, Loss: 5.7016801834106445\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 6, Action: 0, Reward: 2.0, Total Reward: -8.0, Loss: 3.9352476596832275\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 7, Action: 0, Reward: 2.0, Total Reward: -6.0, Loss: 7.083067893981934\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 8, Action: 0, Reward: 2.0, Total Reward: -4.0, Loss: 7.124258041381836\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 9, Action: 0, Reward: -8.0, Total Reward: -12.0, Loss: 3.030409812927246\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 10, Action: 0, Reward: 2.0, Total Reward: -10.0, Loss: 1.6216416358947754\n",
      "\n",
      "episode: 147, reward: -110.0\n",
      "\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: -8.0, Total Reward: -8.0, Loss: 2.206221103668213\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: 2.0, Total Reward: -6.0, Loss: 1.00044846534729\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: 2.0, Total Reward: -4.0, Loss: 3.7758593559265137\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: 2.0, Total Reward: -2.0, Loss: 5.9633588790893555\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: 2.0, Total Reward: 0.0, Loss: 1.0859436988830566\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 5, Action: 0, Reward: 2.0, Total Reward: 2.0, Loss: 4.481845855712891\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 6, Action: 0, Reward: 2.0, Total Reward: 4.0, Loss: 4.478916168212891\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 7, Action: 0, Reward: 2.0, Total Reward: 6.0, Loss: 8.282541275024414\n",
      "Optimizing model...\n",
      "Step: 8, Action: 1, Reward: 0.0, Total Reward: 6.0, Loss: 6.143682479858398\n",
      "Model acting\n",
      "\n",
      "episode: 148, reward: -94.0\n",
      "\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: 2.0, Total Reward: 2.0, Loss: 5.250981330871582\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: -8.0, Total Reward: -6.0, Loss: 4.470040321350098\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: -8.0, Total Reward: -14.0, Loss: 2.313274383544922\n",
      "Optimizing model...\n",
      "Step: 3, Action: 1, Reward: 0.0, Total Reward: -14.0, Loss: 8.467000961303711\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: 2.0, Total Reward: -12.0, Loss: 7.410743236541748\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 5, Action: 0, Reward: -8.0, Total Reward: -20.0, Loss: 6.550187587738037\n",
      "Optimizing model...\n",
      "Step: 6, Action: 4, Reward: 0.2, Total Reward: -19.8, Loss: 5.876314163208008\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 7, Action: 0, Reward: 2.0, Total Reward: -17.8, Loss: 5.231921195983887\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 8, Action: 0, Reward: 2.0, Total Reward: -15.8, Loss: 1.319205641746521\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 9, Action: 0, Reward: 2.0, Total Reward: -13.8, Loss: 5.503273963928223\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 10, Action: 0, Reward: 2.0, Total Reward: -11.8, Loss: 1.1151230335235596\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 11, Action: 0, Reward: -8.0, Total Reward: -19.8, Loss: 2.367095947265625\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 12, Action: 0, Reward: 2.0, Total Reward: -17.8, Loss: 0.6639754772186279\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 13, Action: 0, Reward: 2.0, Total Reward: -15.8, Loss: 1.0174208879470825\n",
      "Optimizing model...\n",
      "Step: 14, Action: 4, Reward: 0.2, Total Reward: -15.600000000000001, Loss: 5.802630424499512\n",
      "Optimizing model...\n",
      "Step: 15, Action: 4, Reward: -9.8, Total Reward: -25.400000000000002, Loss: 5.053731918334961\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 16, Action: 0, Reward: 2.1, Total Reward: -23.3, Loss: 4.453372001647949\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 17, Action: 0, Reward: -8.0, Total Reward: -31.3, Loss: 0.7075948119163513\n",
      "Optimizing model...\n",
      "Step: 18, Action: 1, Reward: 0.0, Total Reward: -31.3, Loss: 2.3258509635925293\n",
      "Model acting\n",
      "\n",
      "episode: 149, reward: -131.3\n",
      "\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: 2.0, Total Reward: 2.0, Loss: 5.474865913391113\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: 2.0, Total Reward: 4.0, Loss: 4.244807720184326\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: 2.0, Total Reward: 6.0, Loss: 4.641948223114014\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: 2.0, Total Reward: 8.0, Loss: 4.739234924316406\n",
      "Model acting\n",
      "\n",
      "episode: 150, reward: -92.0\n",
      "\n",
      "Model saved\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: 2.0, Total Reward: 2.0, Loss: 2.663567543029785\n",
      "Optimizing model...\n",
      "Step: 1, Action: 3, Reward: 0.25, Total Reward: 2.25, Loss: 2.5221774578094482\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: 2.0, Total Reward: 4.25, Loss: 4.5445356369018555\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: -8.0, Total Reward: -3.75, Loss: 3.624591588973999\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: 2.0, Total Reward: -1.75, Loss: 7.3941264152526855\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 5, Action: 0, Reward: 2.0, Total Reward: 0.25, Loss: 2.778714179992676\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 6, Action: 0, Reward: 2.0, Total Reward: 2.25, Loss: 7.428699493408203\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 7, Action: 0, Reward: 2.0, Total Reward: 4.25, Loss: 5.872310161590576\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 8, Action: 0, Reward: 2.0, Total Reward: 6.25, Loss: 2.5417537689208984\n",
      "Model acting\n",
      "\n",
      "episode: 151, reward: -93.75\n",
      "\n",
      "Optimizing model...\n",
      "Step: 0, Action: 4, Reward: 0.2, Total Reward: 0.2, Loss: 2.647400379180908\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: 2.0, Total Reward: 2.2, Loss: 7.471133232116699\n",
      "Optimizing model...\n",
      "Step: 2, Action: 1, Reward: 0.0, Total Reward: 2.2, Loss: 5.483397483825684\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: 2.0, Total Reward: 4.2, Loss: 4.011334419250488\n",
      "Optimizing model...\n",
      "Step: 4, Action: 2, Reward: 0.25, Total Reward: 4.45, Loss: 2.3314266204833984\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 5, Action: 0, Reward: 2.1, Total Reward: 6.550000000000001, Loss: 6.342775344848633\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 6, Action: 0, Reward: -8.0, Total Reward: -1.4499999999999993, Loss: 5.953235626220703\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 7, Action: 0, Reward: 2.0, Total Reward: 0.5500000000000007, Loss: 2.658902645111084\n",
      "Model acting\n",
      "\n",
      "episode: 152, reward: -99.45\n",
      "\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: 2.0, Total Reward: 2.0, Loss: 9.704803466796875\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: -8.0, Total Reward: -6.0, Loss: 11.53587532043457\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: 2.0, Total Reward: -4.0, Loss: 3.8162035942077637\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: -8.0, Total Reward: -12.0, Loss: 6.029335975646973\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: 2.0, Total Reward: -10.0, Loss: 5.771642208099365\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 5, Action: 0, Reward: 2.0, Total Reward: -8.0, Loss: 4.205857753753662\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 6, Action: 0, Reward: -8.0, Total Reward: -16.0, Loss: 2.3552780151367188\n",
      "Model acting\n",
      "\n",
      "episode: 153, reward: -116.0\n",
      "\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: -7.9, Total Reward: -7.9, Loss: 4.132129669189453\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: 2.0, Total Reward: -5.9, Loss: 4.036012649536133\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: 2.0, Total Reward: -3.9000000000000004, Loss: 1.676431655883789\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 1, Reward: 0.0, Total Reward: -3.9000000000000004, Loss: 4.113426685333252\n",
      "Model acting\n",
      "\n",
      "episode: 154, reward: -103.9\n",
      "\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: 2.0, Total Reward: 2.0, Loss: 2.441666841506958\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: -8.0, Total Reward: -6.0, Loss: 4.123241424560547\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: -8.0, Total Reward: -14.0, Loss: 5.2647786140441895\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: 2.0, Total Reward: -12.0, Loss: 5.489008903503418\n",
      "Optimizing model...\n",
      "Step: 4, Action: 2, Reward: 0.25, Total Reward: -11.75, Loss: 3.9164113998413086\n",
      "Optimizing model...\n",
      "Step: 5, Action: 4, Reward: 0.3, Total Reward: -11.45, Loss: 2.445821762084961\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 6, Action: 0, Reward: 2.0, Total Reward: -9.45, Loss: 0.953760027885437\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 7, Action: 0, Reward: -8.0, Total Reward: -17.45, Loss: 4.169637203216553\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 8, Action: 0, Reward: 2.0, Total Reward: -15.45, Loss: 6.997702598571777\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 9, Action: 0, Reward: 2.0, Total Reward: -13.45, Loss: 4.344888210296631\n",
      "Model acting\n",
      "\n",
      "episode: 155, reward: -113.45\n",
      "\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: 2.0, Total Reward: 2.0, Loss: 5.710409164428711\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: -8.0, Total Reward: -6.0, Loss: 5.856231212615967\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: -8.0, Total Reward: -14.0, Loss: 5.813658237457275\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: 2.0, Total Reward: -12.0, Loss: 1.103198528289795\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: -8.0, Total Reward: -20.0, Loss: 2.1883201599121094\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 5, Action: 0, Reward: 2.0, Total Reward: -18.0, Loss: 7.795933246612549\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 6, Action: 0, Reward: 2.0, Total Reward: -16.0, Loss: 5.435877799987793\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 7, Action: 0, Reward: -8.0, Total Reward: -24.0, Loss: 0.997343897819519\n",
      "Model acting\n",
      "\n",
      "episode: 156, reward: -124.0\n",
      "\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: 2.0, Total Reward: 2.0, Loss: 2.474405288696289\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: -8.0, Total Reward: -6.0, Loss: 4.327284812927246\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: 2.0, Total Reward: -4.0, Loss: 2.506497383117676\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: -8.0, Total Reward: -12.0, Loss: 4.27381706237793\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: 2.0, Total Reward: -10.0, Loss: 5.70242166519165\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 5, Action: 0, Reward: -8.0, Total Reward: -18.0, Loss: 7.033219337463379\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 6, Action: 0, Reward: -8.0, Total Reward: -26.0, Loss: 2.447648048400879\n",
      "\n",
      "episode: 157, reward: -126.0\n",
      "\n",
      "Optimizing model...\n",
      "Step: 0, Action: 4, Reward: 0.2, Total Reward: 0.2, Loss: 5.5259599685668945\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: 2.0, Total Reward: 2.2, Loss: 2.605607509613037\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: -8.0, Total Reward: -5.8, Loss: 7.306685447692871\n",
      "Optimizing model...\n",
      "Step: 3, Action: 3, Reward: 0.25, Total Reward: -5.55, Loss: 9.798744201660156\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: 2.0, Total Reward: -3.55, Loss: 4.2096991539001465\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 5, Action: 0, Reward: 2.0, Total Reward: -1.5499999999999998, Loss: 2.8079309463500977\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 6, Action: 0, Reward: 2.0, Total Reward: 0.4500000000000002, Loss: 6.327704906463623\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 7, Action: 0, Reward: -8.0, Total Reward: -7.55, Loss: 3.1688804626464844\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 8, Action: 0, Reward: 2.0, Total Reward: -5.55, Loss: 2.8303613662719727\n",
      "Optimizing model...\n",
      "Step: 9, Action: 2, Reward: 0.25, Total Reward: -5.3, Loss: 6.0823259353637695\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 10, Action: 0, Reward: 2.0, Total Reward: -3.3, Loss: 3.8141355514526367\n",
      "Optimizing model...\n",
      "Step: 11, Action: 1, Reward: 0.0, Total Reward: -3.3, Loss: 1.0600273609161377\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 12, Action: 0, Reward: 2.0, Total Reward: -1.2999999999999998, Loss: 4.127542018890381\n",
      "Optimizing model...\n",
      "Step: 13, Action: 1, Reward: 0.0, Total Reward: -1.2999999999999998, Loss: 2.838742256164551\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 14, Action: 0, Reward: 2.0, Total Reward: 0.7000000000000002, Loss: 7.132200717926025\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 15, Action: 0, Reward: 2.0, Total Reward: 2.7, Loss: 7.139474868774414\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 16, Action: 0, Reward: -8.0, Total Reward: -5.3, Loss: 3.719525098800659\n",
      "Model acting\n",
      "\n",
      "episode: 158, reward: -105.3\n",
      "\n",
      "Optimizing model...\n",
      "Step: 0, Action: 1, Reward: 0.0, Total Reward: 0.0, Loss: 2.597984790802002\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: 2.0, Total Reward: 2.0, Loss: 2.7782187461853027\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: 2.0, Total Reward: 4.0, Loss: 6.969329833984375\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 4, Reward: 0.2, Total Reward: 4.2, Loss: 11.096134185791016\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: 2.0, Total Reward: 6.2, Loss: 4.143509864807129\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 5, Action: 4, Reward: 0.2, Total Reward: 6.4, Loss: 3.777130365371704\n",
      "Optimizing model...\n",
      "Step: 6, Action: 2, Reward: 0.25, Total Reward: 6.65, Loss: 3.928945779800415\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 7, Action: 4, Reward: 0.2, Total Reward: 6.8500000000000005, Loss: 2.936149835586548\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 8, Action: 4, Reward: -9.8, Total Reward: -2.95, Loss: 4.262556076049805\n",
      "Optimizing model...\n",
      "Step: 9, Action: 2, Reward: 0.25, Total Reward: -2.7, Loss: 2.573678731918335\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 10, Action: 4, Reward: 0.2, Total Reward: -2.5, Loss: 2.542748212814331\n",
      "Optimizing model...\n",
      "Step: 11, Action: 0, Reward: 2.0, Total Reward: -0.5, Loss: 4.584141731262207\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 12, Action: 4, Reward: 0.2, Total Reward: -0.3, Loss: 5.9524054527282715\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 13, Action: 4, Reward: -9.8, Total Reward: -10.100000000000001, Loss: 7.160346508026123\n",
      "Optimizing model...\n",
      "Step: 14, Action: 1, Reward: 0.0, Total Reward: -10.100000000000001, Loss: 4.176956653594971\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 15, Action: 0, Reward: 2.0, Total Reward: -8.100000000000001, Loss: 7.470030307769775\n",
      "Optimizing model...\n",
      "Step: 16, Action: 4, Reward: 0.2, Total Reward: -7.900000000000001, Loss: 8.359053611755371\n",
      "Model acting\n",
      "\n",
      "episode: 159, reward: -107.9\n",
      "\n",
      "Optimizing model...\n",
      "Step: 0, Action: 1, Reward: 0.0, Total Reward: 0.0, Loss: 7.128214359283447\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: 2.0, Total Reward: 2.0, Loss: 6.111419677734375\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 3, Reward: 0.35, Total Reward: 2.35, Loss: 7.382351398468018\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: 2.0, Total Reward: 4.35, Loss: 9.568159103393555\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: 2.0, Total Reward: 6.35, Loss: 2.5750417709350586\n",
      "Optimizing model...\n",
      "Step: 5, Action: 0, Reward: -8.0, Total Reward: -1.6500000000000004, Loss: 2.9401984214782715\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 6, Action: 3, Reward: 0.25, Total Reward: -1.4000000000000004, Loss: 5.839722633361816\n",
      "\n",
      "episode: 160, reward: -101.4\n",
      "\n",
      "Model saved\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 3, Reward: 0.25, Total Reward: 0.25, Loss: 4.592267036437988\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 1, Action: 3, Reward: -9.75, Total Reward: -9.5, Loss: 2.2516024112701416\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 3, Reward: -9.65, Total Reward: -19.15, Loss: 9.629642486572266\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 3, Reward: -9.65, Total Reward: -28.799999999999997, Loss: 2.8506174087524414\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 4, Action: 3, Reward: -9.75, Total Reward: -38.55, Loss: 4.505173683166504\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 5, Action: 2, Reward: 0.25, Total Reward: -38.3, Loss: 1.16556978225708\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 6, Action: 3, Reward: 0.35, Total Reward: -37.949999999999996, Loss: 4.098896026611328\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 7, Action: 3, Reward: -9.75, Total Reward: -47.699999999999996, Loss: 10.187395095825195\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 8, Action: 2, Reward: 0.25, Total Reward: -47.449999999999996, Loss: 6.212175369262695\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 9, Action: 2, Reward: -9.75, Total Reward: -57.199999999999996, Loss: 4.022287845611572\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 10, Action: 2, Reward: -9.75, Total Reward: -66.94999999999999, Loss: 5.739387512207031\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 11, Action: 2, Reward: -9.75, Total Reward: -76.69999999999999, Loss: 7.732720851898193\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 12, Action: 2, Reward: -9.75, Total Reward: -86.44999999999999, Loss: 4.091455936431885\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 13, Action: 2, Reward: -9.75, Total Reward: -96.19999999999999, Loss: 9.278726577758789\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 14, Action: 2, Reward: -9.75, Total Reward: -105.94999999999999, Loss: 4.652379035949707\n",
      "Optimizing model...\n",
      "Step: 15, Action: 0, Reward: 2.0, Total Reward: -103.94999999999999, Loss: 7.858260154724121\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 16, Action: 2, Reward: 0.25, Total Reward: -103.69999999999999, Loss: 0.8260900974273682\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 17, Action: 2, Reward: 0.25, Total Reward: -103.44999999999999, Loss: 3.1396660804748535\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 18, Action: 2, Reward: -9.75, Total Reward: -113.19999999999999, Loss: 5.869240760803223\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 19, Action: 2, Reward: -9.65, Total Reward: -122.85, Loss: 5.72494649887085\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 20, Action: 2, Reward: 0.25, Total Reward: -122.6, Loss: 5.90903377532959\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 21, Action: 2, Reward: 0.25, Total Reward: -122.35, Loss: 7.908928871154785\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 22, Action: 2, Reward: -9.75, Total Reward: -132.1, Loss: 4.234882354736328\n",
      "Optimizing model...\n",
      "Step: 23, Action: 1, Reward: 0.0, Total Reward: -132.1, Loss: 2.073289394378662\n",
      "Optimizing model...\n",
      "Step: 24, Action: 4, Reward: 0.2, Total Reward: -131.9, Loss: 4.323193073272705\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 25, Action: 2, Reward: 0.25, Total Reward: -131.65, Loss: 3.0661861896514893\n",
      "Optimizing model...\n",
      "Step: 26, Action: 0, Reward: 2.0, Total Reward: -129.65, Loss: 5.39388370513916\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 27, Action: 2, Reward: 0.25, Total Reward: -129.4, Loss: 4.071222305297852\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 28, Action: 0, Reward: 2.0, Total Reward: -127.4, Loss: 3.9765353202819824\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 29, Action: 2, Reward: 0.25, Total Reward: -127.15, Loss: 1.278916358947754\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 30, Action: 2, Reward: 0.25, Total Reward: -126.9, Loss: 4.099220275878906\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 31, Action: 2, Reward: 0.25, Total Reward: -126.65, Loss: 2.7304153442382812\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 32, Action: 0, Reward: 2.0, Total Reward: -124.65, Loss: 4.567157745361328\n",
      "Optimizing model...\n",
      "Step: 33, Action: 4, Reward: 0.2, Total Reward: -124.45, Loss: 2.6745541095733643\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 34, Action: 2, Reward: 0.25, Total Reward: -124.2, Loss: 3.971497058868408\n",
      "\n",
      "episode: 161, reward: -224.2\n",
      "\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: -8.0, Total Reward: -8.0, Loss: 5.236780643463135\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: 2.0, Total Reward: -6.0, Loss: 6.984661102294922\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: 2.0, Total Reward: -4.0, Loss: 1.2298543453216553\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: 2.0, Total Reward: -2.0, Loss: 2.550752639770508\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: 2.0, Total Reward: 0.0, Loss: 2.3445026874542236\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 5, Action: 0, Reward: -8.0, Total Reward: -8.0, Loss: 4.842256546020508\n",
      "Model acting\n",
      "\n",
      "episode: 162, reward: -108.0\n",
      "\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: -8.0, Total Reward: -8.0, Loss: 4.289752006530762\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: -8.0, Total Reward: -16.0, Loss: 4.715968132019043\n",
      "Optimizing model...\n",
      "Step: 2, Action: 2, Reward: 0.25, Total Reward: -15.75, Loss: 7.488156795501709\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: 2.0, Total Reward: -13.75, Loss: 2.252291202545166\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: -8.0, Total Reward: -21.75, Loss: 7.697864532470703\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 5, Action: 0, Reward: 2.1, Total Reward: -19.65, Loss: 8.47122573852539\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 6, Action: 0, Reward: 2.0, Total Reward: -17.65, Loss: 3.2323880195617676\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 7, Action: 0, Reward: -8.0, Total Reward: -25.65, Loss: 2.58209490776062\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 8, Action: 0, Reward: -8.0, Total Reward: -33.65, Loss: 1.5098706483840942\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 9, Action: 0, Reward: -8.0, Total Reward: -41.65, Loss: 11.093652725219727\n",
      "Optimizing model...\n",
      "Step: 10, Action: 4, Reward: 0.2, Total Reward: -41.449999999999996, Loss: 7.394113540649414\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 11, Action: 0, Reward: 2.0, Total Reward: -39.449999999999996, Loss: 2.315155029296875\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 12, Action: 0, Reward: 2.0, Total Reward: -37.449999999999996, Loss: 4.146099090576172\n",
      "Optimizing model...\n",
      "Step: 13, Action: 4, Reward: 0.3, Total Reward: -37.15, Loss: 8.040037155151367\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 14, Action: 0, Reward: 2.1, Total Reward: -35.05, Loss: 2.936910390853882\n",
      "Optimizing model...\n",
      "Step: 15, Action: 3, Reward: 0.25, Total Reward: -34.8, Loss: 7.287513732910156\n",
      "\n",
      "episode: 163, reward: -134.8\n",
      "\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: -8.0, Total Reward: -8.0, Loss: 0.877152681350708\n",
      "Optimizing model...\n",
      "Step: 1, Action: 3, Reward: 0.25, Total Reward: -7.75, Loss: 2.741894006729126\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: 2.0, Total Reward: -5.75, Loss: 6.0636796951293945\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: -8.0, Total Reward: -13.75, Loss: 6.75653076171875\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: -7.9, Total Reward: -21.65, Loss: 4.470361709594727\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 5, Action: 0, Reward: -8.0, Total Reward: -29.65, Loss: 1.179932951927185\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 6, Action: 0, Reward: -8.0, Total Reward: -37.65, Loss: 3.522130012512207\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 7, Action: 0, Reward: 2.0, Total Reward: -35.65, Loss: 6.774178504943848\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 8, Action: 0, Reward: -8.0, Total Reward: -43.65, Loss: 4.165322303771973\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 9, Action: 0, Reward: -8.0, Total Reward: -51.65, Loss: 4.3980512619018555\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 10, Action: 0, Reward: 2.0, Total Reward: -49.65, Loss: 7.35273551940918\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 11, Action: 0, Reward: 2.0, Total Reward: -47.65, Loss: 4.299282550811768\n",
      "Optimizing model...\n",
      "Step: 12, Action: 1, Reward: 0.0, Total Reward: -47.65, Loss: 3.196625232696533\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 13, Action: 0, Reward: 2.0, Total Reward: -45.65, Loss: 5.862496376037598\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 14, Action: 0, Reward: -8.0, Total Reward: -53.65, Loss: 7.14210319519043\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 15, Action: 0, Reward: -8.0, Total Reward: -61.65, Loss: 2.5282788276672363\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 16, Action: 0, Reward: 2.0, Total Reward: -59.65, Loss: 4.568870544433594\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 17, Action: 0, Reward: 2.0, Total Reward: -57.65, Loss: 3.345053195953369\n",
      "Optimizing model...\n",
      "Step: 18, Action: 0, Reward: -8.0, Total Reward: -65.65, Loss: 6.167413711547852\n",
      "Optimizing model...\n",
      "Step: 19, Action: 2, Reward: 0.25, Total Reward: -65.4, Loss: 2.8512067794799805\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 20, Action: 0, Reward: 2.0, Total Reward: -63.400000000000006, Loss: 2.4049339294433594\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 21, Action: 0, Reward: 2.0, Total Reward: -61.400000000000006, Loss: 3.8173136711120605\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 22, Action: 0, Reward: 2.0, Total Reward: -59.400000000000006, Loss: 5.79014253616333\n",
      "Optimizing model...\n",
      "Step: 23, Action: 4, Reward: 0.2, Total Reward: -59.2, Loss: 5.468951225280762\n",
      "Model acting\n",
      "\n",
      "episode: 164, reward: -159.2\n",
      "\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: -8.0, Total Reward: -8.0, Loss: 2.1634504795074463\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: 2.0, Total Reward: -6.0, Loss: 3.0741589069366455\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: 2.0, Total Reward: -4.0, Loss: 2.797426700592041\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: 2.0, Total Reward: -2.0, Loss: 3.8839406967163086\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: 2.0, Total Reward: 0.0, Loss: 4.084798336029053\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 5, Action: 0, Reward: -8.0, Total Reward: -8.0, Loss: 6.885772228240967\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 6, Action: 0, Reward: 2.0, Total Reward: -6.0, Loss: 1.7598729133605957\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 7, Action: 0, Reward: 2.0, Total Reward: -4.0, Loss: 2.7682747840881348\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 8, Action: 0, Reward: 2.0, Total Reward: -2.0, Loss: 5.250993728637695\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 9, Action: 0, Reward: 2.0, Total Reward: 0.0, Loss: 1.2878961563110352\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 10, Action: 0, Reward: 2.0, Total Reward: 2.0, Loss: 0.5932408571243286\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 11, Action: 0, Reward: -8.0, Total Reward: -6.0, Loss: 1.3875195980072021\n",
      "Model acting\n",
      "\n",
      "episode: 165, reward: -106.0\n",
      "\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: -8.0, Total Reward: -8.0, Loss: 2.8661885261535645\n",
      "Optimizing model...\n",
      "Step: 1, Action: 2, Reward: 0.25, Total Reward: -7.75, Loss: 2.338329792022705\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: 2.0, Total Reward: -5.75, Loss: 4.252950668334961\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: 2.0, Total Reward: -3.75, Loss: 0.7273503541946411\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: 2.0, Total Reward: -1.75, Loss: 4.175481796264648\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 5, Action: 0, Reward: -8.0, Total Reward: -9.75, Loss: 5.878046035766602\n",
      "Optimizing model...\n",
      "Step: 6, Action: 1, Reward: 0.0, Total Reward: -9.75, Loss: 3.021026134490967\n",
      "Optimizing model...\n",
      "Step: 7, Action: 2, Reward: 0.25, Total Reward: -9.5, Loss: 2.674765110015869\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 8, Action: 0, Reward: 2.0, Total Reward: -7.5, Loss: 3.082547187805176\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 9, Action: 0, Reward: -8.0, Total Reward: -15.5, Loss: 2.9001431465148926\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 10, Action: 0, Reward: -8.0, Total Reward: -23.5, Loss: 2.869621992111206\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 11, Action: 0, Reward: 2.0, Total Reward: -21.5, Loss: 7.138367652893066\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 12, Action: 0, Reward: -8.0, Total Reward: -29.5, Loss: 4.116472244262695\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 13, Action: 0, Reward: 2.0, Total Reward: -27.5, Loss: 6.874270439147949\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 14, Action: 2, Reward: 0.25, Total Reward: -27.25, Loss: 5.652825355529785\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 15, Action: 0, Reward: 2.1, Total Reward: -25.15, Loss: 5.42289924621582\n",
      "Model acting\n",
      "\n",
      "episode: 166, reward: -125.15\n",
      "\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: -8.0, Total Reward: -8.0, Loss: 0.6998941898345947\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: 2.0, Total Reward: -6.0, Loss: 5.8774824142456055\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: -8.0, Total Reward: -14.0, Loss: 1.1709777116775513\n",
      "Optimizing model...\n",
      "Step: 3, Action: 4, Reward: 0.2, Total Reward: -13.8, Loss: 5.55980920791626\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: 2.1, Total Reward: -11.700000000000001, Loss: 1.3684333562850952\n",
      "Optimizing model...\n",
      "Step: 5, Action: 0, Reward: 2.0, Total Reward: -9.700000000000001, Loss: 5.910965919494629\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 6, Action: 2, Reward: 0.25, Total Reward: -9.450000000000001, Loss: 4.323883056640625\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 7, Action: 0, Reward: 2.0, Total Reward: -7.450000000000001, Loss: 7.249370098114014\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 8, Action: 0, Reward: -8.0, Total Reward: -15.450000000000001, Loss: 12.043201446533203\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 9, Action: 0, Reward: 2.0, Total Reward: -13.450000000000001, Loss: 2.4029297828674316\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 10, Action: 0, Reward: 2.0, Total Reward: -11.450000000000001, Loss: 5.831430912017822\n",
      "\n",
      "episode: 167, reward: -111.45\n",
      "\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: 2.0, Total Reward: 2.0, Loss: 5.852205753326416\n",
      "Optimizing model...\n",
      "Step: 1, Action: 1, Reward: 0.0, Total Reward: 2.0, Loss: 0.8419190645217896\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: 2.0, Total Reward: 4.0, Loss: 7.4028730392456055\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: -8.0, Total Reward: -4.0, Loss: 3.1311867237091064\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: -8.0, Total Reward: -12.0, Loss: 5.324467182159424\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 5, Action: 0, Reward: -8.0, Total Reward: -20.0, Loss: 7.133668422698975\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 6, Action: 0, Reward: 2.0, Total Reward: -18.0, Loss: 5.739513397216797\n",
      "Optimizing model...\n",
      "Step: 7, Action: 4, Reward: 0.2, Total Reward: -17.8, Loss: 10.31243896484375\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 8, Action: 0, Reward: 2.1, Total Reward: -15.700000000000001, Loss: 6.462501049041748\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 9, Action: 0, Reward: 2.0, Total Reward: -13.700000000000001, Loss: 7.2209882736206055\n",
      "Optimizing model...\n",
      "Step: 10, Action: 2, Reward: 0.25, Total Reward: -13.450000000000001, Loss: 4.368906497955322\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 11, Action: 0, Reward: 2.0, Total Reward: -11.450000000000001, Loss: 6.010583400726318\n",
      "Model acting\n",
      "\n",
      "episode: 168, reward: -111.45\n",
      "\n",
      "Optimizing model...\n",
      "Step: 0, Action: 1, Reward: 0.0, Total Reward: 0.0, Loss: 4.11203145980835\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: 2.1, Total Reward: 2.1, Loss: 2.543482780456543\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: -7.9, Total Reward: -5.800000000000001, Loss: 7.663458824157715\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: -8.0, Total Reward: -13.8, Loss: 5.523270606994629\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: -8.0, Total Reward: -21.8, Loss: 3.245163917541504\n",
      "Optimizing model...\n",
      "Step: 5, Action: 0, Reward: -8.0, Total Reward: -29.8, Loss: 2.4065990447998047\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 6, Action: 0, Reward: 2.0, Total Reward: -27.8, Loss: 7.846248626708984\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 7, Action: 0, Reward: 2.0, Total Reward: -25.8, Loss: 2.8928797245025635\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 8, Action: 0, Reward: -8.0, Total Reward: -33.8, Loss: 1.1774351596832275\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 9, Action: 0, Reward: -8.0, Total Reward: -41.8, Loss: 4.688751697540283\n",
      "Optimizing model...\n",
      "Step: 10, Action: 0, Reward: -8.0, Total Reward: -49.8, Loss: 10.950218200683594\n",
      "Optimizing model...\n",
      "Step: 11, Action: 0, Reward: -8.0, Total Reward: -57.8, Loss: 2.847412586212158\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 12, Action: 0, Reward: -8.0, Total Reward: -65.8, Loss: 8.768047332763672\n",
      "Model acting\n",
      "\n",
      "episode: 169, reward: -165.8\n",
      "\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: 2.0, Total Reward: 2.0, Loss: 3.26491117477417\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: 2.0, Total Reward: 4.0, Loss: 5.855855941772461\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: 2.0, Total Reward: 6.0, Loss: 6.639723300933838\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: 2.0, Total Reward: 8.0, Loss: 3.9763736724853516\n",
      "Optimizing model...\n",
      "Step: 4, Action: 3, Reward: 0.25, Total Reward: 8.25, Loss: 3.8532555103302\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 5, Action: 0, Reward: 2.0, Total Reward: 10.25, Loss: 2.6013660430908203\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 6, Action: 0, Reward: 2.0, Total Reward: 12.25, Loss: 3.9111199378967285\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 7, Action: 0, Reward: -8.0, Total Reward: 4.25, Loss: 5.8744049072265625\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 8, Action: 0, Reward: 2.0, Total Reward: 6.25, Loss: 4.035799503326416\n",
      "Optimizing model...\n",
      "Step: 9, Action: 2, Reward: 0.25, Total Reward: 6.5, Loss: 5.326010704040527\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 10, Action: 0, Reward: 2.0, Total Reward: 8.5, Loss: 5.435069561004639\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 11, Action: 0, Reward: 2.0, Total Reward: 10.5, Loss: 0.9808151125907898\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 12, Action: 0, Reward: 2.0, Total Reward: 12.5, Loss: 4.438206672668457\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 13, Action: 0, Reward: -8.0, Total Reward: 4.5, Loss: 7.209169864654541\n",
      "Optimizing model...\n",
      "Step: 14, Action: 1, Reward: 0.0, Total Reward: 4.5, Loss: 2.9577574729919434\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 15, Action: 0, Reward: 2.0, Total Reward: 6.5, Loss: 0.9163515567779541\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 16, Action: 4, Reward: 0.2, Total Reward: 6.7, Loss: 5.4655585289001465\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 17, Action: 0, Reward: 2.0, Total Reward: 8.7, Loss: 5.2554240226745605\n",
      "Optimizing model...\n",
      "Step: 18, Action: 4, Reward: 0.3, Total Reward: 9.0, Loss: 1.0925790071487427\n",
      "Optimizing model...\n",
      "Step: 19, Action: 0, Reward: 2.0, Total Reward: 11.0, Loss: 4.315641403198242\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 20, Action: 4, Reward: 0.2, Total Reward: 11.2, Loss: 5.351221084594727\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 21, Action: 0, Reward: 2.0, Total Reward: 13.2, Loss: 7.2960004806518555\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 22, Action: 4, Reward: 0.2, Total Reward: 13.399999999999999, Loss: 7.641419410705566\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 23, Action: 4, Reward: -9.8, Total Reward: 3.599999999999998, Loss: 3.8398900032043457\n",
      "Optimizing model...\n",
      "Step: 24, Action: 0, Reward: 2.0, Total Reward: 5.599999999999998, Loss: 6.427804946899414\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 25, Action: 4, Reward: 0.2, Total Reward: 5.799999999999998, Loss: 4.222018718719482\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 26, Action: 0, Reward: 2.0, Total Reward: 7.799999999999998, Loss: 2.7439136505126953\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 27, Action: 4, Reward: 0.2, Total Reward: 7.999999999999998, Loss: 5.923291206359863\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 28, Action: 4, Reward: -9.8, Total Reward: -1.8000000000000025, Loss: 6.421616554260254\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 29, Action: 4, Reward: -9.8, Total Reward: -11.600000000000003, Loss: 5.248603343963623\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 30, Action: 4, Reward: 0.2, Total Reward: -11.400000000000004, Loss: 7.019532203674316\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 31, Action: 4, Reward: -9.8, Total Reward: -21.200000000000003, Loss: 4.409320831298828\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 32, Action: 4, Reward: -9.8, Total Reward: -31.000000000000004, Loss: 6.18812894821167\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 33, Action: 4, Reward: -9.8, Total Reward: -40.800000000000004, Loss: 2.7361109256744385\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 34, Action: 4, Reward: 0.2, Total Reward: -40.6, Loss: 6.017951011657715\n",
      "Optimizing model...\n",
      "Step: 35, Action: 0, Reward: 2.0, Total Reward: -38.6, Loss: 5.055865287780762\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 36, Action: 4, Reward: 0.2, Total Reward: -38.4, Loss: 3.663640022277832\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 37, Action: 4, Reward: -9.8, Total Reward: -48.2, Loss: 1.2539713382720947\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 38, Action: 4, Reward: 0.2, Total Reward: -48.0, Loss: 6.983663558959961\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 39, Action: 4, Reward: -9.8, Total Reward: -57.8, Loss: 6.774168968200684\n",
      "Optimizing model...\n",
      "Step: 40, Action: 1, Reward: 0.0, Total Reward: -57.8, Loss: 4.246438980102539\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 41, Action: 4, Reward: 0.2, Total Reward: -57.599999999999994, Loss: 2.656292676925659\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 42, Action: 4, Reward: 0.2, Total Reward: -57.39999999999999, Loss: 7.300960540771484\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 43, Action: 4, Reward: -9.8, Total Reward: -67.19999999999999, Loss: 0.7654023170471191\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 44, Action: 4, Reward: -9.8, Total Reward: -76.99999999999999, Loss: 5.256801128387451\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 45, Action: 4, Reward: -9.8, Total Reward: -86.79999999999998, Loss: 2.650438070297241\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 46, Action: 4, Reward: -9.8, Total Reward: -96.59999999999998, Loss: 9.235849380493164\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 47, Action: 4, Reward: -9.8, Total Reward: -106.39999999999998, Loss: 7.943689823150635\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 48, Action: 4, Reward: 0.2, Total Reward: -106.19999999999997, Loss: 3.8999547958374023\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 49, Action: 4, Reward: 0.2, Total Reward: -105.99999999999997, Loss: 2.661555767059326\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 50, Action: 4, Reward: -9.8, Total Reward: -115.79999999999997, Loss: 2.701188087463379\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 51, Action: 0, Reward: 2.0, Total Reward: -113.79999999999997, Loss: 2.3961076736450195\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 52, Action: 2, Reward: 0.25, Total Reward: -113.54999999999997, Loss: 2.755431652069092\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 53, Action: 0, Reward: 2.0, Total Reward: -111.54999999999997, Loss: 6.745473861694336\n",
      "Optimizing model...\n",
      "Step: 54, Action: 2, Reward: 0.25, Total Reward: -111.29999999999997, Loss: 5.47371768951416\n",
      "Optimizing model...\n",
      "Step: 55, Action: 0, Reward: 2.0, Total Reward: -109.29999999999997, Loss: 1.2724847793579102\n",
      "Model acting\n",
      "\n",
      "episode: 170, reward: -209.29999999999995\n",
      "\n",
      "Model saved\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: 2.0, Total Reward: 2.0, Loss: 4.649479389190674\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: 2.0, Total Reward: 4.0, Loss: 1.5303874015808105\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: 2.0, Total Reward: 6.0, Loss: 7.307613372802734\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: 2.0, Total Reward: 8.0, Loss: 6.821319103240967\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: 2.0, Total Reward: 10.0, Loss: 5.337734222412109\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 5, Action: 0, Reward: 2.0, Total Reward: 12.0, Loss: 4.468379020690918\n",
      "Model acting\n",
      "\n",
      "episode: 171, reward: -88.0\n",
      "\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: -8.0, Total Reward: -8.0, Loss: 4.755457401275635\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: 2.0, Total Reward: -6.0, Loss: 5.796603679656982\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: 2.0, Total Reward: -4.0, Loss: 2.881758689880371\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 2, Reward: 0.25, Total Reward: -3.75, Loss: 5.946483612060547\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 4, Action: 2, Reward: 0.25, Total Reward: -3.5, Loss: 6.664676666259766\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 5, Action: 0, Reward: 2.0, Total Reward: -1.5, Loss: 3.2026820182800293\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 6, Action: 0, Reward: 2.1, Total Reward: 0.6000000000000001, Loss: 1.3796823024749756\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 7, Action: 0, Reward: 2.0, Total Reward: 2.6, Loss: 1.7164275646209717\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 8, Action: 0, Reward: 2.0, Total Reward: 4.6, Loss: 4.8020243644714355\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 9, Action: 0, Reward: 2.0, Total Reward: 6.6, Loss: 6.819178104400635\n",
      "Optimizing model...\n",
      "Step: 10, Action: 2, Reward: 0.25, Total Reward: 6.85, Loss: 6.84133768081665\n",
      "Optimizing model...\n",
      "Step: 11, Action: 3, Reward: 0.25, Total Reward: 7.1, Loss: 1.8381199836730957\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 12, Action: 0, Reward: 2.0, Total Reward: 9.1, Loss: 2.489501476287842\n",
      "Optimizing model...\n",
      "Step: 13, Action: 1, Reward: 0.0, Total Reward: 9.1, Loss: 5.486686706542969\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 14, Action: 0, Reward: 2.0, Total Reward: 11.1, Loss: 4.7195234298706055\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 15, Action: 0, Reward: 2.0, Total Reward: 13.1, Loss: 3.592031955718994\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 16, Action: 0, Reward: -8.0, Total Reward: 5.1, Loss: 5.469204902648926\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 17, Action: 0, Reward: 2.1, Total Reward: 7.199999999999999, Loss: 0.8017847537994385\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 18, Action: 0, Reward: -8.0, Total Reward: -0.8000000000000007, Loss: 2.5400710105895996\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 19, Action: 0, Reward: 2.0, Total Reward: 1.1999999999999993, Loss: 8.740169525146484\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 20, Action: 0, Reward: 2.0, Total Reward: 3.1999999999999993, Loss: 2.5072038173675537\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 21, Action: 0, Reward: -8.0, Total Reward: -4.800000000000001, Loss: 2.411611318588257\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 22, Action: 0, Reward: 2.0, Total Reward: -2.8000000000000007, Loss: 7.267728805541992\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 23, Action: 0, Reward: -8.0, Total Reward: -10.8, Loss: 0.9476174116134644\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 24, Action: 0, Reward: 2.0, Total Reward: -8.8, Loss: 4.642653465270996\n",
      "Model acting\n",
      "\n",
      "episode: 172, reward: -108.8\n",
      "\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: 2.0, Total Reward: 2.0, Loss: 4.514655113220215\n",
      "Optimizing model...\n",
      "Step: 1, Action: 1, Reward: 0.0, Total Reward: 2.0, Loss: 2.759891986846924\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: 2.0, Total Reward: 4.0, Loss: 4.435848236083984\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: -8.0, Total Reward: -4.0, Loss: 4.289388179779053\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: 2.0, Total Reward: -2.0, Loss: 3.46107816696167\n",
      "Optimizing model...\n",
      "Step: 5, Action: 0, Reward: 2.0, Total Reward: 0.0, Loss: 4.092254638671875\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 6, Action: 0, Reward: 2.0, Total Reward: 2.0, Loss: 4.399971008300781\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 7, Action: 0, Reward: 2.0, Total Reward: 4.0, Loss: 1.2117393016815186\n",
      "Optimizing model...\n",
      "Step: 8, Action: 4, Reward: 0.2, Total Reward: 4.2, Loss: 4.043820381164551\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 9, Action: 0, Reward: 2.1, Total Reward: 6.300000000000001, Loss: 4.985161304473877\n",
      "Optimizing model...\n",
      "Step: 10, Action: 3, Reward: 0.25, Total Reward: 6.550000000000001, Loss: 7.830554962158203\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 11, Action: 1, Reward: 0.0, Total Reward: 6.550000000000001, Loss: 4.171429634094238\n",
      "\n",
      "episode: 173, reward: -93.45\n",
      "\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: 2.0, Total Reward: 2.0, Loss: 2.424434185028076\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: 2.0, Total Reward: 4.0, Loss: 2.4447293281555176\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: 2.0, Total Reward: 6.0, Loss: 6.2452263832092285\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: -7.9, Total Reward: -1.9000000000000004, Loss: 4.18903923034668\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: -8.0, Total Reward: -9.9, Loss: 5.4421916007995605\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 5, Action: 0, Reward: 2.0, Total Reward: -7.9, Loss: 5.665224552154541\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 6, Action: 0, Reward: 2.0, Total Reward: -5.9, Loss: 8.888259887695312\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 7, Action: 0, Reward: 2.0, Total Reward: -3.9000000000000004, Loss: 5.334057807922363\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 8, Action: 0, Reward: 2.1, Total Reward: -1.8000000000000003, Loss: 6.971152305603027\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 9, Action: 0, Reward: 2.0, Total Reward: 0.19999999999999973, Loss: 4.582669258117676\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 10, Action: 0, Reward: 2.0, Total Reward: 2.1999999999999997, Loss: 2.654512405395508\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 11, Action: 0, Reward: 2.0, Total Reward: 4.199999999999999, Loss: 4.012188911437988\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 12, Action: 0, Reward: 2.0, Total Reward: 6.199999999999999, Loss: 0.5501998066902161\n",
      "Optimizing model...\n",
      "Step: 13, Action: 2, Reward: 0.25, Total Reward: 6.449999999999999, Loss: 4.285832405090332\n",
      "Optimizing model...\n",
      "Step: 14, Action: 3, Reward: 0.25, Total Reward: 6.699999999999999, Loss: 3.861328363418579\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 15, Action: 0, Reward: 2.0, Total Reward: 8.7, Loss: 4.7080793380737305\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 16, Action: 0, Reward: 2.0, Total Reward: 10.7, Loss: 10.741572380065918\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 17, Action: 0, Reward: 2.0, Total Reward: 12.7, Loss: 4.3909125328063965\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 18, Action: 0, Reward: 2.1, Total Reward: 14.799999999999999, Loss: 5.402942657470703\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 19, Action: 0, Reward: -8.0, Total Reward: 6.799999999999999, Loss: 4.922223091125488\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 20, Action: 0, Reward: 2.0, Total Reward: 8.799999999999999, Loss: 3.9725427627563477\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 21, Action: 0, Reward: -7.9, Total Reward: 0.8999999999999986, Loss: 3.211305618286133\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 22, Action: 0, Reward: -7.9, Total Reward: -7.000000000000002, Loss: 3.1193907260894775\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 23, Action: 0, Reward: 2.0, Total Reward: -5.000000000000002, Loss: 2.774651288986206\n",
      "Model acting\n",
      "\n",
      "episode: 174, reward: -105.0\n",
      "\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: 2.0, Total Reward: 2.0, Loss: 7.493834018707275\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: 2.0, Total Reward: 4.0, Loss: 5.862820625305176\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: 2.0, Total Reward: 6.0, Loss: 2.2630295753479004\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: 2.0, Total Reward: 8.0, Loss: 3.9247915744781494\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: 2.1, Total Reward: 10.1, Loss: 3.0506863594055176\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 5, Action: 0, Reward: 2.1, Total Reward: 12.2, Loss: 3.464582920074463\n",
      "Model acting\n",
      "\n",
      "episode: 175, reward: -87.8\n",
      "\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: -8.0, Total Reward: -8.0, Loss: 4.362613677978516\n",
      "Optimizing model...\n",
      "Step: 1, Action: 2, Reward: 0.25, Total Reward: -7.75, Loss: 7.5037126541137695\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: 2.0, Total Reward: -5.75, Loss: 7.68240213394165\n",
      "Optimizing model...\n",
      "Step: 3, Action: 2, Reward: 0.25, Total Reward: -5.5, Loss: 2.5161561965942383\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: 2.0, Total Reward: -3.5, Loss: 2.361588954925537\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 5, Action: 0, Reward: 2.0, Total Reward: -1.5, Loss: 6.222315311431885\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 6, Action: 0, Reward: 2.0, Total Reward: 0.5, Loss: 2.744647264480591\n",
      "Optimizing model...\n",
      "Step: 7, Action: 0, Reward: 2.0, Total Reward: 2.5, Loss: 9.220649719238281\n",
      "Model acting\n",
      "\n",
      "episode: 176, reward: -97.5\n",
      "\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: -8.0, Total Reward: -8.0, Loss: 4.658438682556152\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: -7.9, Total Reward: -15.9, Loss: 7.713601589202881\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 1, Reward: 0.0, Total Reward: -15.9, Loss: 5.047300338745117\n",
      "Optimizing model...\n",
      "Step: 3, Action: 3, Reward: 0.25, Total Reward: -15.65, Loss: 2.4573662281036377\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: 2.0, Total Reward: -13.65, Loss: 6.974419116973877\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 5, Action: 0, Reward: -8.0, Total Reward: -21.65, Loss: 5.963395118713379\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 6, Action: 1, Reward: 0.0, Total Reward: -21.65, Loss: 3.596351385116577\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 7, Action: 4, Reward: 0.2, Total Reward: -21.45, Loss: 10.008000373840332\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 8, Action: 1, Reward: 0.0, Total Reward: -21.45, Loss: 2.503499984741211\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 9, Action: 0, Reward: 2.0, Total Reward: -19.45, Loss: 2.836697816848755\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 10, Action: 0, Reward: -8.0, Total Reward: -27.45, Loss: 4.575710296630859\n",
      "Model acting\n",
      "\n",
      "episode: 177, reward: -127.45\n",
      "\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 4, Reward: 0.2, Total Reward: 0.2, Loss: 8.006340026855469\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 1, Action: 4, Reward: -9.8, Total Reward: -9.600000000000001, Loss: 3.6728601455688477\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: 2.0, Total Reward: -7.600000000000001, Loss: 4.1607818603515625\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 4, Reward: 0.2, Total Reward: -7.400000000000001, Loss: 4.456087112426758\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 4, Action: 4, Reward: -9.7, Total Reward: -17.1, Loss: 5.715432167053223\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 5, Action: 4, Reward: -9.8, Total Reward: -26.900000000000002, Loss: 3.027813673019409\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 6, Action: 0, Reward: 2.0, Total Reward: -24.900000000000002, Loss: 2.517300605773926\n",
      "Optimizing model...\n",
      "Step: 7, Action: 0, Reward: -8.0, Total Reward: -32.900000000000006, Loss: 4.276041030883789\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 8, Action: 4, Reward: 0.2, Total Reward: -32.7, Loss: 4.052881240844727\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 9, Action: 4, Reward: -9.8, Total Reward: -42.5, Loss: 5.309102535247803\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 10, Action: 0, Reward: 2.0, Total Reward: -40.5, Loss: 4.256564140319824\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 11, Action: 0, Reward: 2.0, Total Reward: -38.5, Loss: 3.607174873352051\n",
      "Model acting\n",
      "\n",
      "episode: 178, reward: -138.5\n",
      "\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: 2.0, Total Reward: 2.0, Loss: 6.859084129333496\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: 2.0, Total Reward: 4.0, Loss: 2.960270881652832\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 1, Reward: 0.0, Total Reward: 4.0, Loss: 6.06578254699707\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: 2.0, Total Reward: 6.0, Loss: 6.243807792663574\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: -8.0, Total Reward: -2.0, Loss: 4.206665992736816\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 5, Action: 0, Reward: 2.0, Total Reward: 0.0, Loss: 2.6426680088043213\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 6, Action: 0, Reward: -8.0, Total Reward: -8.0, Loss: 2.8776073455810547\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 7, Action: 0, Reward: 2.0, Total Reward: -6.0, Loss: 5.442241668701172\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 8, Action: 0, Reward: 2.0, Total Reward: -4.0, Loss: 4.952547073364258\n",
      "Model acting\n",
      "\n",
      "episode: 179, reward: -104.0\n",
      "\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: 2.0, Total Reward: 2.0, Loss: 4.4290337562561035\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: -7.9, Total Reward: -5.9, Loss: 7.264843940734863\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 1, Reward: 0.0, Total Reward: -5.9, Loss: 5.873100280761719\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 1, Reward: 0.0, Total Reward: -5.9, Loss: 3.775322437286377\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 4, Action: 1, Reward: 0.0, Total Reward: -5.9, Loss: 4.7369866371154785\n",
      "Model acting\n",
      "\n",
      "episode: 180, reward: -105.9\n",
      "\n",
      "Model saved\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 2, Reward: 0.25, Total Reward: 0.25, Loss: 7.63083553314209\n",
      "Optimizing model...\n",
      "Step: 1, Action: 1, Reward: 0.0, Total Reward: 0.25, Loss: 6.871359825134277\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 1, Reward: 0.0, Total Reward: 0.25, Loss: 3.216308116912842\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: 2.1, Total Reward: 2.35, Loss: 4.480956077575684\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: 2.0, Total Reward: 4.35, Loss: 4.949833869934082\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 5, Action: 0, Reward: 2.0, Total Reward: 6.35, Loss: 2.585757255554199\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 6, Action: 1, Reward: 0.0, Total Reward: 6.35, Loss: 4.692008972167969\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 7, Action: 0, Reward: 2.0, Total Reward: 8.35, Loss: 2.9042599201202393\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 8, Action: 0, Reward: 2.0, Total Reward: 10.35, Loss: 8.595211029052734\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 9, Action: 0, Reward: -8.0, Total Reward: 2.3499999999999996, Loss: 5.969132423400879\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 10, Action: 0, Reward: 2.0, Total Reward: 4.35, Loss: 4.267130374908447\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 11, Action: 1, Reward: 0.1, Total Reward: 4.449999999999999, Loss: 4.233088493347168\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 12, Action: 0, Reward: 2.0, Total Reward: 6.449999999999999, Loss: 4.3701581954956055\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 13, Action: 0, Reward: 2.0, Total Reward: 8.45, Loss: 2.64931058883667\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 14, Action: 1, Reward: 0.0, Total Reward: 8.45, Loss: 4.079065322875977\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 15, Action: 0, Reward: 2.0, Total Reward: 10.45, Loss: 4.2430315017700195\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 16, Action: 1, Reward: 0.0, Total Reward: 10.45, Loss: 2.502500057220459\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 17, Action: 1, Reward: 0.0, Total Reward: 10.45, Loss: 4.317914009094238\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 18, Action: 0, Reward: 2.0, Total Reward: 12.45, Loss: 5.100090026855469\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 19, Action: 0, Reward: 2.0, Total Reward: 14.45, Loss: 4.842551231384277\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 20, Action: 0, Reward: -7.9, Total Reward: 6.549999999999999, Loss: 2.7142677307128906\n",
      "Optimizing model...\n",
      "Step: 21, Action: 1, Reward: 0.0, Total Reward: 6.549999999999999, Loss: 6.358960151672363\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 22, Action: 0, Reward: 2.0, Total Reward: 8.549999999999999, Loss: 2.5096659660339355\n",
      "Optimizing model...\n",
      "Step: 23, Action: 1, Reward: 0.0, Total Reward: 8.549999999999999, Loss: 4.386140823364258\n",
      "Model acting\n",
      "\n",
      "episode: 181, reward: -91.45\n",
      "\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: -8.0, Total Reward: -8.0, Loss: 5.702119827270508\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: 2.0, Total Reward: -6.0, Loss: 2.4060957431793213\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: -8.0, Total Reward: -14.0, Loss: 3.1778440475463867\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 1, Reward: 0.0, Total Reward: -14.0, Loss: 1.120147466659546\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: 2.0, Total Reward: -12.0, Loss: 2.3843369483947754\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 5, Action: 0, Reward: 2.0, Total Reward: -10.0, Loss: 2.5204663276672363\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 6, Action: 0, Reward: -7.9, Total Reward: -17.9, Loss: 6.605201721191406\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 7, Action: 0, Reward: 2.0, Total Reward: -15.899999999999999, Loss: 3.8717379570007324\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 8, Action: 0, Reward: -8.0, Total Reward: -23.9, Loss: 2.852245330810547\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 9, Action: 0, Reward: -8.0, Total Reward: -31.9, Loss: 1.6185827255249023\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 10, Action: 0, Reward: 2.0, Total Reward: -29.9, Loss: 7.547272682189941\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 11, Action: 0, Reward: 2.0, Total Reward: -27.9, Loss: 3.056243658065796\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 12, Action: 0, Reward: -8.0, Total Reward: -35.9, Loss: 4.321005821228027\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 13, Action: 0, Reward: -8.0, Total Reward: -43.9, Loss: 7.362068176269531\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 14, Action: 0, Reward: 2.0, Total Reward: -41.9, Loss: 4.996458053588867\n",
      "Model acting\n",
      "\n",
      "episode: 182, reward: -141.9\n",
      "\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: -8.0, Total Reward: -8.0, Loss: 3.4900622367858887\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: -8.0, Total Reward: -16.0, Loss: 3.209453582763672\n",
      "Optimizing model...\n",
      "Step: 2, Action: 1, Reward: 0.0, Total Reward: -16.0, Loss: 7.5981974601745605\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: 2.0, Total Reward: -14.0, Loss: 6.825599670410156\n",
      "Optimizing model...\n",
      "Step: 4, Action: 2, Reward: 0.25, Total Reward: -13.75, Loss: 5.678328037261963\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 5, Action: 0, Reward: 2.0, Total Reward: -11.75, Loss: 8.36733627319336\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 6, Action: 0, Reward: 2.1, Total Reward: -9.65, Loss: 4.200101375579834\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 7, Action: 0, Reward: 2.1, Total Reward: -7.550000000000001, Loss: 6.208774566650391\n",
      "Model acting\n",
      "\n",
      "episode: 183, reward: -107.55\n",
      "\n",
      "Optimizing model...\n",
      "Step: 0, Action: 2, Reward: 0.25, Total Reward: 0.25, Loss: 3.096738338470459\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: 2.0, Total Reward: 2.25, Loss: 1.040647268295288\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: 2.0, Total Reward: 4.25, Loss: 4.727222442626953\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: -8.0, Total Reward: -3.75, Loss: 4.060275077819824\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: 2.0, Total Reward: -1.75, Loss: 6.205965995788574\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 5, Action: 0, Reward: -8.0, Total Reward: -9.75, Loss: 1.2779539823532104\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 6, Action: 0, Reward: 2.0, Total Reward: -7.75, Loss: 1.8594260215759277\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 7, Action: 0, Reward: 2.0, Total Reward: -5.75, Loss: 5.541242599487305\n",
      "Optimizing model...\n",
      "Step: 8, Action: 4, Reward: 0.2, Total Reward: -5.55, Loss: 2.0963144302368164\n",
      "Optimizing model...\n",
      "Step: 9, Action: 4, Reward: -9.8, Total Reward: -15.350000000000001, Loss: 7.595089912414551\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 10, Action: 1, Reward: 0.0, Total Reward: -15.350000000000001, Loss: 4.33443546295166\n",
      "Model acting\n",
      "\n",
      "episode: 184, reward: -115.35\n",
      "\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: -8.0, Total Reward: -8.0, Loss: 1.9233901500701904\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: 2.0, Total Reward: -6.0, Loss: 4.086162567138672\n",
      "Optimizing model...\n",
      "Step: 2, Action: 3, Reward: 0.25, Total Reward: -5.75, Loss: 6.156850814819336\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: 2.0, Total Reward: -3.75, Loss: 1.4756801128387451\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: -8.0, Total Reward: -11.75, Loss: 0.9205359220504761\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 5, Action: 0, Reward: 2.0, Total Reward: -9.75, Loss: 2.6977295875549316\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 6, Action: 0, Reward: 2.0, Total Reward: -7.75, Loss: 8.764945030212402\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 7, Action: 0, Reward: -8.0, Total Reward: -15.75, Loss: 4.164388656616211\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 8, Action: 0, Reward: 2.0, Total Reward: -13.75, Loss: 2.5766279697418213\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 9, Action: 0, Reward: 2.1, Total Reward: -11.65, Loss: 6.3097124099731445\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 10, Action: 0, Reward: -8.0, Total Reward: -19.65, Loss: 5.70684814453125\n",
      "\n",
      "episode: 185, reward: -119.65\n",
      "\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: 2.0, Total Reward: 2.0, Loss: 2.584557056427002\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: 2.0, Total Reward: 4.0, Loss: 0.7094658613204956\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: -8.0, Total Reward: -4.0, Loss: 6.352235794067383\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: 2.0, Total Reward: -2.0, Loss: 3.9268622398376465\n",
      "Optimizing model...\n",
      "Step: 4, Action: 4, Reward: 0.2, Total Reward: -1.8, Loss: 2.5014448165893555\n",
      "Optimizing model...\n",
      "Step: 5, Action: 4, Reward: -9.8, Total Reward: -11.600000000000001, Loss: 5.194886207580566\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 6, Action: 0, Reward: 2.0, Total Reward: -9.600000000000001, Loss: 1.293988823890686\n",
      "Optimizing model...\n",
      "Step: 7, Action: 3, Reward: 0.25, Total Reward: -9.350000000000001, Loss: 1.3005329370498657\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 8, Action: 0, Reward: 2.0, Total Reward: -7.350000000000001, Loss: 2.8974881172180176\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 9, Action: 0, Reward: 2.0, Total Reward: -5.350000000000001, Loss: 5.662181854248047\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 10, Action: 0, Reward: -8.0, Total Reward: -13.350000000000001, Loss: 8.296077728271484\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 11, Action: 0, Reward: 2.0, Total Reward: -11.350000000000001, Loss: 5.958671569824219\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 12, Action: 0, Reward: 2.0, Total Reward: -9.350000000000001, Loss: 6.189014434814453\n",
      "Optimizing model...\n",
      "Step: 13, Action: 2, Reward: 0.25, Total Reward: -9.100000000000001, Loss: 5.576911926269531\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 14, Action: 0, Reward: 2.0, Total Reward: -7.100000000000001, Loss: 8.061246871948242\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 15, Action: 0, Reward: -8.0, Total Reward: -15.100000000000001, Loss: 4.548851490020752\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 16, Action: 0, Reward: -8.0, Total Reward: -23.1, Loss: 3.9468259811401367\n",
      "\n",
      "episode: 186, reward: -123.1\n",
      "\n",
      "Optimizing model...\n",
      "Step: 0, Action: 1, Reward: 0.0, Total Reward: 0.0, Loss: 4.799295425415039\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: 2.0, Total Reward: 2.0, Loss: 5.742852687835693\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: -8.0, Total Reward: -6.0, Loss: 3.624213695526123\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: 2.1, Total Reward: -3.9, Loss: 1.233011245727539\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: -8.0, Total Reward: -11.9, Loss: 9.588932037353516\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 5, Action: 0, Reward: 2.0, Total Reward: -9.9, Loss: 4.1861958503723145\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 6, Action: 0, Reward: -8.0, Total Reward: -17.9, Loss: 6.56600284576416\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 7, Action: 0, Reward: 2.0, Total Reward: -15.899999999999999, Loss: 3.753962278366089\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 8, Action: 0, Reward: 2.0, Total Reward: -13.899999999999999, Loss: 8.81047248840332\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 9, Action: 0, Reward: 2.0, Total Reward: -11.899999999999999, Loss: 3.4779064655303955\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 10, Action: 0, Reward: -8.0, Total Reward: -19.9, Loss: 3.075173854827881\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 11, Action: 0, Reward: -8.0, Total Reward: -27.9, Loss: 7.773216247558594\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 12, Action: 0, Reward: 2.0, Total Reward: -25.9, Loss: 7.9600419998168945\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 13, Action: 0, Reward: -8.0, Total Reward: -33.9, Loss: 5.33683967590332\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 14, Action: 0, Reward: -8.0, Total Reward: -41.9, Loss: 5.798402786254883\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 15, Action: 0, Reward: -8.0, Total Reward: -49.9, Loss: 5.53889274597168\n",
      "Model acting\n",
      "\n",
      "episode: 187, reward: -149.9\n",
      "\n",
      "Optimizing model...\n",
      "Step: 0, Action: 3, Reward: 0.25, Total Reward: 0.25, Loss: 7.755047798156738\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: 2.0, Total Reward: 2.25, Loss: 4.64441442489624\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: 2.0, Total Reward: 4.25, Loss: 5.639809608459473\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: -8.0, Total Reward: -3.75, Loss: 6.44972038269043\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: -8.0, Total Reward: -11.75, Loss: 5.560909748077393\n",
      "Model acting\n",
      "\n",
      "episode: 188, reward: -111.75\n",
      "\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: -8.0, Total Reward: -8.0, Loss: 3.743863105773926\n",
      "Optimizing model...\n",
      "Step: 1, Action: 4, Reward: 0.2, Total Reward: -7.8, Loss: 2.3165149688720703\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: 2.1, Total Reward: -5.699999999999999, Loss: 2.694213390350342\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: 2.0, Total Reward: -3.6999999999999993, Loss: 2.679090976715088\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: -8.0, Total Reward: -11.7, Loss: 7.732236862182617\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 5, Action: 0, Reward: 2.0, Total Reward: -9.7, Loss: 4.256331920623779\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 6, Action: 0, Reward: 2.0, Total Reward: -7.699999999999999, Loss: 8.974043846130371\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 7, Action: 0, Reward: 2.0, Total Reward: -5.699999999999999, Loss: 4.41023588180542\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 8, Action: 1, Reward: 0.0, Total Reward: -5.699999999999999, Loss: 2.683253288269043\n",
      "Model acting\n",
      "\n",
      "episode: 189, reward: -105.7\n",
      "\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: 2.0, Total Reward: 2.0, Loss: 2.730093479156494\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: 2.0, Total Reward: 4.0, Loss: 2.281402826309204\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: 2.0, Total Reward: 6.0, Loss: 6.6451640129089355\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: 2.0, Total Reward: 8.0, Loss: 2.647789478302002\n",
      "Model acting\n",
      "\n",
      "episode: 190, reward: -92.0\n",
      "\n",
      "Model saved\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: 2.0, Total Reward: 2.0, Loss: 2.7270970344543457\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: 2.0, Total Reward: 4.0, Loss: 6.110830307006836\n",
      "Optimizing model...\n",
      "Step: 2, Action: 3, Reward: 0.25, Total Reward: 4.25, Loss: 3.076206684112549\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: 2.0, Total Reward: 6.25, Loss: 5.073819160461426\n",
      "Optimizing model...\n",
      "Step: 4, Action: 3, Reward: 0.35, Total Reward: 6.6, Loss: 2.347367286682129\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 5, Action: 0, Reward: 2.0, Total Reward: 8.6, Loss: 4.4106855392456055\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 6, Action: 0, Reward: 2.0, Total Reward: 10.6, Loss: 1.7561662197113037\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 7, Action: 0, Reward: 2.0, Total Reward: 12.6, Loss: 2.0786097049713135\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 8, Action: 0, Reward: 2.0, Total Reward: 14.6, Loss: 2.4940412044525146\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 9, Action: 0, Reward: 2.0, Total Reward: 16.6, Loss: 2.7220518589019775\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 10, Action: 0, Reward: -8.0, Total Reward: 8.600000000000001, Loss: 5.4151692390441895\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 11, Action: 0, Reward: -8.0, Total Reward: 0.6000000000000014, Loss: 6.383863925933838\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 12, Action: 0, Reward: -8.0, Total Reward: -7.399999999999999, Loss: 1.647845983505249\n",
      "Optimizing model...\n",
      "Step: 13, Action: 3, Reward: 0.25, Total Reward: -7.149999999999999, Loss: 5.540172100067139\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 14, Action: 0, Reward: 2.1, Total Reward: -5.049999999999999, Loss: 6.051967620849609\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 15, Action: 0, Reward: 2.0, Total Reward: -3.049999999999999, Loss: 5.757370948791504\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 16, Action: 0, Reward: 2.0, Total Reward: -1.049999999999999, Loss: 2.553574800491333\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 17, Action: 0, Reward: 2.0, Total Reward: 0.9500000000000011, Loss: 5.887547969818115\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 18, Action: 0, Reward: 2.0, Total Reward: 2.950000000000001, Loss: 3.61430025100708\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 19, Action: 0, Reward: -8.0, Total Reward: -5.049999999999999, Loss: 5.969245910644531\n",
      "Optimizing model...\n",
      "Step: 20, Action: 3, Reward: 0.25, Total Reward: -4.799999999999999, Loss: 3.9979898929595947\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 21, Action: 0, Reward: 2.0, Total Reward: -2.799999999999999, Loss: 4.320799827575684\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 22, Action: 0, Reward: -8.0, Total Reward: -10.799999999999999, Loss: 7.3361968994140625\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 23, Action: 0, Reward: 2.0, Total Reward: -8.799999999999999, Loss: 3.0780413150787354\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 24, Action: 0, Reward: 2.0, Total Reward: -6.799999999999999, Loss: 5.554129600524902\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 25, Action: 0, Reward: 2.0, Total Reward: -4.799999999999999, Loss: 4.580684661865234\n",
      "Model acting\n",
      "\n",
      "episode: 191, reward: -104.8\n",
      "\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: 2.0, Total Reward: 2.0, Loss: 5.662270545959473\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: -8.0, Total Reward: -6.0, Loss: 3.0300724506378174\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: 2.0, Total Reward: -4.0, Loss: 2.4505462646484375\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: 2.0, Total Reward: -2.0, Loss: 5.683879375457764\n",
      "Optimizing model...\n",
      "Step: 4, Action: 4, Reward: 0.2, Total Reward: -1.8, Loss: 3.842745304107666\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 5, Action: 0, Reward: 2.0, Total Reward: 0.19999999999999996, Loss: 11.830009460449219\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 6, Action: 0, Reward: 2.0, Total Reward: 2.2, Loss: 2.7832581996917725\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 7, Action: 0, Reward: 2.0, Total Reward: 4.2, Loss: 4.233054161071777\n",
      "Optimizing model...\n",
      "Step: 8, Action: 2, Reward: 0.25, Total Reward: 4.45, Loss: 2.6605515480041504\n",
      "Optimizing model...\n",
      "Step: 9, Action: 2, Reward: 0.25, Total Reward: 4.7, Loss: 3.879094123840332\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 10, Action: 0, Reward: 2.0, Total Reward: 6.7, Loss: 4.5622029304504395\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 11, Action: 0, Reward: -8.0, Total Reward: -1.2999999999999998, Loss: 2.697749137878418\n",
      "Model acting\n",
      "\n",
      "episode: 192, reward: -101.3\n",
      "\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: 2.0, Total Reward: 2.0, Loss: 2.4587717056274414\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: -8.0, Total Reward: -6.0, Loss: 2.666555404663086\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 1, Reward: 0.0, Total Reward: -6.0, Loss: 6.340567588806152\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: 2.0, Total Reward: -4.0, Loss: 5.762748718261719\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: 2.0, Total Reward: -2.0, Loss: 4.155237197875977\n",
      "Optimizing model...\n",
      "Step: 5, Action: 3, Reward: 0.25, Total Reward: -1.75, Loss: 0.8933464288711548\n",
      "Model acting\n",
      "\n",
      "episode: 193, reward: -101.75\n",
      "\n",
      "Optimizing model...\n",
      "Step: 0, Action: 2, Reward: 0.25, Total Reward: 0.25, Loss: 2.529486656188965\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: 2.0, Total Reward: 2.25, Loss: 6.88325309753418\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: 2.0, Total Reward: 4.25, Loss: 4.489829063415527\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: 2.0, Total Reward: 6.25, Loss: 5.515553951263428\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: 2.0, Total Reward: 8.25, Loss: 4.227436065673828\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 5, Action: 0, Reward: -8.0, Total Reward: 0.25, Loss: 6.029061794281006\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 6, Action: 0, Reward: 2.0, Total Reward: 2.25, Loss: 7.639533996582031\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 7, Action: 0, Reward: -8.0, Total Reward: -5.75, Loss: 9.024730682373047\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 8, Action: 0, Reward: -8.0, Total Reward: -13.75, Loss: 0.8776475191116333\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 9, Action: 0, Reward: 2.0, Total Reward: -11.75, Loss: 2.6713132858276367\n",
      "Optimizing model...\n",
      "Step: 10, Action: 1, Reward: 0.0, Total Reward: -11.75, Loss: 7.896129608154297\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 11, Action: 0, Reward: 2.0, Total Reward: -9.75, Loss: 4.8905720710754395\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 12, Action: 0, Reward: 2.0, Total Reward: -7.75, Loss: 4.169532775878906\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 13, Action: 2, Reward: 0.25, Total Reward: -7.5, Loss: 8.594114303588867\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 14, Action: 2, Reward: -9.75, Total Reward: -17.25, Loss: 2.4851107597351074\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 15, Action: 2, Reward: 0.25, Total Reward: -17.0, Loss: 6.203657627105713\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 16, Action: 2, Reward: -9.75, Total Reward: -26.75, Loss: 2.665652275085449\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 17, Action: 2, Reward: 0.25, Total Reward: -26.5, Loss: 4.043516159057617\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 18, Action: 2, Reward: -9.75, Total Reward: -36.25, Loss: 6.09606409072876\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 19, Action: 2, Reward: -9.75, Total Reward: -46.0, Loss: 5.560795783996582\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 20, Action: 2, Reward: -9.65, Total Reward: -55.65, Loss: 3.649470806121826\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 21, Action: 2, Reward: -9.75, Total Reward: -65.4, Loss: 2.439687490463257\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 22, Action: 2, Reward: 0.25, Total Reward: -65.15, Loss: 2.967987060546875\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 23, Action: 2, Reward: -9.75, Total Reward: -74.9, Loss: 5.804969787597656\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 24, Action: 2, Reward: 0.25, Total Reward: -74.65, Loss: 4.134493827819824\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 25, Action: 2, Reward: 0.25, Total Reward: -74.4, Loss: 1.2898818254470825\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 26, Action: 2, Reward: 0.25, Total Reward: -74.15, Loss: 4.557401657104492\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 27, Action: 2, Reward: 0.25, Total Reward: -73.9, Loss: 3.9890074729919434\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 28, Action: 2, Reward: 0.25, Total Reward: -73.65, Loss: 2.136385202407837\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 29, Action: 2, Reward: -9.75, Total Reward: -83.4, Loss: 1.1062356233596802\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 30, Action: 2, Reward: -9.75, Total Reward: -93.15, Loss: 6.542412281036377\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 31, Action: 2, Reward: -9.75, Total Reward: -102.9, Loss: 1.144120454788208\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 32, Action: 2, Reward: -9.75, Total Reward: -112.65, Loss: 7.126243591308594\n",
      "Model acting\n",
      "\n",
      "episode: 194, reward: -212.65\n",
      "\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: -8.0, Total Reward: -8.0, Loss: 2.8318376541137695\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 1, Action: 2, Reward: 0.25, Total Reward: -7.75, Loss: 2.729059934616089\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: 2.0, Total Reward: -5.75, Loss: 4.869338512420654\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 3, Reward: 0.25, Total Reward: -5.5, Loss: 1.2689348459243774\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: 2.0, Total Reward: -3.5, Loss: 3.547926425933838\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 5, Action: 3, Reward: 0.25, Total Reward: -3.25, Loss: 9.774561882019043\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 6, Action: 3, Reward: 0.25, Total Reward: -3.0, Loss: 4.352291107177734\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 7, Action: 3, Reward: 0.35, Total Reward: -2.65, Loss: 4.507210731506348\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 8, Action: 3, Reward: -9.75, Total Reward: -12.4, Loss: 4.202243804931641\n",
      "Model acting\n",
      "\n",
      "episode: 195, reward: -112.4\n",
      "\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 3, Reward: 0.25, Total Reward: 0.25, Loss: 4.402902603149414\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 1, Action: 3, Reward: -9.75, Total Reward: -9.5, Loss: 6.474485874176025\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: 2.0, Total Reward: -7.5, Loss: 2.6054418087005615\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: -8.0, Total Reward: -15.5, Loss: 3.1947708129882812\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 4, Action: 3, Reward: 0.25, Total Reward: -15.25, Loss: 2.4754858016967773\n",
      "Optimizing model...\n",
      "Step: 5, Action: 4, Reward: 0.2, Total Reward: -15.05, Loss: 2.6368584632873535\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 6, Action: 2, Reward: 0.25, Total Reward: -14.8, Loss: 2.5087342262268066\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 7, Action: 2, Reward: 0.25, Total Reward: -14.55, Loss: 5.505597114562988\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 8, Action: 2, Reward: -9.75, Total Reward: -24.3, Loss: 9.155708312988281\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 9, Action: 2, Reward: 0.25, Total Reward: -24.05, Loss: 1.023399829864502\n",
      "Optimizing model...\n",
      "Step: 10, Action: 1, Reward: 0.0, Total Reward: -24.05, Loss: 4.391570091247559\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 11, Action: 0, Reward: 2.0, Total Reward: -22.05, Loss: 4.3283233642578125\n",
      "Optimizing model...\n",
      "Step: 12, Action: 0, Reward: 2.0, Total Reward: -20.05, Loss: 7.315773963928223\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 13, Action: 0, Reward: -8.0, Total Reward: -28.05, Loss: 7.7199554443359375\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 14, Action: 0, Reward: 2.0, Total Reward: -26.05, Loss: 2.6676602363586426\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 15, Action: 0, Reward: 2.0, Total Reward: -24.05, Loss: 2.7956159114837646\n",
      "Optimizing model...\n",
      "Step: 16, Action: 3, Reward: 0.25, Total Reward: -23.8, Loss: 1.2098445892333984\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 17, Action: 0, Reward: 2.0, Total Reward: -21.8, Loss: 2.3120803833007812\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 18, Action: 0, Reward: 2.0, Total Reward: -19.8, Loss: 8.347439765930176\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 19, Action: 0, Reward: 2.0, Total Reward: -17.8, Loss: 5.427789688110352\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 20, Action: 0, Reward: 2.0, Total Reward: -15.8, Loss: 6.489135265350342\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 21, Action: 0, Reward: 2.0, Total Reward: -13.8, Loss: 2.6114730834960938\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 22, Action: 0, Reward: 2.0, Total Reward: -11.8, Loss: 2.6176180839538574\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 23, Action: 0, Reward: 2.0, Total Reward: -9.8, Loss: 3.8843374252319336\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 24, Action: 0, Reward: -7.9, Total Reward: -17.700000000000003, Loss: 1.6452913284301758\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 25, Action: 0, Reward: -8.0, Total Reward: -25.700000000000003, Loss: 7.329414367675781\n",
      "Optimizing model...\n",
      "Step: 26, Action: 2, Reward: 0.25, Total Reward: -25.450000000000003, Loss: 2.992105484008789\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 27, Action: 0, Reward: 2.0, Total Reward: -23.450000000000003, Loss: 2.640388011932373\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 28, Action: 0, Reward: 2.0, Total Reward: -21.450000000000003, Loss: 4.0535383224487305\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 29, Action: 0, Reward: -8.0, Total Reward: -29.450000000000003, Loss: 4.934258460998535\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 30, Action: 0, Reward: 2.0, Total Reward: -27.450000000000003, Loss: 5.01395320892334\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 31, Action: 0, Reward: 2.0, Total Reward: -25.450000000000003, Loss: 8.708084106445312\n",
      "Optimizing model...\n",
      "Step: 32, Action: 0, Reward: -8.0, Total Reward: -33.45, Loss: 1.4045125246047974\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 33, Action: 0, Reward: 2.0, Total Reward: -31.450000000000003, Loss: 4.520011901855469\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 34, Action: 0, Reward: -8.0, Total Reward: -39.45, Loss: 4.436008930206299\n",
      "Model acting\n",
      "\n",
      "episode: 196, reward: -139.45\n",
      "\n",
      "Optimizing model...\n",
      "Step: 0, Action: 1, Reward: 0.0, Total Reward: 0.0, Loss: 4.229738235473633\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: 2.0, Total Reward: 2.0, Loss: 7.027355670928955\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: 2.0, Total Reward: 4.0, Loss: 5.65194034576416\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: -8.0, Total Reward: -4.0, Loss: 2.3194360733032227\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: -8.0, Total Reward: -12.0, Loss: 6.94551944732666\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 5, Action: 0, Reward: 2.1, Total Reward: -9.9, Loss: 3.216226577758789\n",
      "Optimizing model...\n",
      "Step: 6, Action: 3, Reward: 0.25, Total Reward: -9.65, Loss: 11.76694107055664\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 7, Action: 0, Reward: 2.0, Total Reward: -7.65, Loss: 7.67213249206543\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 8, Action: 0, Reward: 2.0, Total Reward: -5.65, Loss: 5.821760654449463\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 9, Action: 0, Reward: -8.0, Total Reward: -13.65, Loss: 2.569912910461426\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 10, Action: 0, Reward: 2.0, Total Reward: -11.65, Loss: 0.8775092363357544\n",
      "Optimizing model...\n",
      "Step: 11, Action: 1, Reward: 0.1, Total Reward: -11.55, Loss: 4.101442337036133\n",
      "Model acting\n",
      "\n",
      "episode: 197, reward: -111.55\n",
      "\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: -8.0, Total Reward: -8.0, Loss: 1.3561534881591797\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: 2.0, Total Reward: -6.0, Loss: 5.2088847160339355\n",
      "Optimizing model...\n",
      "Step: 2, Action: 1, Reward: 0.0, Total Reward: -6.0, Loss: 6.681253910064697\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: 2.1, Total Reward: -3.9, Loss: 7.495321750640869\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: 2.0, Total Reward: -1.9, Loss: 2.2428903579711914\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 5, Action: 0, Reward: 2.0, Total Reward: 0.10000000000000009, Loss: 5.831674575805664\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 6, Action: 0, Reward: -8.0, Total Reward: -7.9, Loss: 0.920567512512207\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 7, Action: 0, Reward: 2.0, Total Reward: -5.9, Loss: 9.094694137573242\n",
      "Model acting\n",
      "\n",
      "episode: 198, reward: -105.9\n",
      "\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: -8.0, Total Reward: -8.0, Loss: 7.533973693847656\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: 2.0, Total Reward: -6.0, Loss: 4.344936370849609\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: 2.0, Total Reward: -4.0, Loss: 9.165130615234375\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: 2.0, Total Reward: -2.0, Loss: 3.4075114727020264\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: -8.0, Total Reward: -10.0, Loss: 7.203705787658691\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 5, Action: 0, Reward: -8.0, Total Reward: -18.0, Loss: 6.371671676635742\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 6, Action: 0, Reward: -8.0, Total Reward: -26.0, Loss: 9.208565711975098\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 7, Action: 0, Reward: -8.0, Total Reward: -34.0, Loss: 5.619471549987793\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 8, Action: 0, Reward: -8.0, Total Reward: -42.0, Loss: 5.6480302810668945\n",
      "Optimizing model...\n",
      "Step: 9, Action: 0, Reward: -8.0, Total Reward: -50.0, Loss: 4.517979621887207\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 10, Action: 0, Reward: -8.0, Total Reward: -58.0, Loss: 0.7717854380607605\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 11, Action: 0, Reward: -8.0, Total Reward: -66.0, Loss: 9.29809284210205\n",
      "Optimizing model...\n",
      "Step: 12, Action: 2, Reward: 0.25, Total Reward: -65.75, Loss: 7.806876182556152\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 13, Action: 0, Reward: 2.0, Total Reward: -63.75, Loss: 8.999992370605469\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 14, Action: 0, Reward: 2.0, Total Reward: -61.75, Loss: 2.4773805141448975\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 15, Action: 0, Reward: 2.0, Total Reward: -59.75, Loss: 4.917543411254883\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 16, Action: 0, Reward: 2.0, Total Reward: -57.75, Loss: 7.213904857635498\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 17, Action: 0, Reward: 2.0, Total Reward: -55.75, Loss: 2.8012382984161377\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 18, Action: 0, Reward: 2.0, Total Reward: -53.75, Loss: 6.9831037521362305\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 19, Action: 0, Reward: -8.0, Total Reward: -61.75, Loss: 6.454208850860596\n",
      "Optimizing model...\n",
      "Step: 20, Action: 2, Reward: 0.25, Total Reward: -61.5, Loss: 5.383277416229248\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 21, Action: 0, Reward: 2.0, Total Reward: -59.5, Loss: 4.368277549743652\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 22, Action: 0, Reward: -8.0, Total Reward: -67.5, Loss: 4.010721206665039\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 23, Action: 0, Reward: 2.0, Total Reward: -65.5, Loss: 3.6465868949890137\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 24, Action: 0, Reward: 2.0, Total Reward: -63.5, Loss: 5.646509170532227\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 25, Action: 0, Reward: 2.0, Total Reward: -61.5, Loss: 3.234659194946289\n",
      "Model acting\n",
      "\n",
      "episode: 199, reward: -161.5\n",
      "\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: -8.0, Total Reward: -8.0, Loss: 3.732787847518921\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: -8.0, Total Reward: -16.0, Loss: 7.589527130126953\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: 2.0, Total Reward: -14.0, Loss: 4.038841247558594\n",
      "Optimizing model...\n",
      "Step: 3, Action: 4, Reward: 0.2, Total Reward: -13.8, Loss: 6.41280460357666\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: 2.0, Total Reward: -11.8, Loss: 4.599471569061279\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 5, Action: 0, Reward: 2.0, Total Reward: -9.8, Loss: 3.520042657852173\n",
      "Optimizing model...\n",
      "Step: 6, Action: 3, Reward: 0.25, Total Reward: -9.55, Loss: 1.1784379482269287\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 7, Action: 0, Reward: 2.0, Total Reward: -7.550000000000001, Loss: 1.128402590751648\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 8, Action: 0, Reward: -8.0, Total Reward: -15.55, Loss: 2.7663164138793945\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 9, Action: 0, Reward: -8.0, Total Reward: -23.55, Loss: 1.2412970066070557\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 10, Action: 0, Reward: 2.0, Total Reward: -21.55, Loss: 7.979767322540283\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 11, Action: 0, Reward: 2.0, Total Reward: -19.55, Loss: 5.955972671508789\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 12, Action: 0, Reward: -8.0, Total Reward: -27.55, Loss: 5.90101432800293\n",
      "Model acting\n",
      "\n",
      "episode: 200, reward: -127.55\n",
      "\n",
      "Model saved\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: -8.0, Total Reward: -8.0, Loss: 6.0511651039123535\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: -8.0, Total Reward: -16.0, Loss: 4.108942985534668\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: 2.0, Total Reward: -14.0, Loss: 6.234052658081055\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: 2.0, Total Reward: -12.0, Loss: 4.328676223754883\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: 2.0, Total Reward: -10.0, Loss: 4.254746437072754\n",
      "Model acting\n",
      "\n",
      "episode: 201, reward: -110.0\n",
      "\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: -8.0, Total Reward: -8.0, Loss: 1.5256469249725342\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: 2.0, Total Reward: -6.0, Loss: 7.016733169555664\n",
      "Optimizing model...\n",
      "Step: 2, Action: 3, Reward: 0.25, Total Reward: -5.75, Loss: 4.734889984130859\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: 2.0, Total Reward: -3.75, Loss: 4.252025604248047\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: -8.0, Total Reward: -11.75, Loss: 1.2752711772918701\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 5, Action: 0, Reward: -8.0, Total Reward: -19.75, Loss: 4.460982322692871\n",
      "Optimizing model...\n",
      "Step: 6, Action: 1, Reward: 0.0, Total Reward: -19.75, Loss: 4.601805210113525\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 7, Action: 0, Reward: 2.0, Total Reward: -17.75, Loss: 6.261213302612305\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 8, Action: 0, Reward: 2.0, Total Reward: -15.75, Loss: 4.516773223876953\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 9, Action: 0, Reward: -8.0, Total Reward: -23.75, Loss: 7.439088344573975\n",
      "Optimizing model...\n",
      "Step: 10, Action: 4, Reward: 0.2, Total Reward: -23.55, Loss: 4.462496757507324\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 11, Action: 0, Reward: 2.0, Total Reward: -21.55, Loss: 5.72384786605835\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 12, Action: 1, Reward: 0.0, Total Reward: -21.55, Loss: 7.826725482940674\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 13, Action: 1, Reward: 0.0, Total Reward: -21.55, Loss: 3.996387004852295\n",
      "Optimizing model...\n",
      "Step: 14, Action: 1, Reward: 0.0, Total Reward: -21.55, Loss: 6.021833896636963\n",
      "Model acting\n",
      "\n",
      "episode: 202, reward: -121.55\n",
      "\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: -8.0, Total Reward: -8.0, Loss: 8.639236450195312\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: -8.0, Total Reward: -16.0, Loss: 8.645606994628906\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: 2.0, Total Reward: -14.0, Loss: 6.235908031463623\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: 2.0, Total Reward: -12.0, Loss: 2.922807216644287\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: -8.0, Total Reward: -20.0, Loss: 1.143606424331665\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 5, Action: 0, Reward: -8.0, Total Reward: -28.0, Loss: 8.106948852539062\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 6, Action: 0, Reward: 2.0, Total Reward: -26.0, Loss: 6.462050437927246\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 7, Action: 0, Reward: 2.0, Total Reward: -24.0, Loss: 2.488949775695801\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 8, Action: 0, Reward: 2.0, Total Reward: -22.0, Loss: 2.8156485557556152\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 9, Action: 0, Reward: 2.0, Total Reward: -20.0, Loss: 7.576447010040283\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 10, Action: 0, Reward: 2.0, Total Reward: -18.0, Loss: 3.092085361480713\n",
      "Optimizing model...\n",
      "Step: 11, Action: 1, Reward: 0.0, Total Reward: -18.0, Loss: 5.781693458557129\n",
      "Optimizing model...\n",
      "Step: 12, Action: 2, Reward: 0.35, Total Reward: -17.65, Loss: 8.542153358459473\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 13, Action: 0, Reward: 2.0, Total Reward: -15.649999999999999, Loss: 5.414480209350586\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 14, Action: 0, Reward: 2.0, Total Reward: -13.649999999999999, Loss: 5.83490514755249\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 15, Action: 0, Reward: 2.0, Total Reward: -11.649999999999999, Loss: 5.5957794189453125\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 16, Action: 0, Reward: 2.0, Total Reward: -9.649999999999999, Loss: 8.966328620910645\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 17, Action: 0, Reward: 2.0, Total Reward: -7.649999999999999, Loss: 2.01587176322937\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 18, Action: 0, Reward: 2.0, Total Reward: -5.649999999999999, Loss: 6.100379467010498\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 19, Action: 0, Reward: -8.0, Total Reward: -13.649999999999999, Loss: 7.736810684204102\n",
      "Model acting\n",
      "\n",
      "episode: 203, reward: -113.65\n",
      "\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: 2.0, Total Reward: 2.0, Loss: 6.762948513031006\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: -8.0, Total Reward: -6.0, Loss: 10.618881225585938\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: 2.0, Total Reward: -4.0, Loss: 3.6833667755126953\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: 2.0, Total Reward: -2.0, Loss: 4.200225353240967\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: 2.0, Total Reward: 0.0, Loss: 2.6912622451782227\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 5, Action: 0, Reward: -8.0, Total Reward: -8.0, Loss: 9.381438255310059\n",
      "Model acting\n",
      "\n",
      "episode: 204, reward: -108.0\n",
      "\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: -8.0, Total Reward: -8.0, Loss: 7.41889762878418\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: 2.0, Total Reward: -6.0, Loss: 3.25215482711792\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: 2.0, Total Reward: -4.0, Loss: 5.331700325012207\n",
      "Optimizing model...\n",
      "Step: 3, Action: 3, Reward: 0.25, Total Reward: -3.75, Loss: 7.0734968185424805\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: 2.0, Total Reward: -1.75, Loss: 3.8117780685424805\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 5, Action: 0, Reward: -8.0, Total Reward: -9.75, Loss: 3.8398447036743164\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 6, Action: 0, Reward: 2.0, Total Reward: -7.75, Loss: 6.058986663818359\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 7, Action: 0, Reward: -8.0, Total Reward: -15.75, Loss: 8.895280838012695\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 8, Action: 0, Reward: 2.0, Total Reward: -13.75, Loss: 2.2949416637420654\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 9, Action: 0, Reward: 2.0, Total Reward: -11.75, Loss: 2.764061689376831\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 10, Action: 0, Reward: 2.0, Total Reward: -9.75, Loss: 2.787381172180176\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 11, Action: 0, Reward: -8.0, Total Reward: -17.75, Loss: 1.9376335144042969\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 12, Action: 0, Reward: 2.1, Total Reward: -15.65, Loss: 2.963284969329834\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 13, Action: 0, Reward: -8.0, Total Reward: -23.65, Loss: 2.5934998989105225\n",
      "Optimizing model...\n",
      "Step: 14, Action: 4, Reward: 0.2, Total Reward: -23.45, Loss: 2.8025736808776855\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 15, Action: 0, Reward: 2.0, Total Reward: -21.45, Loss: 2.926093101501465\n",
      "Model acting\n",
      "\n",
      "episode: 205, reward: -121.45\n",
      "\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: 2.0, Total Reward: 2.0, Loss: 3.5239901542663574\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: -8.0, Total Reward: -6.0, Loss: 5.628777503967285\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: -8.0, Total Reward: -14.0, Loss: 2.6227173805236816\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: 2.0, Total Reward: -12.0, Loss: 0.7679833769798279\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 4, Action: 1, Reward: 0.0, Total Reward: -12.0, Loss: 7.265066623687744\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 5, Action: 0, Reward: 2.0, Total Reward: -10.0, Loss: 7.372595310211182\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 6, Action: 0, Reward: 2.0, Total Reward: -8.0, Loss: 4.784497261047363\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 7, Action: 0, Reward: 2.0, Total Reward: -6.0, Loss: 5.531918048858643\n",
      "\n",
      "episode: 206, reward: -106.0\n",
      "\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: 2.0, Total Reward: 2.0, Loss: 2.779845714569092\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: 2.0, Total Reward: 4.0, Loss: 10.90467357635498\n",
      "Optimizing model...\n",
      "Step: 2, Action: 2, Reward: 0.25, Total Reward: 4.25, Loss: 7.473992347717285\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: 2.0, Total Reward: 6.25, Loss: 4.143309593200684\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: -8.0, Total Reward: -1.75, Loss: 1.4770772457122803\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 5, Action: 0, Reward: 2.0, Total Reward: 0.25, Loss: 4.52138090133667\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 6, Action: 0, Reward: -7.9, Total Reward: -7.65, Loss: 5.916623115539551\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 7, Action: 0, Reward: -8.0, Total Reward: -15.65, Loss: 4.411271095275879\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 8, Action: 0, Reward: 2.0, Total Reward: -13.65, Loss: 2.4469289779663086\n",
      "Optimizing model...\n",
      "Step: 9, Action: 0, Reward: 2.0, Total Reward: -11.65, Loss: 4.547374725341797\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 10, Action: 0, Reward: -8.0, Total Reward: -19.65, Loss: 4.447196960449219\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 11, Action: 0, Reward: -8.0, Total Reward: -27.65, Loss: 5.85661506652832\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 12, Action: 0, Reward: 2.0, Total Reward: -25.65, Loss: 7.613649845123291\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 13, Action: 1, Reward: 0.0, Total Reward: -25.65, Loss: 7.6344380378723145\n",
      "Optimizing model...\n",
      "Step: 14, Action: 3, Reward: 0.25, Total Reward: -25.4, Loss: 5.411511421203613\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 15, Action: 0, Reward: 2.0, Total Reward: -23.4, Loss: 10.295318603515625\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 16, Action: 1, Reward: 0.0, Total Reward: -23.4, Loss: 4.473102569580078\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 17, Action: 1, Reward: 0.0, Total Reward: -23.4, Loss: 5.803746223449707\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 18, Action: 0, Reward: 2.0, Total Reward: -21.4, Loss: 0.8780163526535034\n",
      "Optimizing model...\n",
      "Step: 19, Action: 0, Reward: 2.0, Total Reward: -19.4, Loss: 5.532312393188477\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 20, Action: 0, Reward: -8.0, Total Reward: -27.4, Loss: 1.8239426612854004\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 21, Action: 0, Reward: -8.0, Total Reward: -35.4, Loss: 6.394775390625\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 22, Action: 0, Reward: 2.0, Total Reward: -33.4, Loss: 4.2098541259765625\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 23, Action: 0, Reward: 2.0, Total Reward: -31.4, Loss: 5.444097995758057\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 24, Action: 0, Reward: 2.0, Total Reward: -29.4, Loss: 3.1284666061401367\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 25, Action: 0, Reward: 2.0, Total Reward: -27.4, Loss: 7.057476997375488\n",
      "\n",
      "episode: 207, reward: -127.4\n",
      "\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: 2.0, Total Reward: 2.0, Loss: 3.3537745475769043\n",
      "Optimizing model...\n",
      "Step: 1, Action: 4, Reward: 0.2, Total Reward: 2.2, Loss: 6.001852989196777\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: 2.0, Total Reward: 4.2, Loss: 7.3263139724731445\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: 2.0, Total Reward: 6.2, Loss: 2.412929058074951\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: -8.0, Total Reward: -1.7999999999999998, Loss: 4.121042251586914\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 5, Action: 0, Reward: 2.0, Total Reward: 0.20000000000000018, Loss: 1.681058645248413\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 6, Action: 0, Reward: -8.0, Total Reward: -7.8, Loss: 3.3061718940734863\n",
      "Model acting\n",
      "\n",
      "episode: 208, reward: -107.8\n",
      "\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: -8.0, Total Reward: -8.0, Loss: 3.165515899658203\n",
      "Optimizing model...\n",
      "Step: 1, Action: 4, Reward: 0.2, Total Reward: -7.8, Loss: 5.203771591186523\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: 2.0, Total Reward: -5.8, Loss: 5.723238468170166\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: 2.0, Total Reward: -3.8, Loss: 2.82245135307312\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: -8.0, Total Reward: -11.8, Loss: 2.808505058288574\n",
      "Optimizing model...\n",
      "Step: 5, Action: 0, Reward: 2.0, Total Reward: -9.8, Loss: 2.5016708374023438\n",
      "Optimizing model...\n",
      "Step: 6, Action: 0, Reward: 2.0, Total Reward: -7.800000000000001, Loss: 2.6316542625427246\n",
      "Model acting\n",
      "\n",
      "episode: 209, reward: -107.8\n",
      "\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: -8.0, Total Reward: -8.0, Loss: 5.404952526092529\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: -8.0, Total Reward: -16.0, Loss: 4.248446464538574\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: 2.0, Total Reward: -14.0, Loss: 7.164231300354004\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: -8.0, Total Reward: -22.0, Loss: 7.767087936401367\n",
      "Optimizing model...\n",
      "Step: 4, Action: 2, Reward: 0.35, Total Reward: -21.65, Loss: 2.8946621417999268\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 5, Action: 0, Reward: 2.0, Total Reward: -19.65, Loss: 7.345251083374023\n",
      "Model acting\n",
      "\n",
      "episode: 210, reward: -119.65\n",
      "\n",
      "Model saved\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: 2.0, Total Reward: 2.0, Loss: 4.456659317016602\n",
      "Optimizing model...\n",
      "Step: 1, Action: 2, Reward: 0.25, Total Reward: 2.25, Loss: 6.395013809204102\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: 2.0, Total Reward: 4.25, Loss: 6.7767815589904785\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: 2.0, Total Reward: 6.25, Loss: 4.1614789962768555\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: 2.0, Total Reward: 8.25, Loss: 7.1381635665893555\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 5, Action: 0, Reward: 2.0, Total Reward: 10.25, Loss: 4.196377754211426\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 6, Action: 0, Reward: 2.0, Total Reward: 12.25, Loss: 3.6774442195892334\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 7, Action: 0, Reward: -8.0, Total Reward: 4.25, Loss: 10.642881393432617\n",
      "Optimizing model...\n",
      "Step: 8, Action: 2, Reward: 0.25, Total Reward: 4.5, Loss: 4.7537078857421875\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 9, Action: 0, Reward: 2.0, Total Reward: 6.5, Loss: 2.7912330627441406\n",
      "Model acting\n",
      "\n",
      "episode: 211, reward: -93.5\n",
      "\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: -8.0, Total Reward: -8.0, Loss: 1.1334415674209595\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: 2.0, Total Reward: -6.0, Loss: 6.467682361602783\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: -8.0, Total Reward: -14.0, Loss: 1.4459037780761719\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: -8.0, Total Reward: -22.0, Loss: 10.377086639404297\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: 2.0, Total Reward: -20.0, Loss: 4.41957950592041\n",
      "Optimizing model...\n",
      "Step: 5, Action: 2, Reward: 0.25, Total Reward: -19.75, Loss: 5.992199897766113\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 6, Action: 0, Reward: 2.0, Total Reward: -17.75, Loss: 7.344752311706543\n",
      "Optimizing model...\n",
      "Step: 7, Action: 1, Reward: 0.0, Total Reward: -17.75, Loss: 1.80594801902771\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 8, Action: 0, Reward: 2.0, Total Reward: -15.75, Loss: 5.830232620239258\n",
      "Optimizing model...\n",
      "Step: 9, Action: 3, Reward: 0.25, Total Reward: -15.5, Loss: 4.427120208740234\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 10, Action: 0, Reward: 2.0, Total Reward: -13.5, Loss: 7.593494415283203\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 11, Action: 0, Reward: 2.0, Total Reward: -11.5, Loss: 7.242037773132324\n",
      "Optimizing model...\n",
      "Step: 12, Action: 3, Reward: 0.25, Total Reward: -11.25, Loss: 7.142938613891602\n",
      "Optimizing model...\n",
      "Step: 13, Action: 2, Reward: 0.25, Total Reward: -11.0, Loss: 3.985508680343628\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 14, Action: 0, Reward: 2.0, Total Reward: -9.0, Loss: 2.5930192470550537\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 15, Action: 0, Reward: -8.0, Total Reward: -17.0, Loss: 4.187039375305176\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 16, Action: 0, Reward: 2.0, Total Reward: -15.0, Loss: 5.859484672546387\n",
      "Model acting\n",
      "\n",
      "episode: 212, reward: -115.0\n",
      "\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: 2.0, Total Reward: 2.0, Loss: 5.534578323364258\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 1, Action: 1, Reward: 0.0, Total Reward: 2.0, Loss: 5.759007453918457\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: 2.0, Total Reward: 4.0, Loss: 7.487852573394775\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: 2.0, Total Reward: 6.0, Loss: 1.4609694480895996\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: 2.0, Total Reward: 8.0, Loss: 6.112858772277832\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 5, Action: 0, Reward: 2.0, Total Reward: 10.0, Loss: 4.344455718994141\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 6, Action: 0, Reward: 2.0, Total Reward: 12.0, Loss: 2.5081558227539062\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 7, Action: 0, Reward: -8.0, Total Reward: 4.0, Loss: 2.885129928588867\n",
      "Optimizing model...\n",
      "Step: 8, Action: 0, Reward: -8.0, Total Reward: -4.0, Loss: 4.176996231079102\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 9, Action: 0, Reward: 2.0, Total Reward: -2.0, Loss: 5.377789497375488\n",
      "\n",
      "episode: 213, reward: -102.0\n",
      "\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: 2.0, Total Reward: 2.0, Loss: 6.026905536651611\n",
      "Optimizing model...\n",
      "Step: 1, Action: 1, Reward: 0.0, Total Reward: 2.0, Loss: 4.176094055175781\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: 2.1, Total Reward: 4.1, Loss: 6.81337833404541\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: 2.0, Total Reward: 6.1, Loss: 4.409063339233398\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: -8.0, Total Reward: -1.9000000000000004, Loss: 3.030397415161133\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 5, Action: 0, Reward: 2.0, Total Reward: 0.09999999999999964, Loss: 2.785965919494629\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 6, Action: 0, Reward: 2.0, Total Reward: 2.0999999999999996, Loss: 5.475117206573486\n",
      "Optimizing model...\n",
      "Step: 7, Action: 2, Reward: 0.25, Total Reward: 2.3499999999999996, Loss: 1.37923264503479\n",
      "Model acting\n",
      "\n",
      "episode: 214, reward: -97.65\n",
      "\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: 2.0, Total Reward: 2.0, Loss: 7.578246116638184\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: 2.0, Total Reward: 4.0, Loss: 3.9839141368865967\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: -8.0, Total Reward: -4.0, Loss: 5.692147254943848\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: -7.9, Total Reward: -11.9, Loss: 6.880767822265625\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: 2.0, Total Reward: -9.9, Loss: 7.585179805755615\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 5, Action: 0, Reward: 2.0, Total Reward: -7.9, Loss: 1.6726467609405518\n",
      "Optimizing model...\n",
      "Step: 6, Action: 1, Reward: 0.0, Total Reward: -7.9, Loss: 8.926305770874023\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 7, Action: 0, Reward: 2.0, Total Reward: -5.9, Loss: 5.804830074310303\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 8, Action: 0, Reward: 2.0, Total Reward: -3.9000000000000004, Loss: 5.942812442779541\n",
      "Model acting\n",
      "\n",
      "episode: 215, reward: -103.9\n",
      "\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: 2.0, Total Reward: 2.0, Loss: 4.189967155456543\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: -8.0, Total Reward: -6.0, Loss: 4.141414642333984\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: 2.0, Total Reward: -4.0, Loss: 1.2714661359786987\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: 2.0, Total Reward: -2.0, Loss: 2.9153342247009277\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: 2.0, Total Reward: 0.0, Loss: 2.235199451446533\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 5, Action: 0, Reward: 2.0, Total Reward: 2.0, Loss: 4.795933723449707\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 6, Action: 0, Reward: -8.0, Total Reward: -6.0, Loss: 3.495612859725952\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 7, Action: 0, Reward: -8.0, Total Reward: -14.0, Loss: 6.037384033203125\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 8, Action: 0, Reward: -8.0, Total Reward: -22.0, Loss: 4.467842102050781\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 9, Action: 0, Reward: -8.0, Total Reward: -30.0, Loss: 4.127039909362793\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 10, Action: 0, Reward: -8.0, Total Reward: -38.0, Loss: 7.271166801452637\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 11, Action: 1, Reward: 0.0, Total Reward: -38.0, Loss: 6.013152122497559\n",
      "Optimizing model...\n",
      "Step: 12, Action: 1, Reward: -9.9, Total Reward: -47.9, Loss: 7.668761253356934\n",
      "Optimizing model...\n",
      "Step: 13, Action: 1, Reward: 0.0, Total Reward: -47.9, Loss: 3.582029342651367\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 14, Action: 0, Reward: 2.0, Total Reward: -45.9, Loss: 4.353038311004639\n",
      "Optimizing model...\n",
      "Step: 15, Action: 1, Reward: 0.0, Total Reward: -45.9, Loss: 7.50531530380249\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 16, Action: 1, Reward: 0.0, Total Reward: -45.9, Loss: 5.814907073974609\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 17, Action: 0, Reward: 2.0, Total Reward: -43.9, Loss: 3.1068265438079834\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 18, Action: 0, Reward: -8.0, Total Reward: -51.9, Loss: 1.0884908437728882\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 19, Action: 1, Reward: 0.0, Total Reward: -51.9, Loss: 2.710599422454834\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 20, Action: 0, Reward: 2.0, Total Reward: -49.9, Loss: 5.9707841873168945\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 21, Action: 0, Reward: 2.0, Total Reward: -47.9, Loss: 4.232886791229248\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 22, Action: 0, Reward: 2.0, Total Reward: -45.9, Loss: 5.205689430236816\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 23, Action: 0, Reward: 2.0, Total Reward: -43.9, Loss: 3.1351194381713867\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 24, Action: 0, Reward: 2.0, Total Reward: -41.9, Loss: 6.448920249938965\n",
      "Model acting\n",
      "\n",
      "episode: 216, reward: -141.9\n",
      "\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: -8.0, Total Reward: -8.0, Loss: 4.233848571777344\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: -8.0, Total Reward: -16.0, Loss: 4.4630632400512695\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: 2.0, Total Reward: -14.0, Loss: 9.389642715454102\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: 2.0, Total Reward: -12.0, Loss: 5.667747974395752\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: 2.0, Total Reward: -10.0, Loss: 5.489434242248535\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 5, Action: 0, Reward: 2.0, Total Reward: -8.0, Loss: 4.473203659057617\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 6, Action: 0, Reward: -8.0, Total Reward: -16.0, Loss: 1.4589078426361084\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 7, Action: 0, Reward: -8.0, Total Reward: -24.0, Loss: 2.910888195037842\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 8, Action: 0, Reward: -8.0, Total Reward: -32.0, Loss: 3.493861436843872\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 9, Action: 0, Reward: -7.9, Total Reward: -39.9, Loss: 4.232283115386963\n",
      "Model acting\n",
      "\n",
      "episode: 217, reward: -139.9\n",
      "\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 3, Reward: 0.25, Total Reward: 0.25, Loss: 1.3361519575119019\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: 2.0, Total Reward: 2.25, Loss: 6.966780662536621\n",
      "Optimizing model...\n",
      "Step: 2, Action: 2, Reward: 0.35, Total Reward: 2.6, Loss: 7.378237724304199\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: 2.0, Total Reward: 4.6, Loss: 8.962542533874512\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: 2.0, Total Reward: 6.6, Loss: 4.799475193023682\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 5, Action: 0, Reward: 2.0, Total Reward: 8.6, Loss: 4.130423545837402\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 6, Action: 0, Reward: 2.1, Total Reward: 10.7, Loss: 3.5484089851379395\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 7, Action: 0, Reward: 2.0, Total Reward: 12.7, Loss: 1.2950109243392944\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 8, Action: 0, Reward: 2.0, Total Reward: 14.7, Loss: 3.1355509757995605\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 9, Action: 0, Reward: 2.0, Total Reward: 16.7, Loss: 4.151769161224365\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 10, Action: 0, Reward: 2.0, Total Reward: 18.7, Loss: 2.721548318862915\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 11, Action: 0, Reward: 2.0, Total Reward: 20.7, Loss: 2.8323442935943604\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 12, Action: 3, Reward: 0.25, Total Reward: 20.95, Loss: 1.2052433490753174\n",
      "Model acting\n",
      "\n",
      "episode: 218, reward: -79.05\n",
      "\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: 2.0, Total Reward: 2.0, Loss: 1.9470261335372925\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: -8.0, Total Reward: -6.0, Loss: 6.718510627746582\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: -8.0, Total Reward: -14.0, Loss: 1.819950819015503\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: -8.0, Total Reward: -22.0, Loss: 10.816604614257812\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: 2.0, Total Reward: -20.0, Loss: 5.632497787475586\n",
      "Optimizing model...\n",
      "Step: 5, Action: 0, Reward: -8.0, Total Reward: -28.0, Loss: 4.8057403564453125\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 6, Action: 0, Reward: 2.0, Total Reward: -26.0, Loss: 3.9224600791931152\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 7, Action: 0, Reward: -8.0, Total Reward: -34.0, Loss: 2.8792715072631836\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 8, Action: 0, Reward: -8.0, Total Reward: -42.0, Loss: 4.201719284057617\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 9, Action: 0, Reward: -8.0, Total Reward: -50.0, Loss: 7.479583740234375\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 10, Action: 0, Reward: -8.0, Total Reward: -58.0, Loss: 5.877783298492432\n",
      "Model acting\n",
      "\n",
      "episode: 219, reward: -158.0\n",
      "\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: 2.0, Total Reward: 2.0, Loss: 1.03773033618927\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: -8.0, Total Reward: -6.0, Loss: 6.390605449676514\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: -8.0, Total Reward: -14.0, Loss: 6.181680202484131\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: 2.0, Total Reward: -12.0, Loss: 6.129913806915283\n",
      "Optimizing model...\n",
      "Step: 4, Action: 2, Reward: 0.25, Total Reward: -11.75, Loss: 4.4359540939331055\n",
      "Model acting\n",
      "\n",
      "episode: 220, reward: -111.75\n",
      "\n",
      "Model saved\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: 2.0, Total Reward: 2.0, Loss: 2.7589099407196045\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: 2.0, Total Reward: 4.0, Loss: 2.952157735824585\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: 2.0, Total Reward: 6.0, Loss: 2.4401159286499023\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: 2.0, Total Reward: 8.0, Loss: 4.467177867889404\n",
      "Optimizing model...\n",
      "Step: 4, Action: 1, Reward: 0.0, Total Reward: 8.0, Loss: 5.421699523925781\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 5, Action: 0, Reward: 2.1, Total Reward: 10.1, Loss: 6.009902000427246\n",
      "Optimizing model...\n",
      "Step: 6, Action: 2, Reward: 0.25, Total Reward: 10.35, Loss: 5.494183540344238\n",
      "Optimizing model...\n",
      "Step: 7, Action: 1, Reward: 0.0, Total Reward: 10.35, Loss: 2.8938188552856445\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 8, Action: 0, Reward: 2.0, Total Reward: 12.35, Loss: 2.598541259765625\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 9, Action: 0, Reward: -8.0, Total Reward: 4.35, Loss: 8.865885734558105\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 10, Action: 0, Reward: 2.0, Total Reward: 6.35, Loss: 6.214751243591309\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 11, Action: 0, Reward: -8.0, Total Reward: -1.6500000000000004, Loss: 4.207978248596191\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 12, Action: 0, Reward: 2.0, Total Reward: 0.34999999999999964, Loss: 9.633563995361328\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 13, Action: 0, Reward: 2.0, Total Reward: 2.3499999999999996, Loss: 4.717634201049805\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 14, Action: 0, Reward: 2.0, Total Reward: 4.35, Loss: 4.43797492980957\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 15, Action: 0, Reward: -8.0, Total Reward: -3.6500000000000004, Loss: 4.196536064147949\n",
      "Model acting\n",
      "\n",
      "episode: 221, reward: -103.65\n",
      "\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: -8.0, Total Reward: -8.0, Loss: 3.771345615386963\n",
      "Optimizing model...\n",
      "Step: 1, Action: 1, Reward: 0.0, Total Reward: -8.0, Loss: 2.9684159755706787\n",
      "Optimizing model...\n",
      "Step: 2, Action: 1, Reward: 0.0, Total Reward: -8.0, Loss: 6.426737308502197\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: 2.0, Total Reward: -6.0, Loss: 7.6526665687561035\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: 2.0, Total Reward: -4.0, Loss: 3.8420844078063965\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 5, Action: 0, Reward: 2.0, Total Reward: -2.0, Loss: 6.124022483825684\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 6, Action: 0, Reward: 2.0, Total Reward: 0.0, Loss: 4.4177398681640625\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 7, Action: 0, Reward: -8.0, Total Reward: -8.0, Loss: 4.345030784606934\n",
      "Optimizing model...\n",
      "Step: 8, Action: 2, Reward: 0.25, Total Reward: -7.75, Loss: 4.429502010345459\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 9, Action: 0, Reward: 2.0, Total Reward: -5.75, Loss: 6.2567830085754395\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 10, Action: 0, Reward: -8.0, Total Reward: -13.75, Loss: 6.379422664642334\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 11, Action: 0, Reward: 2.0, Total Reward: -11.75, Loss: 6.271580219268799\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 12, Action: 0, Reward: 2.0, Total Reward: -9.75, Loss: 9.796263694763184\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 13, Action: 0, Reward: 2.0, Total Reward: -7.75, Loss: 5.862613677978516\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 14, Action: 0, Reward: 2.1, Total Reward: -5.65, Loss: 5.590303421020508\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 15, Action: 0, Reward: -8.0, Total Reward: -13.65, Loss: 3.753519058227539\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 16, Action: 0, Reward: 2.0, Total Reward: -11.65, Loss: 4.320992946624756\n",
      "Model acting\n",
      "\n",
      "episode: 222, reward: -111.65\n",
      "\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: 2.0, Total Reward: 2.0, Loss: 3.903336524963379\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: -8.0, Total Reward: -6.0, Loss: 5.518584728240967\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: 2.0, Total Reward: -4.0, Loss: 7.904994010925293\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: -8.0, Total Reward: -12.0, Loss: 7.121929168701172\n",
      "Optimizing model...\n",
      "Step: 4, Action: 4, Reward: 0.3, Total Reward: -11.7, Loss: 6.207267761230469\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 5, Action: 0, Reward: 2.0, Total Reward: -9.7, Loss: 2.72369384765625\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 6, Action: 0, Reward: 2.0, Total Reward: -7.699999999999999, Loss: 5.64955997467041\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 7, Action: 0, Reward: -8.0, Total Reward: -15.7, Loss: 4.213230133056641\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 8, Action: 0, Reward: -8.0, Total Reward: -23.7, Loss: 7.579208850860596\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 9, Action: 0, Reward: -8.0, Total Reward: -31.7, Loss: 2.269228458404541\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 10, Action: 0, Reward: -8.0, Total Reward: -39.7, Loss: 1.9281909465789795\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 11, Action: 0, Reward: -8.0, Total Reward: -47.7, Loss: 9.47532844543457\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 12, Action: 0, Reward: 2.0, Total Reward: -45.7, Loss: 7.505619049072266\n",
      "Model acting\n",
      "\n",
      "episode: 223, reward: -145.7\n",
      "\n",
      "Model acting\n",
      "\n",
      "episode: 224, reward: -100\n",
      "\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: 2.0, Total Reward: 2.0, Loss: 6.023200988769531\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: 2.0, Total Reward: 4.0, Loss: 9.081012725830078\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: -8.0, Total Reward: -4.0, Loss: 3.683523654937744\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: -8.0, Total Reward: -12.0, Loss: 6.149663925170898\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: 2.0, Total Reward: -10.0, Loss: 4.567432403564453\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 5, Action: 0, Reward: -7.9, Total Reward: -17.9, Loss: 4.7761640548706055\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 6, Action: 0, Reward: 2.0, Total Reward: -15.899999999999999, Loss: 4.810267448425293\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 7, Action: 0, Reward: 2.0, Total Reward: -13.899999999999999, Loss: 2.1333489418029785\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 8, Action: 0, Reward: 2.0, Total Reward: -11.899999999999999, Loss: 6.108856201171875\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 9, Action: 0, Reward: 2.0, Total Reward: -9.899999999999999, Loss: 3.993865489959717\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 10, Action: 0, Reward: 2.0, Total Reward: -7.899999999999999, Loss: 4.109968185424805\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 11, Action: 0, Reward: 2.0, Total Reward: -5.899999999999999, Loss: 3.7609052658081055\n",
      "Optimizing model...\n",
      "Step: 12, Action: 3, Reward: 0.35, Total Reward: -5.549999999999999, Loss: 5.375439643859863\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 13, Action: 0, Reward: 2.0, Total Reward: -3.549999999999999, Loss: 7.747562408447266\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 14, Action: 0, Reward: -8.0, Total Reward: -11.549999999999999, Loss: 6.115859031677246\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 15, Action: 0, Reward: 2.0, Total Reward: -9.549999999999999, Loss: 4.24415397644043\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 16, Action: 0, Reward: 2.0, Total Reward: -7.549999999999999, Loss: 2.450329303741455\n",
      "Optimizing model...\n",
      "Step: 17, Action: 2, Reward: 0.25, Total Reward: -7.299999999999999, Loss: 3.2980339527130127\n",
      "Model acting\n",
      "\n",
      "episode: 225, reward: -107.3\n",
      "\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: -8.0, Total Reward: -8.0, Loss: 2.751776695251465\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: 2.0, Total Reward: -6.0, Loss: 4.649369239807129\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: 2.0, Total Reward: -4.0, Loss: 9.348980903625488\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: 2.0, Total Reward: -2.0, Loss: 2.7535035610198975\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: 2.0, Total Reward: 0.0, Loss: 3.8962388038635254\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 5, Action: 0, Reward: 2.0, Total Reward: 2.0, Loss: 4.065101623535156\n",
      "Optimizing model...\n",
      "Step: 6, Action: 2, Reward: 0.25, Total Reward: 2.25, Loss: 6.547896385192871\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 7, Action: 0, Reward: 2.0, Total Reward: 4.25, Loss: 4.107507705688477\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 8, Action: 0, Reward: 2.0, Total Reward: 6.25, Loss: 7.50181770324707\n",
      "Optimizing model...\n",
      "Step: 9, Action: 2, Reward: 0.25, Total Reward: 6.5, Loss: 2.8617453575134277\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 10, Action: 3, Reward: 0.25, Total Reward: 6.75, Loss: 4.543252468109131\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 11, Action: 3, Reward: 0.25, Total Reward: 7.0, Loss: 5.441500663757324\n",
      "Model acting\n",
      "\n",
      "episode: 226, reward: -93.0\n",
      "\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 3, Reward: 0.25, Total Reward: 0.25, Loss: 1.0745916366577148\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 1, Action: 3, Reward: -9.65, Total Reward: -9.4, Loss: 1.3295722007751465\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 3, Reward: -9.75, Total Reward: -19.15, Loss: 4.825085639953613\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 3, Reward: 0.25, Total Reward: -18.9, Loss: 5.3756608963012695\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 4, Action: 3, Reward: -9.75, Total Reward: -28.65, Loss: 2.8964040279388428\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 5, Action: 3, Reward: -9.75, Total Reward: -38.4, Loss: 1.2904937267303467\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 6, Action: 0, Reward: 2.0, Total Reward: -36.4, Loss: 5.68354606628418\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 7, Action: 0, Reward: -8.0, Total Reward: -44.4, Loss: 4.658345699310303\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 8, Action: 0, Reward: 2.0, Total Reward: -42.4, Loss: 2.367227077484131\n",
      "Optimizing model...\n",
      "Step: 9, Action: 1, Reward: 0.0, Total Reward: -42.4, Loss: 2.4516801834106445\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 10, Action: 3, Reward: 0.25, Total Reward: -42.15, Loss: 3.8096933364868164\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 11, Action: 3, Reward: -9.75, Total Reward: -51.9, Loss: 5.974624156951904\n",
      "Optimizing model...\n",
      "Step: 12, Action: 2, Reward: 0.25, Total Reward: -51.65, Loss: 1.469351053237915\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 13, Action: 3, Reward: 0.25, Total Reward: -51.4, Loss: 1.553347110748291\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 14, Action: 3, Reward: -9.75, Total Reward: -61.15, Loss: 6.4696125984191895\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 15, Action: 3, Reward: -9.75, Total Reward: -70.9, Loss: 9.072492599487305\n",
      "Model acting\n",
      "\n",
      "episode: 227, reward: -170.9\n",
      "\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 3, Reward: 0.25, Total Reward: 0.25, Loss: 2.903398036956787\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 1, Action: 3, Reward: 0.25, Total Reward: 0.5, Loss: 4.36125373840332\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 3, Reward: 0.25, Total Reward: 0.75, Loss: 7.483422756195068\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: 2.0, Total Reward: 2.75, Loss: 12.118977546691895\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: -8.0, Total Reward: -5.25, Loss: 2.468972682952881\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 5, Action: 0, Reward: 2.0, Total Reward: -3.25, Loss: 6.4065656661987305\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 6, Action: 0, Reward: 2.0, Total Reward: -1.25, Loss: 5.225276947021484\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 7, Action: 0, Reward: -8.0, Total Reward: -9.25, Loss: 8.704113006591797\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 8, Action: 3, Reward: 0.35, Total Reward: -8.9, Loss: 7.273373603820801\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 9, Action: 1, Reward: 0.0, Total Reward: -8.9, Loss: 6.3352813720703125\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 10, Action: 1, Reward: -10.0, Total Reward: -18.9, Loss: 3.830075263977051\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 11, Action: 1, Reward: -10.0, Total Reward: -28.9, Loss: 7.278592586517334\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 12, Action: 1, Reward: 0.0, Total Reward: -28.9, Loss: 4.302312850952148\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 13, Action: 1, Reward: 0.0, Total Reward: -28.9, Loss: 6.072943210601807\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 14, Action: 1, Reward: 0.0, Total Reward: -28.9, Loss: 6.175553321838379\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 15, Action: 0, Reward: 2.0, Total Reward: -26.9, Loss: 2.4369611740112305\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 16, Action: 0, Reward: -8.0, Total Reward: -34.9, Loss: 9.368675231933594\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 17, Action: 0, Reward: -8.0, Total Reward: -42.9, Loss: 5.99458122253418\n",
      "Optimizing model...\n",
      "Step: 18, Action: 3, Reward: 0.25, Total Reward: -42.65, Loss: 4.3069233894348145\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 19, Action: 0, Reward: 2.0, Total Reward: -40.65, Loss: 2.6543171405792236\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 20, Action: 0, Reward: -8.0, Total Reward: -48.65, Loss: 2.7828354835510254\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 21, Action: 0, Reward: -8.0, Total Reward: -56.65, Loss: 4.588009834289551\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 22, Action: 0, Reward: -8.0, Total Reward: -64.65, Loss: 5.611781120300293\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 23, Action: 3, Reward: 0.25, Total Reward: -64.4, Loss: 4.218686103820801\n",
      "Model acting\n",
      "\n",
      "episode: 228, reward: -164.4\n",
      "\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 3, Reward: 0.25, Total Reward: 0.25, Loss: 6.434932708740234\n",
      "Optimizing model...\n",
      "Step: 1, Action: 1, Reward: 0.0, Total Reward: 0.25, Loss: 7.268013000488281\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 3, Reward: 0.25, Total Reward: 0.5, Loss: 1.315277099609375\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: 2.0, Total Reward: 2.5, Loss: 5.287647724151611\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: -8.0, Total Reward: -5.5, Loss: 7.844211578369141\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 5, Action: 0, Reward: -8.0, Total Reward: -13.5, Loss: 4.23238468170166\n",
      "Optimizing model...\n",
      "Step: 6, Action: 4, Reward: 0.2, Total Reward: -13.3, Loss: 5.540504455566406\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 7, Action: 0, Reward: 2.0, Total Reward: -11.3, Loss: 5.428362846374512\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 8, Action: 0, Reward: -8.0, Total Reward: -19.3, Loss: 3.5494470596313477\n",
      "Optimizing model...\n",
      "Step: 9, Action: 1, Reward: 0.0, Total Reward: -19.3, Loss: 5.719167232513428\n",
      "Optimizing model...\n",
      "Step: 10, Action: 1, Reward: -10.0, Total Reward: -29.3, Loss: 2.794949769973755\n",
      "Model acting\n",
      "\n",
      "episode: 229, reward: -129.3\n",
      "\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: 2.0, Total Reward: 2.0, Loss: 5.9241485595703125\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: 2.0, Total Reward: 4.0, Loss: 5.735868453979492\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: 2.0, Total Reward: 6.0, Loss: 3.220054864883423\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: -8.0, Total Reward: -2.0, Loss: 4.397846221923828\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: 2.0, Total Reward: 0.0, Loss: 4.3071489334106445\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 5, Action: 0, Reward: 2.1, Total Reward: 2.1, Loss: 7.338869094848633\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 6, Action: 0, Reward: 2.0, Total Reward: 4.1, Loss: 1.2244292497634888\n",
      "Optimizing model...\n",
      "Step: 7, Action: 4, Reward: 0.2, Total Reward: 4.3, Loss: 5.977801322937012\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 8, Action: 0, Reward: 2.0, Total Reward: 6.3, Loss: 3.076056957244873\n",
      "Model acting\n",
      "\n",
      "episode: 230, reward: -93.7\n",
      "\n",
      "Model saved\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: 2.0, Total Reward: 2.0, Loss: 5.150697231292725\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: -8.0, Total Reward: -6.0, Loss: 5.103714942932129\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: 2.0, Total Reward: -4.0, Loss: 2.79066801071167\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: -8.0, Total Reward: -12.0, Loss: 3.827427387237549\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: -8.0, Total Reward: -20.0, Loss: 5.460460662841797\n",
      "Optimizing model...\n",
      "Step: 5, Action: 0, Reward: -8.0, Total Reward: -28.0, Loss: 6.372773170471191\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 6, Action: 0, Reward: -8.0, Total Reward: -36.0, Loss: 4.117804527282715\n",
      "Optimizing model...\n",
      "Step: 7, Action: 3, Reward: 0.25, Total Reward: -35.75, Loss: 4.373593807220459\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 8, Action: 0, Reward: 2.0, Total Reward: -33.75, Loss: 6.18387508392334\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 9, Action: 0, Reward: -8.0, Total Reward: -41.75, Loss: 3.2718701362609863\n",
      "Model acting\n",
      "\n",
      "episode: 231, reward: -141.75\n",
      "\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: -8.0, Total Reward: -8.0, Loss: 5.314022541046143\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: -7.9, Total Reward: -15.9, Loss: 7.090025424957275\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: -8.0, Total Reward: -23.9, Loss: 4.7285261154174805\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: -8.0, Total Reward: -31.9, Loss: 7.561712265014648\n",
      "\n",
      "episode: 232, reward: -131.9\n",
      "\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: 2.0, Total Reward: 2.0, Loss: 6.7747015953063965\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: -8.0, Total Reward: -6.0, Loss: 5.8772711753845215\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: 2.0, Total Reward: -4.0, Loss: 1.4905774593353271\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: 2.0, Total Reward: -2.0, Loss: 5.8332624435424805\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: -8.0, Total Reward: -10.0, Loss: 8.37716293334961\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 5, Action: 0, Reward: 2.0, Total Reward: -8.0, Loss: 4.074154853820801\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 6, Action: 0, Reward: 2.0, Total Reward: -6.0, Loss: 6.55565071105957\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 7, Action: 0, Reward: 2.0, Total Reward: -4.0, Loss: 2.423424243927002\n",
      "Model acting\n",
      "\n",
      "episode: 233, reward: -104.0\n",
      "\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: -8.0, Total Reward: -8.0, Loss: 5.443026542663574\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: -8.0, Total Reward: -16.0, Loss: 4.445413112640381\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: 2.0, Total Reward: -14.0, Loss: 2.70463228225708\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: 2.0, Total Reward: -12.0, Loss: 2.6201229095458984\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: 2.0, Total Reward: -10.0, Loss: 5.153215408325195\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 5, Action: 0, Reward: 2.0, Total Reward: -8.0, Loss: 6.160737991333008\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 6, Action: 0, Reward: 2.0, Total Reward: -6.0, Loss: 5.724353790283203\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 7, Action: 0, Reward: 2.0, Total Reward: -4.0, Loss: 4.498960494995117\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 8, Action: 0, Reward: 2.1, Total Reward: -1.9, Loss: 3.4576165676116943\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 9, Action: 1, Reward: 0.0, Total Reward: -1.9, Loss: 6.613434791564941\n",
      "\n",
      "episode: 234, reward: -101.9\n",
      "\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 1, Reward: 0.0, Total Reward: 0.0, Loss: 7.631415367126465\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 1, Action: 1, Reward: -10.0, Total Reward: -10.0, Loss: 6.044801235198975\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 1, Reward: 0.0, Total Reward: -10.0, Loss: 6.217477321624756\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: 2.0, Total Reward: -8.0, Loss: 4.546494483947754\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 4, Action: 2, Reward: 0.25, Total Reward: -7.75, Loss: 6.175004959106445\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 5, Action: 0, Reward: 2.0, Total Reward: -5.75, Loss: 4.504444122314453\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 6, Action: 2, Reward: 0.35, Total Reward: -5.4, Loss: 7.688351631164551\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 7, Action: 0, Reward: 2.0, Total Reward: -3.4000000000000004, Loss: 5.495579719543457\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 8, Action: 2, Reward: 0.25, Total Reward: -3.1500000000000004, Loss: 7.516792297363281\n",
      "Optimizing model...\n",
      "Step: 9, Action: 0, Reward: 2.0, Total Reward: -1.1500000000000004, Loss: 2.07625675201416\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 10, Action: 2, Reward: 0.25, Total Reward: -0.9000000000000004, Loss: 5.0915327072143555\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 11, Action: 2, Reward: 0.25, Total Reward: -0.6500000000000004, Loss: 2.7449235916137695\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 12, Action: 2, Reward: 0.25, Total Reward: -0.40000000000000036, Loss: 4.212530136108398\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 13, Action: 2, Reward: 0.25, Total Reward: -0.15000000000000036, Loss: 3.2532095909118652\n",
      "Optimizing model...\n",
      "Step: 14, Action: 3, Reward: 0.25, Total Reward: 0.09999999999999964, Loss: 2.4572219848632812\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 15, Action: 2, Reward: 0.25, Total Reward: 0.34999999999999964, Loss: 4.077195644378662\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 16, Action: 2, Reward: -9.75, Total Reward: -9.4, Loss: 2.9594063758850098\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 17, Action: 2, Reward: -9.75, Total Reward: -19.15, Loss: 5.789772987365723\n",
      "Optimizing model...\n",
      "Step: 18, Action: 2, Reward: 0.25, Total Reward: -18.9, Loss: 2.951422691345215\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 19, Action: 2, Reward: 0.25, Total Reward: -18.65, Loss: 4.627408981323242\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 20, Action: 2, Reward: -9.75, Total Reward: -28.4, Loss: 4.54052734375\n",
      "Optimizing model...\n",
      "Step: 21, Action: 4, Reward: 0.3, Total Reward: -28.099999999999998, Loss: 2.839569091796875\n",
      "Optimizing model...\n",
      "Step: 22, Action: 1, Reward: 0.0, Total Reward: -28.099999999999998, Loss: 6.04918098449707\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 23, Action: 2, Reward: 0.25, Total Reward: -27.849999999999998, Loss: 4.59591007232666\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 24, Action: 2, Reward: -9.75, Total Reward: -37.599999999999994, Loss: 2.359194755554199\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 25, Action: 2, Reward: 0.25, Total Reward: -37.349999999999994, Loss: 6.340319633483887\n",
      "Optimizing model...\n",
      "Step: 26, Action: 2, Reward: 0.25, Total Reward: -37.099999999999994, Loss: 6.102579593658447\n",
      "Optimizing model...\n",
      "Step: 27, Action: 3, Reward: 0.25, Total Reward: -36.849999999999994, Loss: 2.5738096237182617\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 28, Action: 2, Reward: 0.25, Total Reward: -36.599999999999994, Loss: 2.270313262939453\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 29, Action: 2, Reward: -9.75, Total Reward: -46.349999999999994, Loss: 5.979659080505371\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 30, Action: 2, Reward: 0.25, Total Reward: -46.099999999999994, Loss: 2.657078504562378\n",
      "Model acting\n",
      "\n",
      "episode: 235, reward: -146.1\n",
      "\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 2, Reward: 0.25, Total Reward: 0.25, Loss: 6.85223388671875\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: 2.0, Total Reward: 2.25, Loss: 7.055698394775391\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 2, Reward: 0.35, Total Reward: 2.6, Loss: 4.003475189208984\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 2, Reward: -9.75, Total Reward: -7.15, Loss: 6.457443714141846\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: 2.0, Total Reward: -5.15, Loss: 3.6532907485961914\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 5, Action: 2, Reward: 0.35, Total Reward: -4.800000000000001, Loss: 4.190134525299072\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 6, Action: 2, Reward: -9.75, Total Reward: -14.55, Loss: 7.9423651695251465\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 7, Action: 0, Reward: 2.0, Total Reward: -12.55, Loss: 2.648458957672119\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 8, Action: 2, Reward: 0.25, Total Reward: -12.3, Loss: 4.07661247253418\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 9, Action: 2, Reward: 0.25, Total Reward: -12.05, Loss: 8.654056549072266\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 10, Action: 2, Reward: -9.75, Total Reward: -21.8, Loss: 5.938013076782227\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 11, Action: 2, Reward: 0.25, Total Reward: -21.55, Loss: 1.815476417541504\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 12, Action: 2, Reward: 0.25, Total Reward: -21.3, Loss: 6.832400321960449\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 13, Action: 2, Reward: 0.25, Total Reward: -21.05, Loss: 7.020581245422363\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 14, Action: 2, Reward: -9.75, Total Reward: -30.8, Loss: 6.278552055358887\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 15, Action: 2, Reward: -9.75, Total Reward: -40.55, Loss: 5.479905128479004\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 16, Action: 2, Reward: -9.75, Total Reward: -50.3, Loss: 1.5386122465133667\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 17, Action: 2, Reward: -9.65, Total Reward: -59.949999999999996, Loss: 4.366318702697754\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 18, Action: 2, Reward: 0.25, Total Reward: -59.699999999999996, Loss: 5.504241943359375\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 19, Action: 2, Reward: 0.25, Total Reward: -59.449999999999996, Loss: 1.0970433950424194\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 20, Action: 2, Reward: -9.65, Total Reward: -69.1, Loss: 3.905526638031006\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 21, Action: 2, Reward: -9.75, Total Reward: -78.85, Loss: 3.471328020095825\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 22, Action: 2, Reward: -9.75, Total Reward: -88.6, Loss: 5.90167760848999\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 23, Action: 2, Reward: 0.35, Total Reward: -88.25, Loss: 8.763867378234863\n",
      "Optimizing model...\n",
      "Step: 24, Action: 0, Reward: 2.0, Total Reward: -86.25, Loss: 4.640395641326904\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 25, Action: 0, Reward: 2.0, Total Reward: -84.25, Loss: 2.397332191467285\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 26, Action: 2, Reward: 0.25, Total Reward: -84.0, Loss: 8.44101333618164\n",
      "Optimizing model...\n",
      "Step: 27, Action: 3, Reward: 0.25, Total Reward: -83.75, Loss: 4.448622226715088\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 28, Action: 2, Reward: 0.25, Total Reward: -83.5, Loss: 5.686070919036865\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 29, Action: 0, Reward: 2.0, Total Reward: -81.5, Loss: 5.910747528076172\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 30, Action: 0, Reward: 2.0, Total Reward: -79.5, Loss: 6.3694963455200195\n",
      "Model acting\n",
      "\n",
      "episode: 236, reward: -179.5\n",
      "\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: 2.0, Total Reward: 2.0, Loss: 4.846295356750488\n",
      "Optimizing model...\n",
      "Step: 1, Action: 1, Reward: 0.0, Total Reward: 2.0, Loss: 5.653496265411377\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: 2.1, Total Reward: 4.1, Loss: 7.7990570068359375\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: 2.0, Total Reward: 6.1, Loss: 7.203001976013184\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: 2.0, Total Reward: 8.1, Loss: 0.984028160572052\n",
      "Optimizing model...\n",
      "Step: 5, Action: 0, Reward: 2.0, Total Reward: 10.1, Loss: 5.972007751464844\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 6, Action: 2, Reward: 0.25, Total Reward: 10.35, Loss: 2.924426794052124\n",
      "Model acting\n",
      "\n",
      "episode: 237, reward: -89.65\n",
      "\n",
      "Optimizing model...\n",
      "Step: 0, Action: 4, Reward: 0.2, Total Reward: 0.2, Loss: 10.776830673217773\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 1, Action: 2, Reward: 0.25, Total Reward: 0.45, Loss: 4.554919719696045\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: 2.0, Total Reward: 2.45, Loss: 6.91457986831665\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: 2.0, Total Reward: 4.45, Loss: 1.3378664255142212\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: -8.0, Total Reward: -3.55, Loss: 5.7316107749938965\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 5, Action: 0, Reward: -8.0, Total Reward: -11.55, Loss: 8.807440757751465\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 6, Action: 0, Reward: 2.0, Total Reward: -9.55, Loss: 15.633749008178711\n",
      "Optimizing model...\n",
      "Step: 7, Action: 2, Reward: 0.25, Total Reward: -9.3, Loss: 4.53056526184082\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 8, Action: 0, Reward: 2.0, Total Reward: -7.300000000000001, Loss: 5.002107620239258\n",
      "Optimizing model...\n",
      "Step: 9, Action: 2, Reward: 0.25, Total Reward: -7.050000000000001, Loss: 7.332217216491699\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 10, Action: 0, Reward: 2.0, Total Reward: -5.050000000000001, Loss: 3.913084030151367\n",
      "\n",
      "episode: 238, reward: -105.05\n",
      "\n",
      "Optimizing model...\n",
      "Step: 0, Action: 4, Reward: 0.2, Total Reward: 0.2, Loss: 10.623674392700195\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: 2.0, Total Reward: 2.2, Loss: 4.326154708862305\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: 2.0, Total Reward: 4.2, Loss: 2.2310335636138916\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: 2.0, Total Reward: 6.2, Loss: 7.939687728881836\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: -8.0, Total Reward: -1.7999999999999998, Loss: 5.2797136306762695\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 5, Action: 0, Reward: -8.0, Total Reward: -9.8, Loss: 4.536462306976318\n",
      "Optimizing model...\n",
      "Step: 6, Action: 3, Reward: 0.25, Total Reward: -9.55, Loss: 1.2081818580627441\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 7, Action: 0, Reward: 2.0, Total Reward: -7.550000000000001, Loss: 2.544339418411255\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 8, Action: 0, Reward: -8.0, Total Reward: -15.55, Loss: 2.5617079734802246\n",
      "\n",
      "episode: 239, reward: -115.55\n",
      "\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: -8.0, Total Reward: -8.0, Loss: 5.571597099304199\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: 2.0, Total Reward: -6.0, Loss: 4.056669235229492\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: -8.0, Total Reward: -14.0, Loss: 4.841483116149902\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: -7.8, Total Reward: -21.8, Loss: 7.468149662017822\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: -8.0, Total Reward: -29.8, Loss: 1.1425315141677856\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 5, Action: 0, Reward: -8.0, Total Reward: -37.8, Loss: 3.0572359561920166\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 6, Action: 0, Reward: 2.0, Total Reward: -35.8, Loss: 2.4860401153564453\n",
      "Optimizing model...\n",
      "Step: 7, Action: 0, Reward: 2.0, Total Reward: -33.8, Loss: 2.7942261695861816\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 8, Action: 0, Reward: 2.0, Total Reward: -31.799999999999997, Loss: 3.9059107303619385\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 9, Action: 0, Reward: 2.1, Total Reward: -29.699999999999996, Loss: 2.5796127319335938\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 10, Action: 0, Reward: 2.0, Total Reward: -27.699999999999996, Loss: 4.609655380249023\n",
      "Model acting\n",
      "\n",
      "episode: 240, reward: -127.69999999999999\n",
      "\n",
      "Model saved\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: 2.0, Total Reward: 2.0, Loss: 4.361388206481934\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: -8.0, Total Reward: -6.0, Loss: 2.6504180431365967\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: 2.0, Total Reward: -4.0, Loss: 3.056279182434082\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: 2.0, Total Reward: -2.0, Loss: 3.5259642601013184\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: -8.0, Total Reward: -10.0, Loss: 3.0321078300476074\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 5, Action: 0, Reward: 2.0, Total Reward: -8.0, Loss: 7.005545139312744\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 6, Action: 0, Reward: 2.0, Total Reward: -6.0, Loss: 6.502816677093506\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 7, Action: 0, Reward: 2.0, Total Reward: -4.0, Loss: 5.2097272872924805\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 8, Action: 0, Reward: 2.0, Total Reward: -2.0, Loss: 0.9519789218902588\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 9, Action: 0, Reward: -8.0, Total Reward: -10.0, Loss: 1.8366587162017822\n",
      "Model acting\n",
      "\n",
      "episode: 241, reward: -110.0\n",
      "\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: 2.0, Total Reward: 2.0, Loss: 4.9449849128723145\n",
      "Optimizing model...\n",
      "Step: 1, Action: 4, Reward: 0.2, Total Reward: 2.2, Loss: 9.987953186035156\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: 2.1, Total Reward: 4.300000000000001, Loss: 7.211526393890381\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: -8.0, Total Reward: -3.6999999999999993, Loss: 5.825402736663818\n",
      "Optimizing model...\n",
      "Step: 4, Action: 1, Reward: 0.1, Total Reward: -3.599999999999999, Loss: 2.9998250007629395\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 5, Action: 0, Reward: 2.0, Total Reward: -1.5999999999999992, Loss: 4.218791961669922\n",
      "Optimizing model...\n",
      "Step: 6, Action: 4, Reward: 0.2, Total Reward: -1.3999999999999992, Loss: 1.7540123462677002\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 7, Action: 0, Reward: 2.0, Total Reward: 0.6000000000000008, Loss: 6.353390216827393\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 8, Action: 0, Reward: 2.0, Total Reward: 2.6000000000000005, Loss: 5.974909782409668\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 9, Action: 0, Reward: -8.0, Total Reward: -5.3999999999999995, Loss: 4.821561813354492\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 10, Action: 0, Reward: 2.0, Total Reward: -3.3999999999999995, Loss: 6.086119651794434\n",
      "Model acting\n",
      "\n",
      "episode: 242, reward: -103.4\n",
      "\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: 2.0, Total Reward: 2.0, Loss: 4.449896335601807\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: -8.0, Total Reward: -6.0, Loss: 7.5847649574279785\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: 2.0, Total Reward: -4.0, Loss: 9.031081199645996\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: -8.0, Total Reward: -12.0, Loss: 4.41094446182251\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: 2.0, Total Reward: -10.0, Loss: 6.814438343048096\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 5, Action: 0, Reward: -8.0, Total Reward: -18.0, Loss: 5.514773368835449\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 6, Action: 0, Reward: 2.0, Total Reward: -16.0, Loss: 5.131781101226807\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 7, Action: 0, Reward: -8.0, Total Reward: -24.0, Loss: 4.608163833618164\n",
      "Optimizing model...\n",
      "Step: 8, Action: 2, Reward: 0.25, Total Reward: -23.75, Loss: 4.16002082824707\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 9, Action: 0, Reward: 2.0, Total Reward: -21.75, Loss: 3.049241304397583\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 10, Action: 0, Reward: 2.0, Total Reward: -19.75, Loss: 4.019730567932129\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 11, Action: 0, Reward: -8.0, Total Reward: -27.75, Loss: 3.242638111114502\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 12, Action: 0, Reward: -8.0, Total Reward: -35.75, Loss: 2.1509408950805664\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 13, Action: 0, Reward: 2.0, Total Reward: -33.75, Loss: 1.4650553464889526\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 14, Action: 0, Reward: -8.0, Total Reward: -41.75, Loss: 8.127176284790039\n",
      "Optimizing model...\n",
      "Step: 15, Action: 1, Reward: 0.0, Total Reward: -41.75, Loss: 2.7312159538269043\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 16, Action: 0, Reward: 2.0, Total Reward: -39.75, Loss: 8.212882995605469\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 17, Action: 0, Reward: 2.0, Total Reward: -37.75, Loss: 6.230157852172852\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 18, Action: 0, Reward: -8.0, Total Reward: -45.75, Loss: 5.109780311584473\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 19, Action: 0, Reward: 2.0, Total Reward: -43.75, Loss: 4.63454008102417\n",
      "Model acting\n",
      "\n",
      "episode: 243, reward: -143.75\n",
      "\n",
      "Optimizing model...\n",
      "Step: 0, Action: 1, Reward: 0.0, Total Reward: 0.0, Loss: 2.9866788387298584\n",
      "Optimizing model...\n",
      "Step: 1, Action: 1, Reward: -10.0, Total Reward: -10.0, Loss: 4.358379364013672\n",
      "Optimizing model...\n",
      "Step: 2, Action: 3, Reward: 0.45, Total Reward: -9.55, Loss: 5.385283470153809\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: 2.0, Total Reward: -7.550000000000001, Loss: 8.26549243927002\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 4, Action: 1, Reward: 0.0, Total Reward: -7.550000000000001, Loss: 6.54826545715332\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 5, Action: 1, Reward: -10.0, Total Reward: -17.55, Loss: 2.430319309234619\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 6, Action: 1, Reward: 0.0, Total Reward: -17.55, Loss: 4.469871997833252\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 7, Action: 3, Reward: 0.25, Total Reward: -17.3, Loss: 6.283373832702637\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 8, Action: 3, Reward: -9.75, Total Reward: -27.05, Loss: 6.673332214355469\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 9, Action: 3, Reward: -9.75, Total Reward: -36.8, Loss: 4.351884365081787\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 10, Action: 3, Reward: -9.75, Total Reward: -46.55, Loss: 4.654106616973877\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 11, Action: 3, Reward: -9.75, Total Reward: -56.3, Loss: 6.974821090698242\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 12, Action: 3, Reward: -9.75, Total Reward: -66.05, Loss: 7.704582214355469\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 13, Action: 3, Reward: -9.75, Total Reward: -75.8, Loss: 9.037681579589844\n",
      "Model acting\n",
      "\n",
      "episode: 244, reward: -175.8\n",
      "\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 3, Reward: 0.25, Total Reward: 0.25, Loss: 5.52107048034668\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 1, Action: 3, Reward: 0.25, Total Reward: 0.5, Loss: 7.038211822509766\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 3, Reward: -9.75, Total Reward: -9.25, Loss: 4.542387962341309\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 1, Reward: 0.0, Total Reward: -9.25, Loss: 7.35769510269165\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 4, Action: 1, Reward: -10.0, Total Reward: -19.25, Loss: 12.40597152709961\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 5, Action: 0, Reward: 2.0, Total Reward: -17.25, Loss: 4.3709893226623535\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 6, Action: 0, Reward: -8.0, Total Reward: -25.25, Loss: 2.7110533714294434\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 7, Action: 0, Reward: 2.1, Total Reward: -23.15, Loss: 8.03487777709961\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 8, Action: 2, Reward: 0.25, Total Reward: -22.9, Loss: 8.26862621307373\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 9, Action: 2, Reward: -9.75, Total Reward: -32.65, Loss: 1.0754517316818237\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 10, Action: 2, Reward: -9.75, Total Reward: -42.4, Loss: 6.927023410797119\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 11, Action: 2, Reward: -9.75, Total Reward: -52.15, Loss: 2.885521411895752\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 12, Action: 2, Reward: 0.25, Total Reward: -51.9, Loss: 2.6918396949768066\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 13, Action: 3, Reward: 0.25, Total Reward: -51.65, Loss: 2.7407400608062744\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 14, Action: 2, Reward: 0.25, Total Reward: -51.4, Loss: 3.831094264984131\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 15, Action: 3, Reward: 0.25, Total Reward: -51.15, Loss: 8.910720825195312\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 16, Action: 2, Reward: 0.25, Total Reward: -50.9, Loss: 4.184930801391602\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 17, Action: 3, Reward: 0.25, Total Reward: -50.65, Loss: 5.6589035987854\n",
      "Optimizing model...\n",
      "Step: 18, Action: 1, Reward: 0.0, Total Reward: -50.65, Loss: 5.905026912689209\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 19, Action: 3, Reward: 0.35, Total Reward: -50.3, Loss: 3.0087180137634277\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 20, Action: 3, Reward: -9.65, Total Reward: -59.949999999999996, Loss: 1.1450812816619873\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 21, Action: 3, Reward: 0.25, Total Reward: -59.699999999999996, Loss: 4.155368804931641\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 22, Action: 3, Reward: 0.25, Total Reward: -59.449999999999996, Loss: 5.010871410369873\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 23, Action: 2, Reward: 0.25, Total Reward: -59.199999999999996, Loss: 5.930665493011475\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 24, Action: 0, Reward: 2.0, Total Reward: -57.199999999999996, Loss: 1.0021371841430664\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 25, Action: 0, Reward: 2.1, Total Reward: -55.099999999999994, Loss: 4.484318733215332\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 26, Action: 0, Reward: 2.0, Total Reward: -53.099999999999994, Loss: 5.590142250061035\n",
      "Model acting\n",
      "\n",
      "episode: 245, reward: -153.1\n",
      "\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: -8.0, Total Reward: -8.0, Loss: 7.777298450469971\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: -8.0, Total Reward: -16.0, Loss: 3.740659236907959\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: 2.0, Total Reward: -14.0, Loss: 3.791706085205078\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 1, Reward: 0.0, Total Reward: -14.0, Loss: 0.507880449295044\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: 2.1, Total Reward: -11.9, Loss: 1.4836089611053467\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 5, Action: 0, Reward: -8.0, Total Reward: -19.9, Loss: 6.951062202453613\n",
      "Model acting\n",
      "\n",
      "episode: 246, reward: -119.9\n",
      "\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 4, Reward: 0.2, Total Reward: 0.2, Loss: 3.256730079650879\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 1, Action: 4, Reward: 0.2, Total Reward: 0.4, Loss: 2.2454867362976074\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 4, Reward: 0.2, Total Reward: 0.6000000000000001, Loss: 9.688117027282715\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 4, Reward: -9.8, Total Reward: -9.200000000000001, Loss: 7.333763599395752\n",
      "Optimizing model...\n",
      "Step: 4, Action: 2, Reward: 0.25, Total Reward: -8.950000000000001, Loss: 2.849527359008789\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 5, Action: 4, Reward: 0.2, Total Reward: -8.750000000000002, Loss: 4.291844367980957\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 6, Action: 4, Reward: -9.8, Total Reward: -18.550000000000004, Loss: 3.0105090141296387\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 7, Action: 4, Reward: -9.8, Total Reward: -28.350000000000005, Loss: 2.7958409786224365\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 8, Action: 4, Reward: -9.8, Total Reward: -38.150000000000006, Loss: 6.336416244506836\n",
      "Optimizing model...\n",
      "Step: 9, Action: 4, Reward: -9.8, Total Reward: -47.95, Loss: 2.613846778869629\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 10, Action: 0, Reward: 2.0, Total Reward: -45.95, Loss: 4.566003799438477\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 11, Action: 0, Reward: 2.0, Total Reward: -43.95, Loss: 1.993988275527954\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 12, Action: 0, Reward: 2.0, Total Reward: -41.95, Loss: 5.847351551055908\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 13, Action: 0, Reward: -8.0, Total Reward: -49.95, Loss: 5.921055316925049\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 14, Action: 0, Reward: -8.0, Total Reward: -57.95, Loss: 2.7526211738586426\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 15, Action: 0, Reward: 2.0, Total Reward: -55.95, Loss: 7.030840873718262\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 16, Action: 0, Reward: 2.0, Total Reward: -53.95, Loss: 5.587573528289795\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 17, Action: 0, Reward: 2.0, Total Reward: -51.95, Loss: 2.7468340396881104\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 18, Action: 0, Reward: 2.0, Total Reward: -49.95, Loss: 4.719124794006348\n",
      "Optimizing model...\n",
      "Step: 19, Action: 2, Reward: 0.25, Total Reward: -49.7, Loss: 2.150712728500366\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 20, Action: 0, Reward: 2.0, Total Reward: -47.7, Loss: 2.861199378967285\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 21, Action: 0, Reward: 2.0, Total Reward: -45.7, Loss: 2.394763946533203\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 22, Action: 0, Reward: -8.0, Total Reward: -53.7, Loss: 10.376659393310547\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 23, Action: 0, Reward: -8.0, Total Reward: -61.7, Loss: 2.243267297744751\n",
      "Optimizing model...\n",
      "Step: 24, Action: 2, Reward: 0.35, Total Reward: -61.35, Loss: 11.779144287109375\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 25, Action: 0, Reward: 2.0, Total Reward: -59.35, Loss: 3.037292957305908\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 26, Action: 0, Reward: 2.0, Total Reward: -57.35, Loss: 4.99130392074585\n",
      "Model acting\n",
      "\n",
      "episode: 247, reward: -157.35\n",
      "\n",
      "Optimizing model...\n",
      "Step: 0, Action: 4, Reward: 0.2, Total Reward: 0.2, Loss: 3.4360060691833496\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: 2.0, Total Reward: 2.2, Loss: 5.724271297454834\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: 2.1, Total Reward: 4.300000000000001, Loss: 4.292013168334961\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: -8.0, Total Reward: -3.6999999999999993, Loss: 3.464681625366211\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: 2.0, Total Reward: -1.6999999999999993, Loss: 0.8692398071289062\n",
      "Optimizing model...\n",
      "Step: 5, Action: 0, Reward: 2.0, Total Reward: 0.3000000000000007, Loss: 2.8177835941314697\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 6, Action: 2, Reward: 0.25, Total Reward: 0.5500000000000007, Loss: 7.6541643142700195\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 7, Action: 2, Reward: 0.25, Total Reward: 0.8000000000000007, Loss: 3.193718671798706\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 8, Action: 2, Reward: 0.25, Total Reward: 1.0500000000000007, Loss: 2.9882233142852783\n",
      "Optimizing model...\n",
      "Step: 9, Action: 3, Reward: 0.25, Total Reward: 1.3000000000000007, Loss: 4.034522533416748\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 10, Action: 2, Reward: 0.25, Total Reward: 1.5500000000000007, Loss: 6.784482955932617\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 11, Action: 2, Reward: 0.25, Total Reward: 1.8000000000000007, Loss: 9.291887283325195\n",
      "Optimizing model...\n",
      "Step: 12, Action: 3, Reward: 0.25, Total Reward: 2.0500000000000007, Loss: 4.704685211181641\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 13, Action: 2, Reward: 0.25, Total Reward: 2.3000000000000007, Loss: 6.091102600097656\n",
      "Optimizing model...\n",
      "Step: 14, Action: 2, Reward: 0.35, Total Reward: 2.650000000000001, Loss: 7.225192070007324\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 15, Action: 2, Reward: -9.75, Total Reward: -7.1, Loss: 2.3153820037841797\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 16, Action: 2, Reward: -9.75, Total Reward: -16.85, Loss: 5.593686580657959\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 17, Action: 2, Reward: 0.35, Total Reward: -16.5, Loss: 4.27483606338501\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 18, Action: 2, Reward: 0.25, Total Reward: -16.25, Loss: 5.616652011871338\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 19, Action: 2, Reward: -9.65, Total Reward: -25.9, Loss: 4.239274978637695\n",
      "Optimizing model...\n",
      "Step: 20, Action: 4, Reward: 0.2, Total Reward: -25.7, Loss: 10.568756103515625\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 21, Action: 2, Reward: 0.25, Total Reward: -25.45, Loss: 3.806183099746704\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 22, Action: 2, Reward: 0.25, Total Reward: -25.2, Loss: 4.214987754821777\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 23, Action: 2, Reward: -9.75, Total Reward: -34.95, Loss: 3.6318540573120117\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 24, Action: 2, Reward: 0.25, Total Reward: -34.7, Loss: 6.282156944274902\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 25, Action: 2, Reward: 0.35, Total Reward: -34.35, Loss: 7.19011926651001\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 26, Action: 2, Reward: 0.25, Total Reward: -34.1, Loss: 8.483664512634277\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 27, Action: 2, Reward: -9.75, Total Reward: -43.85, Loss: 7.829080104827881\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 28, Action: 0, Reward: 2.1, Total Reward: -41.75, Loss: 6.3156328201293945\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 29, Action: 0, Reward: 2.0, Total Reward: -39.75, Loss: 6.265048980712891\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 30, Action: 0, Reward: 2.0, Total Reward: -37.75, Loss: 4.268786907196045\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 31, Action: 0, Reward: 2.0, Total Reward: -35.75, Loss: 4.4974822998046875\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 32, Action: 0, Reward: 2.0, Total Reward: -33.75, Loss: 2.7008657455444336\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 33, Action: 0, Reward: 2.0, Total Reward: -31.75, Loss: 4.392475128173828\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 34, Action: 2, Reward: 0.25, Total Reward: -31.5, Loss: 7.205193042755127\n",
      "Model acting\n",
      "\n",
      "episode: 248, reward: -131.5\n",
      "\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 1, Reward: 0.0, Total Reward: 0.0, Loss: 4.772132873535156\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: 2.0, Total Reward: 2.0, Loss: 4.716702938079834\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: 2.0, Total Reward: 4.0, Loss: 2.9903807640075684\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 1, Reward: 0.0, Total Reward: 4.0, Loss: 4.442183494567871\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 4, Action: 2, Reward: 0.25, Total Reward: 4.25, Loss: 1.832542896270752\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 5, Action: 2, Reward: -9.75, Total Reward: -5.5, Loss: 2.9944722652435303\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 6, Action: 2, Reward: 0.25, Total Reward: -5.25, Loss: 5.459045886993408\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 7, Action: 0, Reward: 2.0, Total Reward: -3.25, Loss: 3.0528345108032227\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 8, Action: 0, Reward: 2.0, Total Reward: -1.25, Loss: 3.4760990142822266\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 9, Action: 0, Reward: 2.0, Total Reward: 0.75, Loss: 4.7072834968566895\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 10, Action: 2, Reward: 0.25, Total Reward: 1.0, Loss: 4.161287307739258\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 11, Action: 0, Reward: 2.1, Total Reward: 3.1, Loss: 11.449119567871094\n",
      "\n",
      "episode: 249, reward: -96.9\n",
      "\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: 2.0, Total Reward: 2.0, Loss: 1.1473972797393799\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: 2.0, Total Reward: 4.0, Loss: 4.162928581237793\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: -8.0, Total Reward: -4.0, Loss: 4.178251266479492\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: -8.0, Total Reward: -12.0, Loss: 7.691985130310059\n",
      "Model acting\n",
      "\n",
      "episode: 250, reward: -112.0\n",
      "\n",
      "Model saved\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: -8.0, Total Reward: -8.0, Loss: 9.961400985717773\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: 2.0, Total Reward: -6.0, Loss: 4.191720008850098\n",
      "Optimizing model...\n",
      "Step: 2, Action: 2, Reward: 0.25, Total Reward: -5.75, Loss: 1.7872130870819092\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: 2.0, Total Reward: -3.75, Loss: 4.1578803062438965\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: 2.0, Total Reward: -1.75, Loss: 3.226480722427368\n",
      "Optimizing model...\n",
      "Step: 5, Action: 3, Reward: 0.25, Total Reward: -1.5, Loss: 1.9849475622177124\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 6, Action: 0, Reward: 2.0, Total Reward: 0.5, Loss: 5.628481388092041\n",
      "Optimizing model...\n",
      "Step: 7, Action: 4, Reward: 0.2, Total Reward: 0.7, Loss: 2.731640338897705\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 8, Action: 0, Reward: 2.0, Total Reward: 2.7, Loss: 3.162731647491455\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 9, Action: 0, Reward: 2.0, Total Reward: 4.7, Loss: 6.8590803146362305\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 10, Action: 0, Reward: 2.0, Total Reward: 6.7, Loss: 4.149381637573242\n",
      "Model acting\n",
      "\n",
      "episode: 251, reward: -93.3\n",
      "\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: -8.0, Total Reward: -8.0, Loss: 8.493553161621094\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: 2.0, Total Reward: -6.0, Loss: 8.027402877807617\n",
      "Optimizing model...\n",
      "Step: 2, Action: 4, Reward: 0.2, Total Reward: -5.8, Loss: 8.616109848022461\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: 2.0, Total Reward: -3.8, Loss: 5.94336462020874\n",
      "Optimizing model...\n",
      "Step: 4, Action: 4, Reward: 0.2, Total Reward: -3.5999999999999996, Loss: 3.562326192855835\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 5, Action: 0, Reward: 2.0, Total Reward: -1.5999999999999996, Loss: 4.16822624206543\n",
      "Optimizing model...\n",
      "Step: 6, Action: 4, Reward: 0.2, Total Reward: -1.3999999999999997, Loss: 6.248499870300293\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 7, Action: 0, Reward: 2.0, Total Reward: 0.6000000000000003, Loss: 5.762603759765625\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 8, Action: 0, Reward: 2.0, Total Reward: 2.6000000000000005, Loss: 5.790560722351074\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 9, Action: 0, Reward: 2.0, Total Reward: 4.6000000000000005, Loss: 7.361870765686035\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 10, Action: 0, Reward: 2.0, Total Reward: 6.6000000000000005, Loss: 5.791989803314209\n",
      "\n",
      "episode: 252, reward: -93.4\n",
      "\n",
      "Optimizing model...\n",
      "Step: 0, Action: 0, Reward: 2.0, Total Reward: 2.0, Loss: 4.991037845611572\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 1, Action: 0, Reward: -8.0, Total Reward: -6.0, Loss: 3.73956561088562\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 2, Action: 0, Reward: 2.0, Total Reward: -4.0, Loss: 4.438329696655273\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 3, Action: 0, Reward: -8.0, Total Reward: -12.0, Loss: 1.3620729446411133\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 4, Action: 0, Reward: 2.0, Total Reward: -10.0, Loss: 4.170160293579102\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 5, Action: 0, Reward: -7.9, Total Reward: -17.9, Loss: 4.818556785583496\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 6, Action: 0, Reward: 2.0, Total Reward: -15.899999999999999, Loss: 5.460522651672363\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 7, Action: 0, Reward: -8.0, Total Reward: -23.9, Loss: 2.6999478340148926\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 8, Action: 0, Reward: -8.0, Total Reward: -31.9, Loss: 6.746381759643555\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 9, Action: 0, Reward: 2.0, Total Reward: -29.9, Loss: 5.794806480407715\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 10, Action: 0, Reward: 2.0, Total Reward: -27.9, Loss: 3.1506905555725098\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 11, Action: 0, Reward: 2.0, Total Reward: -25.9, Loss: 6.447537899017334\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 12, Action: 0, Reward: 2.1, Total Reward: -23.799999999999997, Loss: 4.795904636383057\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 13, Action: 0, Reward: 2.0, Total Reward: -21.799999999999997, Loss: 4.585567474365234\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 14, Action: 0, Reward: 2.0, Total Reward: -19.799999999999997, Loss: 4.369813919067383\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 15, Action: 0, Reward: 2.0, Total Reward: -17.799999999999997, Loss: 4.175996780395508\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 16, Action: 0, Reward: 2.0, Total Reward: -15.799999999999997, Loss: 7.6765055656433105\n",
      "Model acting\n",
      "Optimizing model...\n",
      "Step: 17, Action: 0, Reward: -8.0, Total Reward: -23.799999999999997, Loss: 6.045949935913086\n",
      "Model acting\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 228\u001b[0m\n\u001b[0;32m    225\u001b[0m action \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mselect_action(state_vector)\n\u001b[0;32m    227\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m action \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m4\u001b[39m:\n\u001b[1;32m--> 228\u001b[0m     \u001b[43mpyautogui\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpress\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mup\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdown\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mleft\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mright\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43maction\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    231\u001b[0m next_screenshot, non_crop_state \u001b[38;5;241m=\u001b[39m get_screen(GAME_REGION)\n\u001b[0;32m    232\u001b[0m next_state_raw \u001b[38;5;241m=\u001b[39m get_state(next_screenshot)\n",
      "File \u001b[1;32mh:\\Python310\\lib\\site-packages\\pyautogui\\__init__.py:595\u001b[0m, in \u001b[0;36m_genericPyAutoGUIChecks.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    593\u001b[0m failSafeCheck()\n\u001b[0;32m    594\u001b[0m returnVal \u001b[38;5;241m=\u001b[39m wrappedFunction(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m--> 595\u001b[0m \u001b[43m_handlePause\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m_pause\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    596\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m returnVal\n",
      "File \u001b[1;32mh:\\Python310\\lib\\site-packages\\pyautogui\\__init__.py:639\u001b[0m, in \u001b[0;36m_handlePause\u001b[1;34m(_pause)\u001b[0m\n\u001b[0;32m    637\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _pause:\n\u001b[0;32m    638\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(PAUSE, \u001b[38;5;28mint\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(PAUSE, \u001b[38;5;28mfloat\u001b[39m)\n\u001b[1;32m--> 639\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mPAUSE\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pyautogui\n",
    "import cv2\n",
    "import time\n",
    "import keyboard\n",
    "import torchvision\n",
    "from ultralytics import YOLO\n",
    "\n",
    "\n",
    "RES_X = 1920\n",
    "RES_Y = 1080\n",
    "\n",
    "GAME_REGION = (405, 210, 850, 480)\n",
    "restart_button = cv2.imread('restart_button.png', cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "\n",
    "def get_screen(region):\n",
    "    screen = pyautogui.screenshot(region=(region[0], region[1], region[2], region[3]))\n",
    "    \n",
    "    non_crop = screen.copy()\n",
    "\n",
    "    transforms = torchvision.transforms.Compose([\n",
    "        torchvision.transforms.RandomRotation((14, 14)),\n",
    "        torchvision.transforms.CenterCrop((320, 566)),\n",
    "        torchvision.transforms.Resize((240, 425)),\n",
    "    ])\n",
    "    \n",
    "    screen = transforms(screen)   \n",
    "    \n",
    "    screen = cv2.cvtColor(np.array(screen), cv2.COLOR_RGB2BGR)\n",
    "    \n",
    "    non_crop = cv2.cvtColor(np.array(non_crop), cv2.COLOR_RGB2BGR)\n",
    "    non_crop = cv2.resize(non_crop, (425, 240))\n",
    "    \n",
    "    return screen, non_crop\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def map_to_grid(image_size, grid_size, boxes, class_labels):\n",
    "    \"\"\"\n",
    "    Map detected bounding boxes to a grid representation.\n",
    "\n",
    "    Args:\n",
    "        image_size: Tuple (width, height) of the image.\n",
    "        grid_size: Tuple (N, M) of the grid dimensions.\n",
    "        boxes: List of bounding boxes [(x_min, y_min, x_max, y_max)].\n",
    "        class_labels: List of class labels corresponding to the boxes.\n",
    "\n",
    "    Returns:\n",
    "        grid: 2D numpy array of shape (N, M) with object class labels.\n",
    "    \"\"\"\n",
    "    width, height = image_size\n",
    "    grid_width, grid_height = grid_size\n",
    "    grid = np.zeros((grid_height, grid_width), dtype=int)\n",
    "\n",
    "    cell_width = width / grid_width\n",
    "    cell_height = height / grid_height\n",
    "\n",
    "    for (x_min, y_min, x_max, y_max), label in zip(boxes, class_labels):\n",
    "        x_start = int(x_min // cell_width)\n",
    "        y_start = int(y_min // cell_height)\n",
    "        x_end = int(np.ceil(x_max / cell_width))\n",
    "        y_end = int(np.ceil(y_max / cell_height))\n",
    "\n",
    "        for y in range(y_start, y_end):\n",
    "            for x in range(x_start, x_end):\n",
    "                grid[y, x] = label + 1\n",
    "\n",
    "    return grid\n",
    "\n",
    "\n",
    "def get_state(screen):\n",
    "    results = cv_model(screen, verbose=False)\n",
    "\n",
    "    image_size = (425, 240)\n",
    "    grid_size = (36, 32)\n",
    "\n",
    "    boxes = []\n",
    "    labels = []\n",
    "    \n",
    "    boxes_ = results[0].boxes\n",
    "    for box in boxes_:\n",
    "        x_min, y_min, x_max, y_max = box.xyxy[0].tolist()\n",
    "        \n",
    "        class_id = int(box.cls[0].item())\n",
    "        \n",
    "        boxes.append((x_min, y_min, x_max, y_max))\n",
    "        labels.append(class_id)\n",
    "\n",
    "    boxes, labels = zip(*sorted(zip(boxes, labels), key=lambda x: -x[1]))    \n",
    "    \n",
    "    grid = map_to_grid(image_size, grid_size, boxes, labels)\n",
    "\n",
    "    return grid\n",
    "\n",
    "def is_game_over(image, score_threshold=0.5, scale=0.5):\n",
    "    grey_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    resized_template = cv2.resize(restart_button, (0, 0), fx=scale, fy=scale)\n",
    "    h, w = grey_image.shape\n",
    "\n",
    "    cropped_search_box = grey_image[int(h * 0.87):, int(w * 0.43):int(w * 0.57)]\n",
    "    \n",
    "    result = cv2.matchTemplate(cropped_search_box, resized_template, cv2.TM_CCOEFF_NORMED)\n",
    "    result = np.sort(result.flatten())[::-1]\n",
    "    \n",
    "    return result.max() > score_threshold\n",
    "\n",
    "def process_state(state, max_obstacles=1152, max_timbers=1152):\n",
    "    agent_pos, obstacles, timbers = state[0], state[1], state[2]\n",
    "    \n",
    "    state_vector = list(agent_pos)\n",
    "    \n",
    "    obstacles = list(obstacles)\n",
    "    for i in range(max_obstacles):\n",
    "        if i < len(obstacles):\n",
    "            state_vector.extend(obstacles[i])\n",
    "        else:\n",
    "            state_vector.extend([0, 0])\n",
    "   \n",
    "    timbers = list(timbers)\n",
    "    for i in range(max_timbers):\n",
    "        if i < len(timbers):\n",
    "            state_vector.extend(timbers[i])\n",
    "        else:\n",
    "            state_vector.extend([0, 0])\n",
    "    \n",
    "    return state_vector\n",
    "\n",
    "def compute_reward(reward_state):\n",
    "    time = reward_state['time']\n",
    "    state = reward_state['state']\n",
    "    action = reward_state['action']\n",
    "    prev_action = reward_state['prev_action']\n",
    "    next_state = reward_state['next_state']\n",
    "    total_reward = reward_state['total_reward']\n",
    "    non_crop_state = reward_state['non_crop_state']\n",
    "    reward = 0\n",
    "    \n",
    "    if is_game_over(non_crop_state):\n",
    "        reward = -100\n",
    "        return reward\n",
    "    \n",
    "    if action == 0:\n",
    "        reward += 2\n",
    "    # elif action == 1:\n",
    "    #     reward -= 0\n",
    "    elif action == 2:\n",
    "        reward += 0.25\n",
    "    elif action == 3:\n",
    "        reward += 0.25\n",
    "    elif action == 4:\n",
    "        reward += 0.2\n",
    "    \n",
    "    hen_count = 0\n",
    "    for row in next_state:\n",
    "        for cell in row:\n",
    "            if cell == 1:\n",
    "                hen_count += 1\n",
    "\n",
    "    if action == prev_action:\n",
    "        if state[0][0] // 5 == next_state[0][0] // 5 \\\n",
    "        and state[0][1] // 5 == next_state[0][1] // 5:\n",
    "            reward -= 10\n",
    "            \n",
    "    reward = round(reward, 2)\n",
    "    \n",
    "    return reward\n",
    "    \n",
    "    \n",
    "actions = [0, 1, 2, 3, 4]\n",
    "\n",
    "input_size = 4610\n",
    "agent = IQNAgent(input_size, 5, 128, 10000, 32, 0.99, 1.0, 0.1, 1000)\n",
    "cv_model = YOLO('best_cv.pt')\n",
    "episodes = 1000\n",
    "episode_length = 1000\n",
    "losses = []\n",
    "rewards = []\n",
    "\n",
    "print(\"Model is ready to train\")\n",
    "\n",
    "keyboard.wait('q')\n",
    "\n",
    "for episode in range(episodes):\n",
    "    screenshot, non_crop_state = get_screen(GAME_REGION)\n",
    "    state_raw = get_state(screenshot)\n",
    "    \n",
    "    total_reward = 0\n",
    "    total_loss = 0\n",
    "    action = 0\n",
    "\n",
    "    state = simplify_state(state_raw)\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "    \n",
    "    \n",
    "    start_time = time.time()\n",
    "    for step in range(episode_length):\n",
    "        prev_action = action\n",
    "        time.sleep(0.025)\n",
    "        \n",
    "        state_vector = process_state(state)\n",
    "        \n",
    "        action = agent.select_action(state_vector)\n",
    "        \n",
    "        if action < 4:\n",
    "            pyautogui.press(['up', 'down', 'left', 'right'][action])\n",
    "        \n",
    "        \n",
    "        next_screenshot, non_crop_state = get_screen(GAME_REGION)\n",
    "        next_state_raw = get_state(next_screenshot)\n",
    "        \n",
    "        next_state = simplify_state(next_state_raw)\n",
    "        next_state_vector = process_state(next_state)\n",
    "        \n",
    "        reward_state = {\n",
    "            'state': state,\n",
    "            'action': action,\n",
    "            'prev_action': prev_action,\n",
    "            'next_state': next_state,\n",
    "            'time': time.time() - start_time,\n",
    "            'total_reward': total_reward,\n",
    "            'non_crop_state': non_crop_state\n",
    "        }\n",
    "        \n",
    "        done = 0\n",
    "        reward = compute_reward(reward_state)\n",
    "\n",
    "        agent.push(state_vector, action, reward, next_state_vector, done)\n",
    "\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "        \n",
    "        if is_game_over(non_crop_state):\n",
    "            done = 1\n",
    "            break\n",
    "        \n",
    "        loss = agent.optimize_model()\n",
    "\n",
    "        print(f\"Step: {step}, Action: {action}, Reward: {reward}, Total Reward: {total_reward}, Loss: {loss}\")\n",
    " \n",
    "    keyboard.press_and_release('space')\n",
    "    losses.append(total_loss)\n",
    "    rewards.append(total_reward)\n",
    "    print('\\nepisode: {}, reward: {}\\n'.format(episode, total_reward))\n",
    "    \n",
    "    agent.pop(4)    \n",
    "\n",
    "    if episode % 10 == 0:\n",
    "        agent.save('dqn.pth')\n",
    "        print(\"Model saved\")\n",
    "    \n",
    "    time.sleep(3.25)\n",
    "    keyboard.press_and_release('space')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
