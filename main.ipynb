{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DQN model for Crossy Road game\n",
    "# input is the 2d matrix of the game state (90 x 90)\n",
    "# output is the action to take (0, 1, 2, 3) for (up, down, left, right)\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_size):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size * 160, hidden_size * 16)\n",
    "        self.fc2 = nn.Linear(hidden_size * 16, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, hidden_size // 4)\n",
    "        self.fc4 = nn.Linear(hidden_size // 4, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        if len(x.shape) > 2:\n",
    "            batch_size = x.shape[0]\n",
    "            x = x.view(batch_size, -1)\n",
    "        else:\n",
    "            x = torch.flatten(x)\n",
    "        \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        \n",
    "        return x\n",
    "        \n",
    "    \n",
    "    def act(self, state, epsilon):\n",
    "        # print(epsilon)\n",
    "        if random.random() < 0.975:\n",
    "            state = torch.tensor(state, dtype=torch.float32)\n",
    "            \n",
    "            q_value = self.forward(state)\n",
    "            action = torch.argmax(input=q_value).item()\n",
    "            \n",
    "        else:\n",
    "            action = random.choice([0, 1, 2, 3, 4])\n",
    "        return action\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replay buffer for DQN\n",
    "# stores the transitions (state, action, reward, next_state, done)\n",
    "# and samples a batch of transitions for training\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.buffer = []\n",
    "        self.position = 0\n",
    "        \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        if len(self.buffer) < self.capacity:\n",
    "            self.buffer.append(None)\n",
    "        self.buffer[self.position] = (state, action, reward, next_state, done)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "        \n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        state, action, reward, next_state, done = map(np.stack, zip(*batch))\n",
    "        return state, action, reward, next_state, done\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "    \n",
    "    \n",
    "# DQN agent for Crossy Road game\n",
    "# uses DQN model and replay buffer for training\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self, input_size, output_size, hidden_size, replay_buffer_capacity, batch_size, gamma, epsilon_start, epsilon_end, epsilon_decay):\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.replay_buffer_capacity = replay_buffer_capacity\n",
    "        self.batch_size = batch_size\n",
    "        self.gamma = gamma\n",
    "        self.epsilon_start = epsilon_start\n",
    "        self.epsilon_end = epsilon_end\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        \n",
    "        self.model = DQN(input_size, output_size, hidden_size)\n",
    "        self.replay_buffer = ReplayBuffer(replay_buffer_capacity)\n",
    "        self.optimizer = optim.Adam(self.model.parameters())\n",
    "        \n",
    "        self.steps_done = 0\n",
    "        \n",
    "    def select_action(self, state):\n",
    "        epsilon = self.epsilon_end + (self.epsilon_start - self.epsilon_end) * np.exp(-1. * self.steps_done / self.epsilon_decay)\n",
    "        self.steps_done += 1\n",
    "        return self.model.act(state, epsilon)\n",
    "    \n",
    "    def optimize_model(self):\n",
    "        if len(self.replay_buffer) < self.batch_size:\n",
    "            return\n",
    "        \n",
    "        states, action, reward, next_states, done = self.replay_buffer.sample(self.batch_size)\n",
    "        \n",
    "        states = torch.FloatTensor(np.float32(states))\n",
    "        action = torch.LongTensor(action)\n",
    "        reward = torch.FloatTensor(reward)\n",
    "        next_states = torch.FloatTensor(np.float32(next_states))\n",
    "        done = torch.FloatTensor(done)\n",
    "\n",
    "        q_values = self.model(states)\n",
    "        next_q_values = self.model(next_states)\n",
    "        \n",
    "        q_value = q_values.gather(1, action.unsqueeze(1)).squeeze(1)\n",
    "        \n",
    "        \n",
    "        next_q_value = next_q_values.max(1)[0]  \n",
    "        expected_q_value = reward + self.gamma *  next_q_value\n",
    "        \n",
    "        # print(q_value, expected_q_value)\n",
    "        \n",
    "        loss = F.mse_loss(q_value, expected_q_value)\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        return loss.item()\n",
    "    \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        self.replay_buffer.push(state, action, reward, next_state, done)\n",
    "        \n",
    "    def save(self, path):\n",
    "        torch.save(self.model.state_dict(), path)\n",
    "        \n",
    "    def load(self, path):\n",
    "        self.model.load_state_dict(torch.load(path))\n",
    "        self.model.eval()\n",
    "        \n",
    "    def reset(self):\n",
    "        self.steps_done = 0\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model is ready to train\n",
      "\n",
      "episode: 0, loss: 0, reward: -100\n",
      "step: 0, loss: None, reward: 2.75, total_reward: 2.75\n",
      "\n",
      "step: 1, loss: None, reward: -0.5, total_reward: 2.25\n",
      "\n",
      "step: 2, loss: None, reward: 3, total_reward: 5.25\n",
      "\n",
      "step: 3, loss: None, reward: -2, total_reward: 3.25\n",
      "\n",
      "step: 4, loss: None, reward: -0.25, total_reward: 3.0\n",
      "\n",
      "\n",
      "episode: 1, loss: 0, reward: -97.0\n",
      "step: 0, loss: None, reward: 2.75, total_reward: 2.75\n",
      "\n",
      "step: 1, loss: None, reward: 2.75, total_reward: 5.5\n",
      "\n",
      "step: 2, loss: None, reward: 2.75, total_reward: 8.25\n",
      "\n",
      "step: 3, loss: None, reward: -0.25, total_reward: 8.0\n",
      "\n",
      "step: 4, loss: None, reward: -0.25, total_reward: 7.75\n",
      "\n",
      "step: 5, loss: None, reward: -0.5, total_reward: 7.25\n",
      "\n",
      "step: 6, loss: None, reward: -0.25, total_reward: 7.0\n",
      "\n",
      "step: 7, loss: None, reward: -0.5, total_reward: 6.5\n",
      "\n",
      "step: 8, loss: None, reward: -0.5, total_reward: 6.0\n",
      "\n",
      "step: 9, loss: None, reward: -0.5, total_reward: 5.5\n",
      "\n",
      "step: 10, loss: None, reward: -0.5, total_reward: 5.0\n",
      "\n",
      "step: 11, loss: None, reward: -0.5, total_reward: 4.5\n",
      "\n",
      "step: 12, loss: None, reward: -0.5, total_reward: 4.0\n",
      "\n",
      "step: 13, loss: None, reward: -0.5, total_reward: 3.5\n",
      "\n",
      "step: 14, loss: None, reward: -0.5, total_reward: 3.0\n",
      "\n",
      "step: 15, loss: None, reward: -0.5, total_reward: 2.5\n",
      "\n",
      "step: 16, loss: None, reward: -2, total_reward: 0.5\n",
      "\n",
      "step: 17, loss: None, reward: -2.25, total_reward: -1.75\n",
      "\n",
      "step: 18, loss: None, reward: -2.25, total_reward: -4.0\n",
      "\n",
      "step: 19, loss: None, reward: -2.25, total_reward: -6.25\n",
      "\n",
      "step: 20, loss: None, reward: -2.25, total_reward: -8.5\n",
      "\n",
      "step: 21, loss: None, reward: -0.25, total_reward: -8.75\n",
      "\n",
      "step: 22, loss: 1015.7952880859375, reward: -0.5, total_reward: -9.25\n",
      "\n",
      "step: 23, loss: 484.2255859375, reward: -0.5, total_reward: -9.75\n",
      "\n",
      "step: 24, loss: 637.1983032226562, reward: -2, total_reward: -11.75\n",
      "\n",
      "step: 25, loss: 562.0654907226562, reward: -0.25, total_reward: -12.0\n",
      "\n",
      "step: 26, loss: 520.8147583007812, reward: -0.5, total_reward: -12.5\n",
      "\n",
      "step: 27, loss: 485.21575927734375, reward: -2, total_reward: -14.5\n",
      "\n",
      "step: 28, loss: 338.9117126464844, reward: 3, total_reward: -11.5\n",
      "\n",
      "\n",
      "episode: 2, loss: 4044.2268981933594, reward: -111.5\n",
      "step: 0, loss: 512.8191528320312, reward: 2.75, total_reward: 2.75\n",
      "\n",
      "step: 1, loss: 470.2088623046875, reward: 2.75, total_reward: 5.5\n",
      "\n",
      "step: 2, loss: 427.451171875, reward: -0.25, total_reward: 5.25\n",
      "\n",
      "step: 3, loss: 388.5429382324219, reward: -0.5, total_reward: 4.75\n",
      "\n",
      "step: 4, loss: 409.1076354980469, reward: 3, total_reward: 7.75\n",
      "\n",
      "\n",
      "episode: 3, loss: 2208.1297607421875, reward: -92.25\n",
      "step: 0, loss: 344.39068603515625, reward: -0.25, total_reward: -0.25\n",
      "\n",
      "step: 1, loss: 969.1963500976562, reward: 3, total_reward: 2.75\n",
      "\n",
      "step: 2, loss: 674.4238891601562, reward: 2.75, total_reward: 5.5\n",
      "\n",
      "step: 3, loss: 671.498291015625, reward: 2.75, total_reward: 8.25\n",
      "\n",
      "step: 4, loss: 603.1603393554688, reward: 2.75, total_reward: 11.0\n",
      "\n",
      "step: 5, loss: 365.97418212890625, reward: 2.75, total_reward: 13.75\n",
      "\n",
      "step: 6, loss: 264.03070068359375, reward: 2.75, total_reward: 16.5\n",
      "\n",
      "step: 7, loss: 565.8878173828125, reward: 2.75, total_reward: 19.25\n",
      "\n",
      "step: 8, loss: 328.4634094238281, reward: -0.25, total_reward: 19.0\n",
      "\n",
      "step: 9, loss: 642.0342407226562, reward: -0.25, total_reward: 18.75\n",
      "\n",
      "step: 10, loss: 617.6573486328125, reward: -0.5, total_reward: 18.25\n",
      "\n",
      "step: 11, loss: 545.3369750976562, reward: -0.5, total_reward: 17.75\n",
      "\n",
      "step: 12, loss: 142.38507080078125, reward: -0.5, total_reward: 17.25\n",
      "\n",
      "step: 13, loss: 28.279632568359375, reward: -0.5, total_reward: 16.75\n",
      "\n",
      "step: 14, loss: 422.24658203125, reward: 3, total_reward: 19.75\n",
      "\n",
      "step: 15, loss: 63.895469665527344, reward: 2.75, total_reward: 22.5\n",
      "\n",
      "step: 16, loss: 49.340118408203125, reward: -0.5, total_reward: 22.0\n",
      "\n",
      "\n",
      "episode: 4, loss: 7298.201103210449, reward: -78.0\n",
      "step: 0, loss: 635.8695678710938, reward: 2.75, total_reward: 2.75\n",
      "\n",
      "step: 1, loss: 27.52078628540039, reward: 2.75, total_reward: 5.5\n",
      "\n",
      "step: 2, loss: 904.9464721679688, reward: 2.75, total_reward: 8.25\n",
      "\n",
      "step: 3, loss: 340.1321716308594, reward: 2.75, total_reward: 11.0\n",
      "\n",
      "step: 4, loss: 607.4492797851562, reward: 2.75, total_reward: 13.75\n",
      "\n",
      "step: 5, loss: 338.97344970703125, reward: -0.25, total_reward: 13.5\n",
      "\n",
      "step: 6, loss: 571.609130859375, reward: -2, total_reward: 11.5\n",
      "\n",
      "step: 7, loss: 233.67750549316406, reward: -0.25, total_reward: 11.25\n",
      "\n",
      "step: 8, loss: 202.5284423828125, reward: -0.5, total_reward: 10.75\n",
      "\n",
      "\n",
      "episode: 5, loss: 3862.7068061828613, reward: -89.25\n",
      "step: 0, loss: 823.5772705078125, reward: 2.75, total_reward: 2.75\n",
      "\n",
      "step: 1, loss: 726.28759765625, reward: 2.75, total_reward: 5.5\n",
      "\n",
      "step: 2, loss: 468.8961486816406, reward: 2.75, total_reward: 8.25\n",
      "\n",
      "\n",
      "episode: 6, loss: 2018.7610168457031, reward: -91.75\n",
      "step: 0, loss: 1010.9484252929688, reward: 2.75, total_reward: 2.75\n",
      "\n",
      "step: 1, loss: 380.3112487792969, reward: 2.75, total_reward: 5.5\n",
      "\n",
      "step: 2, loss: 865.4126586914062, reward: 2.75, total_reward: 8.25\n",
      "\n",
      "step: 3, loss: 974.3140869140625, reward: 2.75, total_reward: 11.0\n",
      "\n",
      "step: 4, loss: 518.6986083984375, reward: -0.25, total_reward: 10.75\n",
      "\n",
      "step: 5, loss: 620.8282470703125, reward: -0.25, total_reward: 10.5\n",
      "\n",
      "step: 6, loss: 1110.68359375, reward: -2, total_reward: 8.5\n",
      "\n",
      "step: 7, loss: 496.7806091308594, reward: -0.25, total_reward: 8.25\n",
      "\n",
      "step: 8, loss: 427.1162414550781, reward: -0.5, total_reward: 7.75\n",
      "\n",
      "step: 9, loss: 277.5398864746094, reward: -2, total_reward: 5.75\n",
      "\n",
      "\n",
      "episode: 7, loss: 6682.633605957031, reward: -94.25\n",
      "step: 0, loss: 705.227294921875, reward: 2.75, total_reward: 2.75\n",
      "\n",
      "step: 1, loss: 134.76011657714844, reward: 2.75, total_reward: 5.5\n",
      "\n",
      "step: 2, loss: 396.6961669921875, reward: 2.75, total_reward: 8.25\n",
      "\n",
      "step: 3, loss: 72.58138275146484, reward: -0.25, total_reward: 8.0\n",
      "\n",
      "step: 4, loss: 57.477439880371094, reward: -0.5, total_reward: 7.5\n",
      "\n",
      "step: 5, loss: 75.7867202758789, reward: -0.25, total_reward: 7.25\n",
      "\n",
      "step: 6, loss: 91.06124877929688, reward: -2, total_reward: 5.25\n",
      "\n",
      "step: 7, loss: 686.08935546875, reward: -2.25, total_reward: 3.0\n",
      "\n",
      "step: 8, loss: 33.44892883300781, reward: -2.25, total_reward: 0.75\n",
      "\n",
      "step: 9, loss: 771.4456176757812, reward: -2.25, total_reward: -1.5\n",
      "\n",
      "step: 10, loss: 358.9908142089844, reward: -0.5, total_reward: -2.0\n",
      "\n",
      "step: 11, loss: 721.3910522460938, reward: -0.75, total_reward: -2.75\n",
      "\n",
      "\n",
      "episode: 8, loss: 4104.95613861084, reward: -102.75\n",
      "step: 0, loss: 1471.25048828125, reward: 2.75, total_reward: 2.75\n",
      "\n",
      "step: 1, loss: 90.12761688232422, reward: 2.75, total_reward: 5.5\n",
      "\n",
      "step: 2, loss: 441.9483947753906, reward: -0.5, total_reward: 5.0\n",
      "\n",
      "step: 3, loss: 54.49449920654297, reward: -0.75, total_reward: 4.25\n",
      "\n",
      "step: 4, loss: 24.703079223632812, reward: 3, total_reward: 7.25\n",
      "\n",
      "step: 5, loss: 915.732421875, reward: -0.25, total_reward: 7.0\n",
      "\n",
      "step: 6, loss: 484.6485290527344, reward: -2, total_reward: 5.0\n",
      "\n",
      "\n",
      "episode: 9, loss: 3482.905029296875, reward: -95.0\n",
      "step: 0, loss: 298.63885498046875, reward: 2.75, total_reward: 2.75\n",
      "\n",
      "step: 1, loss: 328.7915954589844, reward: 2.75, total_reward: 5.5\n",
      "\n",
      "step: 2, loss: 797.9999389648438, reward: 2.75, total_reward: 8.25\n",
      "\n",
      "step: 3, loss: 1215.830810546875, reward: -2, total_reward: 6.25\n",
      "\n",
      "step: 4, loss: 31.0527286529541, reward: -2.25, total_reward: 4.0\n",
      "\n",
      "step: 5, loss: 1315.272216796875, reward: -0.25, total_reward: 3.75\n",
      "\n",
      "step: 6, loss: 446.2373352050781, reward: 3, total_reward: 6.75\n",
      "\n",
      "step: 7, loss: 596.8699951171875, reward: 2.75, total_reward: 9.5\n",
      "\n",
      "step: 8, loss: 866.0902709960938, reward: -0.25, total_reward: 9.25\n",
      "\n",
      "step: 9, loss: 961.5730590820312, reward: -0.5, total_reward: 8.75\n",
      "\n",
      "step: 10, loss: 174.97251892089844, reward: -0.5, total_reward: 8.25\n",
      "\n",
      "step: 11, loss: 333.27362060546875, reward: 3, total_reward: 11.25\n",
      "\n",
      "step: 12, loss: 388.059326171875, reward: -0.25, total_reward: 11.0\n",
      "\n",
      "step: 13, loss: 390.57672119140625, reward: -0.5, total_reward: 10.5\n",
      "\n",
      "step: 14, loss: 40.80712127685547, reward: -0.5, total_reward: 10.0\n",
      "\n",
      "step: 15, loss: 717.196044921875, reward: 3, total_reward: 13.0\n",
      "\n",
      "step: 16, loss: 349.6461486816406, reward: 2.75, total_reward: 15.75\n",
      "\n",
      "\n",
      "episode: 10, loss: 9252.888307571411, reward: -84.25\n",
      "step: 0, loss: 45.976829528808594, reward: 2.75, total_reward: 2.75\n",
      "\n",
      "step: 1, loss: 677.6753540039062, reward: 2.75, total_reward: 5.5\n",
      "\n",
      "step: 2, loss: 671.8338623046875, reward: -0.25, total_reward: 5.25\n",
      "\n",
      "step: 3, loss: 356.14129638671875, reward: -0.5, total_reward: 4.75\n",
      "\n",
      "step: 4, loss: 336.40631103515625, reward: 3, total_reward: 7.75\n",
      "\n",
      "step: 5, loss: 56.68574523925781, reward: -0.25, total_reward: 7.5\n",
      "\n",
      "step: 6, loss: 648.2704467773438, reward: -0.5, total_reward: 7.0\n",
      "\n",
      "step: 7, loss: 905.4619140625, reward: 3, total_reward: 10.0\n",
      "\n",
      "step: 8, loss: 340.34796142578125, reward: 2.75, total_reward: 12.75\n",
      "\n",
      "\n",
      "episode: 11, loss: 4038.79972076416, reward: -87.25\n",
      "step: 0, loss: 884.23828125, reward: 2.75, total_reward: 2.75\n",
      "\n",
      "step: 1, loss: 546.370361328125, reward: 2.75, total_reward: 5.5\n",
      "\n",
      "step: 2, loss: 836.04736328125, reward: -0.25, total_reward: 5.25\n",
      "\n",
      "step: 3, loss: 17.920352935791016, reward: -0.25, total_reward: 5.0\n",
      "\n",
      "step: 4, loss: 14.899866104125977, reward: 3, total_reward: 8.0\n",
      "\n",
      "step: 5, loss: 474.80596923828125, reward: 2.75, total_reward: 10.75\n",
      "\n",
      "step: 6, loss: 355.6729431152344, reward: 2.75, total_reward: 13.5\n",
      "\n",
      "\n",
      "episode: 12, loss: 3129.9551372528076, reward: -86.5\n",
      "step: 0, loss: 1347.657958984375, reward: 2.75, total_reward: 2.75\n",
      "\n",
      "step: 1, loss: 535.34423828125, reward: 2.75, total_reward: 5.5\n",
      "\n",
      "\n",
      "episode: 13, loss: 1883.002197265625, reward: -94.5\n",
      "step: 0, loss: 818.6560668945312, reward: 2.75, total_reward: 2.75\n",
      "\n",
      "step: 1, loss: 331.4671936035156, reward: 2.75, total_reward: 5.5\n",
      "\n",
      "step: 2, loss: 442.39373779296875, reward: 2.75, total_reward: 8.25\n",
      "\n",
      "step: 3, loss: 412.2731628417969, reward: -0.25, total_reward: 8.0\n",
      "\n",
      "\n",
      "episode: 14, loss: 2004.7901611328125, reward: -92.0\n",
      "step: 0, loss: 164.60597229003906, reward: 2.75, total_reward: 2.75\n",
      "\n",
      "step: 1, loss: 774.4503784179688, reward: 2.75, total_reward: 5.5\n",
      "\n",
      "step: 2, loss: 44.3477668762207, reward: 2.75, total_reward: 8.25\n",
      "\n",
      "step: 3, loss: 201.22705078125, reward: 2.75, total_reward: 11.0\n",
      "\n",
      "step: 4, loss: 365.3277587890625, reward: -0.25, total_reward: 10.75\n",
      "\n",
      "\n",
      "episode: 15, loss: 1549.958927154541, reward: -89.25\n",
      "step: 0, loss: 911.3975830078125, reward: 2.75, total_reward: 2.75\n",
      "\n",
      "step: 1, loss: 687.5944213867188, reward: -2, total_reward: 0.75\n",
      "\n",
      "step: 2, loss: 1081.56298828125, reward: -0.25, total_reward: 0.5\n",
      "\n",
      "step: 3, loss: 769.4160766601562, reward: 3, total_reward: 3.5\n",
      "\n",
      "step: 4, loss: 998.0689697265625, reward: 2.75, total_reward: 6.25\n",
      "\n",
      "step: 5, loss: 969.9964599609375, reward: -0.25, total_reward: 6.0\n",
      "\n",
      "\n",
      "episode: 16, loss: 5418.0364990234375, reward: -94.0\n",
      "step: 0, loss: 697.6749877929688, reward: -0.25, total_reward: -0.25\n",
      "\n",
      "step: 1, loss: 1045.9688720703125, reward: 3, total_reward: 2.75\n",
      "\n",
      "step: 2, loss: 421.9913330078125, reward: -0.25, total_reward: 2.5\n",
      "\n",
      "step: 3, loss: 363.9638977050781, reward: 3, total_reward: 5.5\n",
      "\n",
      "step: 4, loss: 360.7636413574219, reward: 2.75, total_reward: 8.25\n",
      "\n",
      "step: 5, loss: 995.5155029296875, reward: 2.75, total_reward: 11.0\n",
      "\n",
      "step: 6, loss: 366.29412841796875, reward: -0.25, total_reward: 10.75\n",
      "\n",
      "step: 7, loss: 573.185302734375, reward: -2, total_reward: 8.75\n",
      "\n",
      "\n",
      "episode: 17, loss: 4825.357666015625, reward: -91.25\n",
      "step: 0, loss: 83.84282684326172, reward: 2.75, total_reward: 2.75\n",
      "\n",
      "step: 1, loss: 568.2592163085938, reward: 2.75, total_reward: 5.5\n",
      "\n",
      "step: 2, loss: 535.2984619140625, reward: 2.75, total_reward: 8.25\n",
      "\n",
      "step: 3, loss: 1104.2763671875, reward: -0.5, total_reward: 7.75\n",
      "\n",
      "step: 4, loss: 772.6319580078125, reward: 3, total_reward: 10.75\n",
      "\n",
      "step: 5, loss: 75.43480682373047, reward: 2.75, total_reward: 13.5\n",
      "\n",
      "step: 6, loss: 361.62371826171875, reward: 2.75, total_reward: 16.25\n",
      "\n",
      "\n",
      "episode: 18, loss: 3501.3673553466797, reward: -83.75\n",
      "step: 0, loss: 362.15875244140625, reward: 2.75, total_reward: 2.75\n",
      "\n",
      "step: 1, loss: 79.68313598632812, reward: 2.75, total_reward: 5.5\n",
      "\n",
      "step: 2, loss: 983.0018920898438, reward: 2.75, total_reward: 8.25\n",
      "\n",
      "step: 3, loss: 372.6033935546875, reward: -0.5, total_reward: 7.75\n",
      "\n",
      "step: 4, loss: 542.6162719726562, reward: -0.25, total_reward: 7.5\n",
      "\n",
      "step: 5, loss: 35.691619873046875, reward: -0.5, total_reward: 7.0\n",
      "\n",
      "step: 6, loss: 662.7658081054688, reward: -0.75, total_reward: 6.25\n",
      "\n",
      "step: 7, loss: 368.6395568847656, reward: -0.25, total_reward: 6.0\n",
      "\n",
      "step: 8, loss: 476.65216064453125, reward: -0.5, total_reward: 5.5\n",
      "\n",
      "step: 9, loss: 357.1861572265625, reward: -2, total_reward: 3.5\n",
      "\n",
      "step: 10, loss: 376.5978088378906, reward: -2.25, total_reward: 1.25\n",
      "\n",
      "step: 11, loss: 29.66753387451172, reward: -0.5, total_reward: 0.75\n",
      "\n",
      "step: 12, loss: 63.28684997558594, reward: -0.25, total_reward: 0.5\n",
      "\n",
      "step: 13, loss: 371.0690002441406, reward: -0.5, total_reward: 0.0\n",
      "\n",
      "step: 14, loss: 66.40762329101562, reward: -0.5, total_reward: -0.5\n",
      "\n",
      "step: 15, loss: 565.8704833984375, reward: -0.5, total_reward: -1.0\n",
      "\n",
      "step: 16, loss: 651.833984375, reward: -0.25, total_reward: -1.25\n",
      "\n",
      "step: 17, loss: 48.655216217041016, reward: -0.5, total_reward: -1.75\n",
      "\n",
      "\n",
      "episode: 19, loss: 6414.38724899292, reward: -101.75\n",
      "step: 0, loss: 674.329833984375, reward: 2.75, total_reward: 2.75\n",
      "\n",
      "step: 1, loss: 221.9557342529297, reward: 2.75, total_reward: 5.5\n",
      "\n",
      "step: 2, loss: 31.450557708740234, reward: 2.75, total_reward: 8.25\n",
      "\n",
      "step: 3, loss: 335.9697570800781, reward: -0.25, total_reward: 8.0\n",
      "\n",
      "step: 4, loss: 438.09307861328125, reward: 3, total_reward: 11.0\n",
      "\n",
      "step: 5, loss: 16.666282653808594, reward: 2.75, total_reward: 13.75\n",
      "\n",
      "step: 6, loss: 386.7000427246094, reward: -0.5, total_reward: 13.25\n",
      "\n",
      "\n",
      "episode: 20, loss: 2105.1652870178223, reward: -86.75\n",
      "step: 0, loss: 604.328125, reward: 2.75, total_reward: 2.75\n",
      "\n",
      "step: 1, loss: 523.5214233398438, reward: 2.75, total_reward: 5.5\n",
      "\n",
      "step: 2, loss: 373.96490478515625, reward: -2, total_reward: 3.5\n",
      "\n",
      "step: 3, loss: 348.408935546875, reward: -2.25, total_reward: 1.25\n",
      "\n",
      "step: 4, loss: 869.2794799804688, reward: 3, total_reward: 4.25\n",
      "\n",
      "step: 5, loss: 968.5225830078125, reward: -0.25, total_reward: 4.0\n",
      "\n",
      "step: 6, loss: 1290.93505859375, reward: 3, total_reward: 7.0\n",
      "\n",
      "step: 7, loss: 101.72118377685547, reward: -2, total_reward: 5.0\n",
      "\n",
      "step: 8, loss: 16.198631286621094, reward: -0.25, total_reward: 4.75\n",
      "\n",
      "step: 9, loss: 651.9334106445312, reward: 3, total_reward: 7.75\n",
      "\n",
      "step: 10, loss: 694.9345703125, reward: 2.75, total_reward: 10.5\n",
      "\n",
      "step: 11, loss: 375.2454528808594, reward: 2.75, total_reward: 13.25\n",
      "\n",
      "step: 12, loss: 357.7104797363281, reward: -0.25, total_reward: 13.0\n",
      "\n",
      "step: 13, loss: 963.1250610351562, reward: -0.25, total_reward: 12.75\n",
      "\n",
      "step: 14, loss: 687.4420166015625, reward: -0.5, total_reward: 12.25\n",
      "\n",
      "step: 15, loss: 340.9451904296875, reward: -2, total_reward: 10.25\n",
      "\n",
      "step: 16, loss: 337.2665100097656, reward: -0.25, total_reward: 10.0\n",
      "\n",
      "step: 17, loss: 967.5112915039062, reward: 3, total_reward: 13.0\n",
      "\n",
      "step: 18, loss: 371.1285705566406, reward: -2, total_reward: 11.0\n",
      "\n",
      "\n",
      "episode: 21, loss: 10844.12287902832, reward: -89.0\n",
      "step: 0, loss: 338.42059326171875, reward: -0.25, total_reward: -0.25\n",
      "\n",
      "step: 1, loss: 622.1638793945312, reward: -0.5, total_reward: -0.75\n",
      "\n",
      "step: 2, loss: 209.45095825195312, reward: -0.5, total_reward: -1.25\n",
      "\n",
      "step: 3, loss: 236.9346923828125, reward: -0.25, total_reward: -1.5\n",
      "\n",
      "step: 4, loss: 690.73486328125, reward: -0.25, total_reward: -1.75\n",
      "\n",
      "step: 5, loss: 322.5703430175781, reward: -0.5, total_reward: -2.25\n",
      "\n",
      "step: 6, loss: 18.632164001464844, reward: 3, total_reward: 0.75\n",
      "\n",
      "step: 7, loss: 419.63763427734375, reward: 2.75, total_reward: 3.5\n",
      "\n",
      "step: 8, loss: 655.7501220703125, reward: -0.25, total_reward: 3.25\n",
      "\n",
      "step: 9, loss: 1972.686767578125, reward: -0.5, total_reward: 2.75\n",
      "\n",
      "step: 10, loss: 653.9548950195312, reward: -0.5, total_reward: 2.25\n",
      "\n",
      "step: 11, loss: 962.1404418945312, reward: -2, total_reward: 0.25\n",
      "\n",
      "step: 12, loss: 53.64500045776367, reward: -0.25, total_reward: 0.0\n",
      "\n",
      "step: 13, loss: 348.8453063964844, reward: -0.5, total_reward: -0.5\n",
      "\n",
      "step: 14, loss: 27.906002044677734, reward: 3, total_reward: 2.5\n",
      "\n",
      "step: 15, loss: 335.46893310546875, reward: -0.5, total_reward: 2.0\n",
      "\n",
      "step: 16, loss: 18.406116485595703, reward: 3, total_reward: 5.0\n",
      "\n",
      "step: 17, loss: 243.88824462890625, reward: 2.75, total_reward: 7.75\n",
      "\n",
      "\n",
      "episode: 22, loss: 8131.236957550049, reward: -92.25\n",
      "step: 0, loss: 330.8284606933594, reward: 2.75, total_reward: 2.75\n",
      "\n",
      "step: 1, loss: 335.2087707519531, reward: 2.75, total_reward: 5.5\n",
      "\n",
      "step: 2, loss: 23.407211303710938, reward: 2.75, total_reward: 8.25\n",
      "\n",
      "\n",
      "episode: 23, loss: 689.4444427490234, reward: -91.75\n",
      "step: 0, loss: 833.9207763671875, reward: 2.75, total_reward: 2.75\n",
      "\n",
      "step: 1, loss: 20.05292510986328, reward: -2, total_reward: 0.75\n",
      "\n",
      "step: 2, loss: 8.909496307373047, reward: 3, total_reward: 3.75\n",
      "\n",
      "step: 3, loss: 611.160400390625, reward: 2.75, total_reward: 6.5\n",
      "\n",
      "step: 4, loss: 648.7161865234375, reward: 2.75, total_reward: 9.25\n",
      "\n",
      "step: 5, loss: 643.400634765625, reward: 2.75, total_reward: 12.0\n",
      "\n",
      "step: 6, loss: 802.7537841796875, reward: -2, total_reward: 10.0\n",
      "\n",
      "\n",
      "episode: 24, loss: 3568.914203643799, reward: -90.0\n",
      "step: 0, loss: 662.2838745117188, reward: 2.75, total_reward: 2.75\n",
      "\n",
      "step: 1, loss: 684.34326171875, reward: -0.25, total_reward: 2.5\n",
      "\n",
      "step: 2, loss: 751.1102294921875, reward: -2, total_reward: 0.5\n",
      "\n",
      "step: 3, loss: 451.1752624511719, reward: -0.5, total_reward: 0.0\n",
      "\n",
      "step: 4, loss: 77.90316009521484, reward: 3, total_reward: 3.0\n",
      "\n",
      "step: 5, loss: 401.5514221191406, reward: -0.25, total_reward: 2.75\n",
      "\n",
      "step: 6, loss: 338.66497802734375, reward: -0.25, total_reward: 2.5\n",
      "\n",
      "step: 7, loss: 25.830045700073242, reward: -0.25, total_reward: 2.25\n",
      "\n",
      "step: 8, loss: 17.533946990966797, reward: -2, total_reward: 0.25\n",
      "\n",
      "step: 9, loss: 339.2989807128906, reward: -0.25, total_reward: 0.0\n",
      "\n",
      "step: 10, loss: 20.130094528198242, reward: -2, total_reward: -2.0\n",
      "\n",
      "step: 11, loss: 481.9644470214844, reward: -0.25, total_reward: -2.25\n",
      "\n",
      "step: 12, loss: 667.2424926757812, reward: -0.25, total_reward: -2.5\n",
      "\n",
      "\n",
      "episode: 25, loss: 4919.032196044922, reward: -102.5\n",
      "step: 0, loss: 144.14578247070312, reward: 2.75, total_reward: 2.75\n",
      "\n",
      "step: 1, loss: 342.32086181640625, reward: 2.75, total_reward: 5.5\n",
      "\n",
      "step: 2, loss: 357.2937927246094, reward: 2.75, total_reward: 8.25\n",
      "\n",
      "\n",
      "episode: 26, loss: 843.7604370117188, reward: -91.75\n",
      "step: 0, loss: 372.94500732421875, reward: 2.75, total_reward: 2.75\n",
      "\n",
      "step: 1, loss: 576.6663208007812, reward: 2.75, total_reward: 5.5\n",
      "\n",
      "step: 2, loss: 479.6449279785156, reward: 2.75, total_reward: 8.25\n",
      "\n",
      "step: 3, loss: 361.9508972167969, reward: -0.25, total_reward: 8.0\n",
      "\n",
      "step: 4, loss: 543.873291015625, reward: -2, total_reward: 6.0\n",
      "\n",
      "step: 5, loss: 176.1009521484375, reward: -2.25, total_reward: 3.75\n",
      "\n",
      "\n",
      "episode: 27, loss: 2511.181396484375, reward: -96.25\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 218\u001b[0m\n\u001b[0;32m    216\u001b[0m rewards\u001b[38;5;241m.\u001b[39mappend(total_reward)\n\u001b[0;32m    217\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mepisode: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, loss: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, reward: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(episode, total_loss, total_reward))\n\u001b[1;32m--> 218\u001b[0m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdqn.pth\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    219\u001b[0m agent\u001b[38;5;241m.\u001b[39mreset()\n\u001b[0;32m    221\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m3.25\u001b[39m)\n",
      "Cell \u001b[1;32mIn[3], line 87\u001b[0m, in \u001b[0;36mDQNAgent.save\u001b[1;34m(self, path)\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msave\u001b[39m(\u001b[38;5;28mself\u001b[39m, path):\n\u001b[1;32m---> 87\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mh:\\Python310\\lib\\site-packages\\ultralytics\\utils\\patches.py:100\u001b[0m, in \u001b[0;36mtorch_save\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m4\u001b[39m):  \u001b[38;5;66;03m# 3 retries\u001b[39;00m\n\u001b[0;32m     99\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 100\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _torch_save(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    101\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# unable to save, possibly waiting for device to flush or antivirus scan\u001b[39;00m\n\u001b[0;32m    102\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m:\n",
      "File \u001b[1;32mh:\\Python310\\lib\\site-packages\\torch\\serialization.py:628\u001b[0m, in \u001b[0;36msave\u001b[1;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[0m\n\u001b[0;32m    626\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[0;32m    627\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m _open_zipfile_writer(f) \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n\u001b[1;32m--> 628\u001b[0m         \u001b[43m_save\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopened_zipfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_protocol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_disable_byteorder_record\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    629\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m    630\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mh:\\Python310\\lib\\site-packages\\torch\\serialization.py:862\u001b[0m, in \u001b[0;36m_save\u001b[1;34m(obj, zip_file, pickle_module, pickle_protocol, _disable_byteorder_record)\u001b[0m\n\u001b[0;32m    860\u001b[0m \u001b[38;5;66;03m# Now that it is on the CPU we can directly copy it into the zip file\u001b[39;00m\n\u001b[0;32m    861\u001b[0m num_bytes \u001b[38;5;241m=\u001b[39m storage\u001b[38;5;241m.\u001b[39mnbytes()\n\u001b[1;32m--> 862\u001b[0m \u001b[43mzip_file\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite_record\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_bytes\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# run DQN agent on Crossy Road game to train and play the game\n",
    "from langcodes import get\n",
    "import pyautogui\n",
    "import cv2\n",
    "import time\n",
    "import keyboard\n",
    "from pygame import ver\n",
    "from torch import res\n",
    "import torchvision\n",
    "from ultralytics import YOLO\n",
    "\n",
    "\n",
    "RES_X = 1920\n",
    "RES_Y = 1080\n",
    "\n",
    "GAME_REGION = (405, 210, 850, 480)\n",
    "restart_button = cv2.imread('restart_button.png', cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "\n",
    "def get_screen(region):\n",
    "    screen = pyautogui.screenshot(region=(region[0], region[1], region[2], region[3]))\n",
    "    \n",
    "    non_crop = screen.copy()\n",
    "\n",
    "    transforms = torchvision.transforms.Compose([\n",
    "        torchvision.transforms.RandomRotation((14, 14)),\n",
    "        torchvision.transforms.CenterCrop((320, 566)),\n",
    "        torchvision.transforms.Resize((240, 425)),\n",
    "    ])\n",
    "    \n",
    "    screen = transforms(screen)   \n",
    "    \n",
    "    screen = cv2.cvtColor(np.array(screen), cv2.COLOR_RGB2BGR)\n",
    "    \n",
    "    non_crop = cv2.cvtColor(np.array(non_crop), cv2.COLOR_RGB2BGR)\n",
    "    non_crop = cv2.resize(non_crop, (425, 240))\n",
    "    \n",
    "    return screen, non_crop\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def map_to_grid(image_size, grid_size, boxes, class_labels):\n",
    "    \"\"\"\n",
    "    Map detected bounding boxes to a grid representation.\n",
    "\n",
    "    Args:\n",
    "        image_size: Tuple (width, height) of the image.\n",
    "        grid_size: Tuple (N, M) of the grid dimensions.\n",
    "        boxes: List of bounding boxes [(x_min, y_min, x_max, y_max)].\n",
    "        class_labels: List of class labels corresponding to the boxes.\n",
    "\n",
    "    Returns:\n",
    "        grid: 2D numpy array of shape (N, M) with object class labels.\n",
    "    \"\"\"\n",
    "    width, height = image_size\n",
    "    grid_width, grid_height = grid_size\n",
    "    grid = np.zeros((grid_height, grid_width), dtype=int)\n",
    "\n",
    "    cell_width = width / grid_width\n",
    "    cell_height = height / grid_height\n",
    "\n",
    "    for (x_min, y_min, x_max, y_max), label in zip(boxes, class_labels):\n",
    "        x_start = int(x_min // cell_width)\n",
    "        y_start = int(y_min // cell_height)\n",
    "        x_end = int(np.ceil(x_max / cell_width))\n",
    "        y_end = int(np.ceil(y_max / cell_height))\n",
    "\n",
    "        for y in range(y_start, y_end):\n",
    "            for x in range(x_start, x_end):\n",
    "                grid[y, x] = label\n",
    "\n",
    "    return grid\n",
    "\n",
    "\n",
    "def get_state(screen):\n",
    "    results = cv_model(screen, verbose=False)\n",
    "\n",
    "    image_size = (425, 240)  # Example image dimensions (width, height)\n",
    "    grid_size = (180, 160)    # Example grid dimensions (N, M)\n",
    "\n",
    "    boxes = []\n",
    "    labels = []\n",
    "    \n",
    "    boxes_ = results[0].boxes\n",
    "    for box in boxes_:\n",
    "        x_min, y_min, x_max, y_max = box.xyxy[0].tolist()\n",
    "        \n",
    "        class_id = int(box.cls[0].item())\n",
    "        \n",
    "        boxes.append((x_min, y_min, x_max, y_max))\n",
    "        labels.append(class_id)  # Assuming class_id is the label\n",
    "\n",
    "    grid = map_to_grid(image_size, grid_size, boxes, labels)\n",
    "    \n",
    "    return grid\n",
    "\n",
    "def is_game_over(image, score_threshold=0.5, scale=0.5):\n",
    "    grey_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    resized_template = cv2.resize(restart_button, (0, 0), fx=scale, fy=scale)\n",
    "    h, w = grey_image.shape\n",
    "\n",
    "    cropped_search_box = grey_image[int(h * 0.87):, int(w * 0.43):int(w * 0.57)]\n",
    "    \n",
    "    # cv2.imwrite('cropped_search_box.png', cropped_search_box)\n",
    "    \n",
    "    result = cv2.matchTemplate(cropped_search_box, resized_template, cv2.TM_CCOEFF_NORMED)\n",
    "    result = np.sort(result.flatten())[::-1]\n",
    "        \n",
    "    return result.max() > score_threshold\n",
    "\n",
    "def compute_reward(reward_state):\n",
    "    # Compute reward based on the change in the game screen   \n",
    "    time = reward_state['time']\n",
    "    state = reward_state['state']\n",
    "    action = reward_state['action']\n",
    "    prev_action = reward_state['prev_action']\n",
    "    next_state = reward_state['next_state']\n",
    "    total_reward = reward_state['total_reward']\n",
    "    non_crop_state = reward_state['non_crop_state']\n",
    "    reward = 0\n",
    "    \n",
    "    if is_game_over(non_crop_state):\n",
    "        reward = -100\n",
    "        return reward\n",
    "    \n",
    "    if action == 0:\n",
    "        reward += 3\n",
    "    elif action == 1:\n",
    "        reward -= 2\n",
    "    elif action == 2:\n",
    "        reward -= 0.25\n",
    "    elif action == 3:\n",
    "        reward -= 0.25\n",
    "    elif action == 4:\n",
    "        reward -= 0.5\n",
    "\n",
    "    if action == prev_action:\n",
    "        reward -= 0.25        \n",
    "    \n",
    "    return reward\n",
    "    \n",
    "\n",
    "# train DQN agent\n",
    "agent = DQNAgent(180, 5, 128, 1000, 32, 0.99, 1.0, 0.1, 10000)\n",
    "cv_model = YOLO('best_cv.pt')\n",
    "episodes = 1000\n",
    "episode_length = 1000\n",
    "losses = []\n",
    "rewards = []\n",
    "\n",
    "agent.load('dqn.pth')\n",
    "\n",
    "print(\"Model is ready to train\")\n",
    "\n",
    "keyboard.wait('q')\n",
    "\n",
    "for episode in range(episodes):\n",
    "    screenshot, non_crop_state = get_screen(GAME_REGION)\n",
    "    state = get_state(screenshot)\n",
    "    total_reward = 0\n",
    "    total_loss = 0\n",
    "    action = 0\n",
    "    \n",
    "    \n",
    "    start_time = time.time()\n",
    "    for step in range(episode_length):\n",
    "        prev_action = action\n",
    "        \n",
    "        action = agent.select_action(state)\n",
    "        \n",
    "        if action < 4:\n",
    "            pyautogui.press(['up', 'down', 'left', 'right'][action])\n",
    "        \n",
    "        # time.sleep(0.5)\n",
    "        \n",
    "        next_screenshot, non_crop_state = get_screen(GAME_REGION)\n",
    "        next_state = get_state(next_screenshot)\n",
    "        # Save the state as a screenshot\n",
    "        \n",
    "        # cv2.imwrite(f'state_{step}.png', state)\n",
    "        \n",
    "        \n",
    "        reward_state = {\n",
    "            'state': state,\n",
    "            'action': action,\n",
    "            'prev_action': prev_action,\n",
    "            'next_state': next_state,\n",
    "            'time': time.time() - start_time,\n",
    "            'total_reward': total_reward,\n",
    "            'non_crop_state': non_crop_state\n",
    "        }\n",
    "        \n",
    "        reward = compute_reward(reward_state)\n",
    "        done = 0\n",
    "        \n",
    "        agent.push(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "        \n",
    "        if is_game_over(non_crop_state):\n",
    "            done = 1\n",
    "            agent.push(state, action, reward, next_state, done)\n",
    "            break\n",
    "        # else:\n",
    "        #     print(\"False\")\n",
    "        \n",
    "        loss = agent.optimize_model()\n",
    "        if loss is not None:\n",
    "            total_loss += loss\n",
    "        \n",
    "        print('step: {}, loss: {}, reward: {}, total_reward: {}\\n'.format(step, loss, reward, total_reward))\n",
    "    \n",
    "    # tap space key to restart the game\n",
    "    keyboard.press_and_release('space')\n",
    "    losses.append(total_loss)\n",
    "    rewards.append(total_reward)\n",
    "    print('\\nepisode: {}, loss: {}, reward: {}'.format(episode, total_loss, total_reward))\n",
    "    agent.save('dqn.pth')\n",
    "    agent.reset()\n",
    "    \n",
    "    time.sleep(3.25)\n",
    "    keyboard.press_and_release('space')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
