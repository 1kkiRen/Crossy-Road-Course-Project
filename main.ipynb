{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DQN model for Crossy Road game\n",
    "# input is image of the game screen\n",
    "# output is the action to take (up, down, left, right)\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_size):\n",
    "        super(DQN, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=5, stride=2)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=5, stride=2)\n",
    "        self.conv3 = nn.Conv2d(32, 32, kernel_size=5, stride=2)\n",
    "        \n",
    "        def conv2d_size_out(size, kernel_size=5, stride=2):\n",
    "            return (size - (kernel_size - 1) - 1) // stride + 1\n",
    "        \n",
    "        convw = conv2d_size_out(conv2d_size_out(conv2d_size_out(input_size[1])))\n",
    "        convh = conv2d_size_out(conv2d_size_out(conv2d_size_out(input_size[2])))\n",
    "        linear_input_size = convw * convh * 32\n",
    "        \n",
    "        self.fc1 = nn.Linear(linear_input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.relu(self.fc1(x.reshape(x.size(0), -1)))\n",
    "        return self.fc2(x)\n",
    "        \n",
    "    def act(self, state, epsilon):\n",
    "        if random.random() > epsilon:\n",
    "            state = torch.FloatTensor(state).unsqueeze(0)\n",
    "            q_value = self.forward(state)\n",
    "            action = q_value.max(1)[1].data[0]\n",
    "        else:\n",
    "            action = random.randrange(self.output_size)\n",
    "        return action\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test passed\n"
     ]
    }
   ],
   "source": [
    "# Replay buffer for DQN\n",
    "# stores the transitions (state, action, reward, next_state, done)\n",
    "# and samples a batch of transitions for training\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.buffer = []\n",
    "        self.position = 0\n",
    "        \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        if len(self.buffer) < self.capacity:\n",
    "            self.buffer.append(None)\n",
    "        self.buffer[self.position] = (state, action, reward, next_state, done)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "        \n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        state, action, reward, next_state, done = map(np.stack, zip(*batch))\n",
    "        return state, action, reward, next_state, done\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "    \n",
    "    \n",
    "# DQN agent for Crossy Road game\n",
    "# uses DQN model and replay buffer for training\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self, input_size, output_size, hidden_size, replay_buffer_capacity, batch_size, gamma, epsilon_start, epsilon_end, epsilon_decay):\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.replay_buffer_capacity = replay_buffer_capacity\n",
    "        self.batch_size = batch_size\n",
    "        self.gamma = gamma\n",
    "        self.epsilon_start = epsilon_start\n",
    "        self.epsilon_end = epsilon_end\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        \n",
    "        self.model = DQN(input_size, output_size, hidden_size)\n",
    "        self.replay_buffer = ReplayBuffer(replay_buffer_capacity)\n",
    "        self.optimizer = optim.Adam(self.model.parameters())\n",
    "        \n",
    "        self.steps_done = 0\n",
    "        \n",
    "    def select_action(self, state):\n",
    "        epsilon = self.epsilon_end + (self.epsilon_start - self.epsilon_end) * np.exp(-1. * self.steps_done / self.epsilon_decay)\n",
    "        self.steps_done += 1\n",
    "        return self.model.act(state, epsilon)\n",
    "    \n",
    "    def optimize_model(self):\n",
    "        if len(self.replay_buffer) < self.batch_size:\n",
    "            return\n",
    "        \n",
    "        state, action, reward, next_state, done = self.replay_buffer.sample(self.batch_size)\n",
    "        state = torch.FloatTensor(np.float32(state))\n",
    "        action = torch.LongTensor(action)\n",
    "        reward = torch.FloatTensor(reward)\n",
    "        next_state = torch.FloatTensor(np.float32(next_state))\n",
    "        done = torch.FloatTensor(done)\n",
    "        \n",
    "        q_values = self.model(state)\n",
    "        next_q_values = self.model(next_state)\n",
    "        q_value = q_values.gather(1, action.unsqueeze(1)).squeeze(1)\n",
    "        next_q_value = next_q_values.max(1)[0]\n",
    "        expected_q_value = reward + self.gamma * next_q_value * (1 - done)\n",
    "        \n",
    "        loss = F.smooth_l1_loss(q_value, expected_q_value)\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        return loss.item()\n",
    "    \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        self.replay_buffer.push(state, action, reward, next_state, done)\n",
    "        \n",
    "    def save(self, path):\n",
    "        torch.save(self.model.state_dict(), path)\n",
    "        \n",
    "    def load(self, path):\n",
    "        self.model.load_state_dict(torch.load(path))\n",
    "        self.model.eval()\n",
    "        \n",
    "    def reset(self):\n",
    "        self.steps_done = 0\n",
    "        \n",
    "\n",
    "# test DQN agent\n",
    "agent = DQNAgent((3, 84, 84), 4, 128, 10000, 32, 0.99, 1.0, 0.1, 10000)\n",
    "state = np.random.rand(3, 84, 84)\n",
    "action = agent.select_action(state)\n",
    "reward = 1.0\n",
    "next_state = np.random.rand(3, 84, 84)\n",
    "done = 0\n",
    "agent.push(state, action, reward, next_state, done)\n",
    "loss = agent.optimize_model()\n",
    "agent.save('dqn.pth')\n",
    "agent.load('dqn.pth')\n",
    "agent.reset()\n",
    "print('test passed')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model is ready to train\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "OpenCV(4.10.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\imgproc\\src\\templmatch.cpp:1164: error: (-215:Assertion failed) (depth == CV_8U || depth == CV_32F) && type == _templ.type() && _img.dims() <= 2 in function 'cv::matchTemplate'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 66\u001b[0m\n\u001b[0;32m     63\u001b[0m state \u001b[38;5;241m=\u001b[39m next_state\n\u001b[0;32m     64\u001b[0m total_reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n\u001b[1;32m---> 66\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mis_game_over\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m     67\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m     69\u001b[0m loss \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39moptimize_model()\n",
      "Cell \u001b[1;32mIn[26], line 29\u001b[0m, in \u001b[0;36mis_game_over\u001b[1;34m(screen_image)\u001b[0m\n\u001b[0;32m     27\u001b[0m screen \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mcvtColor(screen, cv2\u001b[38;5;241m.\u001b[39mCOLOR_BGR2GRAY)\n\u001b[0;32m     28\u001b[0m screen \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mnormalize(screen, \u001b[38;5;28;01mNone\u001b[39;00m, alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, beta\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, norm_type\u001b[38;5;241m=\u001b[39mcv2\u001b[38;5;241m.\u001b[39mNORM_MINMAX, dtype\u001b[38;5;241m=\u001b[39mcv2\u001b[38;5;241m.\u001b[39mCV_32F)\n\u001b[1;32m---> 29\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatchTemplate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscreen_image\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrestart_button\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTM_CCOEFF_NORMED\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     30\u001b[0m min_val, max_val, min_loc, max_loc \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mminMaxLoc(res)\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m max_val \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.9\u001b[39m:\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.10.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\imgproc\\src\\templmatch.cpp:1164: error: (-215:Assertion failed) (depth == CV_8U || depth == CV_32F) && type == _templ.type() && _img.dims() <= 2 in function 'cv::matchTemplate'\n"
     ]
    }
   ],
   "source": [
    "# run DQN agent on Crossy Road game to train and play the game\n",
    "import pyautogui\n",
    "import cv2\n",
    "import time\n",
    "import keyboard\n",
    "from torch import res\n",
    "\n",
    "RES_X = 1920\n",
    "RES_Y = 1080\n",
    "\n",
    "GAME_REGION = (405, 210, 850, 480)\n",
    "\n",
    "# Init restart button image\n",
    "restart_button = cv2.imread('restart_button.png', cv2.IMREAD_GRAYSCALE)\n",
    "restart_button = cv2.normalize(restart_button, None, alpha=0, beta=1, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_32F)\n",
    "\n",
    "def get_screen(region):\n",
    "    screen = pyautogui.screenshot(region=(region[0], region[1], region[2], region[3]))\n",
    "    screen = np.array(screen)\n",
    "    screen = cv2.resize(screen, (425, 240))\n",
    "    # screen = np.moveaxis(screen, 2, 0)\n",
    "    return screen\n",
    "\n",
    "def is_game_over(screen_image):\n",
    "    # Check if the game is over by checking if the restart button is visible\n",
    "\n",
    "    \n",
    "\n",
    "# train DQN agent\n",
    "agent = DQNAgent((3, 425, 240), 4, 128, 10000, 32, 0.99, 1.0, 0.1, 10000)\n",
    "episodes = 1000\n",
    "episode_length = 1000\n",
    "losses = []\n",
    "rewards = []\n",
    "\n",
    "print(\"Model is ready to train\")\n",
    "# start the train after pressing 's' key\n",
    "keyboard.wait('s')\n",
    "\n",
    "for episode in range(episodes):\n",
    "    state = get_screen(GAME_REGION)\n",
    "    total_reward = 0\n",
    "    total_loss = 0\n",
    "    \n",
    "    for step in range(episode_length):\n",
    "        action = agent.select_action(state)\n",
    "        pyautogui.press(['up', 'down', 'left', 'right'][action])\n",
    "        time.sleep(0.1)\n",
    "        next_state = get_screen(GAME_REGION)\n",
    "        reward = 1.0\n",
    "        done = 0\n",
    "        agent.push(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "        \n",
    "        if is_game_over(state):\n",
    "            break\n",
    "        \n",
    "        loss = agent.optimize_model()\n",
    "        if loss is not None:\n",
    "            total_loss += loss\n",
    "    \n",
    "    # tap space key to restart the game\n",
    "    keyboard.press_and_release('space')\n",
    "    time.sleep(4)\n",
    "    keyboard.press_and_release('space')\n",
    "    time.sleep(1)\n",
    "            \n",
    "    losses.append(total_loss)\n",
    "    rewards.append(total_reward)\n",
    "    print('episode: {}, loss: {}, reward: {}'.format(episode, total_loss, total_reward))\n",
    "    agent.save('dqn.pth')\n",
    "    agent.reset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
