{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DQN model for Crossy Road game\n",
    "# input is image of the game screen\n",
    "# output is the action to take (up, down, left, right)\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_size):\n",
    "        super(DQN, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=5, stride=2)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=5, stride=2)\n",
    "        self.conv3 = nn.Conv2d(32, 32, kernel_size=5, stride=2)\n",
    "        \n",
    "        def conv2d_size_out(size, kernel_size=5, stride=2):\n",
    "            return (size - (kernel_size - 1) - 1) // stride + 1\n",
    "        \n",
    "        convw = conv2d_size_out(conv2d_size_out(conv2d_size_out(input_size[1])))\n",
    "        convh = conv2d_size_out(conv2d_size_out(conv2d_size_out(input_size[2])))\n",
    "        linear_input_size = convw * convh * 32\n",
    "        \n",
    "        self.fc1 = nn.Linear(linear_input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.relu(self.fc1(x.reshape(x.size(0), -1)))\n",
    "        return self.fc2(x)\n",
    "        \n",
    "    def act(self, state, epsilon):\n",
    "        # print(epsilon)\n",
    "        if random.random() < epsilon:\n",
    "            state = torch.FloatTensor(state).unsqueeze(0)\n",
    "            \n",
    "            state = state.permute(0, 3, 1, 2)\n",
    "            \n",
    "            q_value = self.forward(state)\n",
    "\n",
    "            action = q_value.max(1)[1].data[0]\n",
    "        else:\n",
    "            action = random.randrange(self.output_size)\n",
    "        return action\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replay buffer for DQN\n",
    "# stores the transitions (state, action, reward, next_state, done)\n",
    "# and samples a batch of transitions for training\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.buffer = []\n",
    "        self.position = 0\n",
    "        \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        if len(self.buffer) < self.capacity:\n",
    "            self.buffer.append(None)\n",
    "        self.buffer[self.position] = (state, action, reward, next_state, done)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "        \n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        state, action, reward, next_state, done = map(np.stack, zip(*batch))\n",
    "        return state, action, reward, next_state, done\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "    \n",
    "    \n",
    "# DQN agent for Crossy Road game\n",
    "# uses DQN model and replay buffer for training\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self, input_size, output_size, hidden_size, replay_buffer_capacity, batch_size, gamma, epsilon_start, epsilon_end, epsilon_decay):\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.replay_buffer_capacity = replay_buffer_capacity\n",
    "        self.batch_size = batch_size\n",
    "        self.gamma = gamma\n",
    "        self.epsilon_start = epsilon_start\n",
    "        self.epsilon_end = epsilon_end\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        \n",
    "        self.model = DQN(input_size, output_size, hidden_size)\n",
    "        self.replay_buffer = ReplayBuffer(replay_buffer_capacity)\n",
    "        self.optimizer = optim.Adam(self.model.parameters())\n",
    "        \n",
    "        self.steps_done = 0\n",
    "        \n",
    "    def select_action(self, state):\n",
    "        epsilon = self.epsilon_end + (self.epsilon_start - self.epsilon_end) * np.exp(-1. * self.steps_done / self.epsilon_decay)\n",
    "        self.steps_done += 1\n",
    "        return self.model.act(state, epsilon)\n",
    "    \n",
    "    def optimize_model(self):\n",
    "        if len(self.replay_buffer) < self.batch_size:\n",
    "            return\n",
    "        \n",
    "        state, action, reward, next_state, done = self.replay_buffer.sample(self.batch_size)\n",
    "        state = torch.FloatTensor(np.float32(state))\n",
    "        action = torch.LongTensor(action)\n",
    "        reward = torch.FloatTensor(reward)\n",
    "        next_state = torch.FloatTensor(np.float32(next_state))\n",
    "        done = torch.FloatTensor(done)\n",
    "        \n",
    "        # print(state.shape, action.shape, reward.shape, next_state.shape, done.shape)\n",
    "        \n",
    "        next_state = next_state.permute(0, 3, 1, 2)\n",
    "        state = state.permute(0, 3, 1, 2)\n",
    "        \n",
    "        q_values = self.model(state)\n",
    "        next_q_values = self.model(next_state)\n",
    "        q_value = q_values.gather(1, action.unsqueeze(1)).squeeze(1)\n",
    "        next_q_value = next_q_values.max(1)[0]\n",
    "        expected_q_value = reward + self.gamma * next_q_value * (1 - done)\n",
    "        \n",
    "        loss = F.mse_loss(q_value, expected_q_value)\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        return loss.item()\n",
    "    \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        self.replay_buffer.push(state, action, reward, next_state, done)\n",
    "        \n",
    "    def save(self, path):\n",
    "        torch.save(self.model.state_dict(), path)\n",
    "        \n",
    "    def load(self, path):\n",
    "        self.model.load_state_dict(torch.load(path))\n",
    "        self.model.eval()\n",
    "        \n",
    "    def reset(self):\n",
    "        self.steps_done = 0\n",
    "        \n",
    "\n",
    "# test DQN agent\n",
    "# agent = DQNAgent((3, 84, 84), 4, 128, 10000, 32, 0.99, 1.0, 0.1, 10000)\n",
    "# state = np.random.rand(1, 3, 425, 240)\n",
    "# action = agent.select_action(state)\n",
    "# reward = 1.0\n",
    "# next_state = np.random.rand(3, 84, 84)\n",
    "# done = 0\n",
    "# agent.push(state, action, reward, next_state, done)\n",
    "# loss = agent.optimize_model()\n",
    "# agent.save('dqn.pth')\n",
    "# agent.load('dqn.pth')\n",
    "# agent.reset()\n",
    "# print('test passed')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model is ready to train\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "OpenCV(4.10.0) :-1: error: (-5:Bad argument) in function 'imwrite'\n> Overload resolution failed:\n>  - img is not a numpy array, neither a scalar\n>  - Expected Ptr<cv::UMat> for argument 'img'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 105\u001b[0m\n\u001b[0;32m    102\u001b[0m keyboard\u001b[38;5;241m.\u001b[39mwait(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    104\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m episode \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(episodes):\n\u001b[1;32m--> 105\u001b[0m     state \u001b[38;5;241m=\u001b[39m \u001b[43mget_screen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mGAME_REGION\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    106\u001b[0m     total_reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    107\u001b[0m     total_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "Cell \u001b[1;32mIn[4], line 41\u001b[0m, in \u001b[0;36mget_screen\u001b[1;34m(region)\u001b[0m\n\u001b[0;32m     37\u001b[0m screen \u001b[38;5;241m=\u001b[39m transforms(screen)    \n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# print(screen.shape)\u001b[39;00m\n\u001b[1;32m---> 41\u001b[0m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mscreen\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mrandom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.png\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscreen\u001b[49m\u001b[43m)\u001b[49m \n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m screen\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.10.0) :-1: error: (-5:Bad argument) in function 'imwrite'\n> Overload resolution failed:\n>  - img is not a numpy array, neither a scalar\n>  - Expected Ptr<cv::UMat> for argument 'img'\n"
     ]
    }
   ],
   "source": [
    "# run DQN agent on Crossy Road game to train and play the game\n",
    "import pyautogui\n",
    "import cv2\n",
    "import time\n",
    "import keyboard\n",
    "from torch import res\n",
    "\n",
    "\n",
    "RES_X = 1920\n",
    "RES_Y = 1080\n",
    "\n",
    "GAME_REGION = (405, 210, 850, 480)\n",
    "restart_button = cv2.imread('restart_button.png', cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "\n",
    "# Init restart button image\n",
    "# restart_button = cv2.imread('restart_button.png', cv2.IMREAD_GRAYSCALE)\n",
    "# restart_button = cv2.normalize(restart_button, None, alpha=0, beta=1, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_32F)\n",
    "\n",
    "def get_screen(region):\n",
    "    screen = pyautogui.screenshot(region=(region[0], region[1], region[2], region[3]))\n",
    "    screen = np.array(screen)\n",
    "    screen = cv2.cvtColor(screen, cv2.COLOR_RGB2BGR)\n",
    "    screen = cv2.resize(screen, (425, 240))\n",
    "    # screen = np.moveaxis(screen, 2, 0)\n",
    "    \n",
    "    \n",
    "    return screen\n",
    "\n",
    "def is_game_over(image, score_threshold=0.5, scale=0.5):\n",
    "    # Check if the game is over by checking if the restart button is visible\n",
    "    grey_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # resized_screenshot = cv2.resize(grey_image, (0, 0), fx=scale, fy=scale)\n",
    "    resized_template = cv2.resize(restart_button, (0, 0), fx=scale, fy=scale)\n",
    "    h, w = grey_image.shape\n",
    "\n",
    "    cropped_search_box = grey_image[int(h * 0.8):, int(w * 0.35):int(w * 0.65)]\n",
    "    \n",
    "    # cv2.imwrite(f'cropped_search_box_{random.random()}.png', cropped_search_box)\n",
    "    \n",
    "    result = cv2.matchTemplate(cropped_search_box, resized_template, cv2.TM_CCOEFF_NORMED)\n",
    "    # locations = np.where(result >= score_threshold)\n",
    "    \n",
    "    # sort result in descending order\n",
    "    result = np.sort(result.flatten())[::-1]\n",
    "    \n",
    "    # print(result[0:5])\n",
    "        \n",
    "    return result.max() > score_threshold\n",
    "\n",
    "def compute_reward(reward_state):\n",
    "    # Compute reward based on the change in the game screen   \n",
    "    time = reward_state['time']\n",
    "    state = reward_state['state']\n",
    "    action = reward_state['action']\n",
    "    next_state = reward_state['next_state']\n",
    "    total_reward = reward_state['total_reward']\n",
    "    reward = 0\n",
    "    \n",
    "    if is_game_over(next_state):\n",
    "        reward = -50\n",
    "        \n",
    "    # reward += time * 0.1\n",
    "    \n",
    "    if action == 0:\n",
    "        reward += 2\n",
    "    elif action == 1:\n",
    "        reward -= 1\n",
    "    elif action == 2:\n",
    "        reward -= 0.25\n",
    "    elif action == 3:\n",
    "        reward -= 0.25\n",
    "    \n",
    "    return reward\n",
    "    \n",
    "\n",
    "# train DQN agent\n",
    "agent = DQNAgent((3, 425, 240), 4, 128, 1000, 8, 0.99, 1.0, 0.1, 10000)\n",
    "episodes = 1000\n",
    "episode_length = 1000\n",
    "losses = []\n",
    "rewards = []\n",
    "\n",
    "print(\"Model is ready to train\")\n",
    "# start the train after pressing 's' key\n",
    "keyboard.wait('s')\n",
    "\n",
    "for episode in range(episodes):\n",
    "    state = get_screen(GAME_REGION)\n",
    "    total_reward = 0\n",
    "    total_loss = 0\n",
    "    \n",
    "    \n",
    "    start_time = time.time()\n",
    "    for step in range(episode_length):\n",
    "        action = agent.select_action(state)\n",
    "        pyautogui.press(['up', 'down', 'left', 'right'][action])\n",
    "        \n",
    "        time.sleep(0.5)\n",
    "        \n",
    "        next_state = get_screen(GAME_REGION)\n",
    "        \n",
    "        # Save the state as a screenshot\n",
    "        # cv2.imwrite(f'state_{step}.png', state)\n",
    "        \n",
    "        \n",
    "        reward_state = {\n",
    "            'state': state,\n",
    "            'action': action,\n",
    "            'next_state': next_state,\n",
    "            'time': time.time() - start_time,\n",
    "            'total_reward': total_reward,\n",
    "        }\n",
    "        \n",
    "        reward = compute_reward(reward_state)\n",
    "        done = 0\n",
    "        \n",
    "        agent.push(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "        \n",
    "        if is_game_over(state):\n",
    "            done = 1\n",
    "            agent.push(state, action, reward, next_state, done)\n",
    "            break\n",
    "        # else:\n",
    "        #     print(\"False\")\n",
    "        \n",
    "        loss = agent.optimize_model()\n",
    "        if loss is not None:\n",
    "            total_loss += loss\n",
    "        \n",
    "        print('step: {}, loss: {}, reward: {}, total_reward: {}\\n'.format(step, loss, reward, total_reward))\n",
    "    \n",
    "    # tap space key to restart the game\n",
    "    keyboard.press_and_release('space')\n",
    "    time.sleep(4)\n",
    "            \n",
    "    losses.append(total_loss)\n",
    "    rewards.append(total_reward)\n",
    "    print('\\nepisode: {}, loss: {}, reward: {}'.format(episode, total_loss, total_reward))\n",
    "    agent.save('dqn.pth')\n",
    "    agent.reset()\n",
    "    \n",
    "    keyboard.press_and_release('space')\n",
    "    time.sleep(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
